{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import duckdb\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from glob import glob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/leash-bio/preprocessed\"\n",
    "save_dir = \"/home/pervinco/Models/leash_bio\"\n",
    "os.makedirs(f\"{save_dir}/weights\", exist_ok=True)\n",
    "os.makedirs(f\"{save_dir}/utils\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "['/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_0.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_1.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_2.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_3.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_4.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_5.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_6.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_7.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_8.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/fingerprints/fingerprints_b1_chunk_9.csv']\n",
      "['/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_0.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_1.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_2.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_3.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_4.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_5.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_6.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_7.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_8.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds1/descriptors/descriptors_b1_chunk_9.csv'] \n",
      "\n",
      "10 10\n",
      "['/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_0.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_1.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_2.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_3.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_4.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_5.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_6.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_7.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_8.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/fingerprints/fingerprints_b0_chunk_9.csv']\n",
      "['/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_0.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_1.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_2.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_3.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_4.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_5.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_6.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_7.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_8.csv', '/home/pervinco/Datasets/leash-bio/preprocessed/binds0/descriptors/descriptors_b0_chunk_9.csv']\n"
     ]
    }
   ],
   "source": [
    "b1_desc_files = sorted(glob(f\"{data_dir}/binds1/descriptors/*.csv\"))\n",
    "b1_fp_files = sorted(glob(f\"{data_dir}/binds1/fingerprints/*.csv\"))\n",
    "print(len(b1_desc_files), len(b1_fp_files))\n",
    "print(b1_fp_files)\n",
    "print(b1_desc_files, '\\n')\n",
    "\n",
    "b0_desc_files = sorted(glob(f\"{data_dir}/binds0/descriptors/*.csv\"))\n",
    "b0_fp_files = sorted(glob(f\"{data_dir}/binds0/fingerprints/*.csv\"))\n",
    "print(len(b0_desc_files), len(b0_fp_files))\n",
    "print(b0_fp_files)\n",
    "print(b0_desc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>binds</th>\n",
       "      <th>_PolarizabilityC1</th>\n",
       "      <th>_PolarizabilityC2</th>\n",
       "      <th>_PolarizabilityC3</th>\n",
       "      <th>_SolventAccessibilityC1</th>\n",
       "      <th>_SolventAccessibilityC2</th>\n",
       "      <th>_SolventAccessibilityC3</th>\n",
       "      <th>_SecondaryStrC1</th>\n",
       "      <th>_SecondaryStrC2</th>\n",
       "      <th>...</th>\n",
       "      <th>_HydrophobicityD2075</th>\n",
       "      <th>_HydrophobicityD2100</th>\n",
       "      <th>_HydrophobicityD3001</th>\n",
       "      <th>_HydrophobicityD3025</th>\n",
       "      <th>_HydrophobicityD3050</th>\n",
       "      <th>_HydrophobicityD3075</th>\n",
       "      <th>_HydrophobicityD3100</th>\n",
       "      <th>protein_name_BRD4</th>\n",
       "      <th>protein_name_HSA</th>\n",
       "      <th>protein_name_sEH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0100100000000000000100000001000000000000010000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.164</td>\n",
       "      <td>...</td>\n",
       "      <td>72.687</td>\n",
       "      <td>99.486</td>\n",
       "      <td>0.073</td>\n",
       "      <td>19.677</td>\n",
       "      <td>45.668</td>\n",
       "      <td>69.604</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0100100000000000000100000001000000000000010000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.282</td>\n",
       "      <td>...</td>\n",
       "      <td>74.713</td>\n",
       "      <td>99.836</td>\n",
       "      <td>0.164</td>\n",
       "      <td>22.989</td>\n",
       "      <td>49.754</td>\n",
       "      <td>74.548</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0100100000000000000100000001000000000000010000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>70.811</td>\n",
       "      <td>99.640</td>\n",
       "      <td>0.180</td>\n",
       "      <td>25.225</td>\n",
       "      <td>48.829</td>\n",
       "      <td>74.955</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0100000000000001000000000001000000000000010000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.164</td>\n",
       "      <td>...</td>\n",
       "      <td>72.687</td>\n",
       "      <td>99.486</td>\n",
       "      <td>0.073</td>\n",
       "      <td>19.677</td>\n",
       "      <td>45.668</td>\n",
       "      <td>69.604</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0100000000000001000000000001000000000000010000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.282</td>\n",
       "      <td>...</td>\n",
       "      <td>74.713</td>\n",
       "      <td>99.836</td>\n",
       "      <td>0.164</td>\n",
       "      <td>22.989</td>\n",
       "      <td>49.754</td>\n",
       "      <td>74.548</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  binds  \\\n",
       "0  0100100000000000000100000001000000000000010000...      0   \n",
       "1  0100100000000000000100000001000000000000010000...      0   \n",
       "2  0100100000000000000100000001000000000000010000...      0   \n",
       "3  0100000000000001000000000001000000000000010000...      0   \n",
       "4  0100000000000001000000000001000000000000010000...      0   \n",
       "\n",
       "   _PolarizabilityC1  _PolarizabilityC2  _PolarizabilityC3  \\\n",
       "0              0.251              0.526              0.222   \n",
       "1              0.278              0.450              0.273   \n",
       "2              0.283              0.456              0.261   \n",
       "3              0.251              0.526              0.222   \n",
       "4              0.278              0.450              0.273   \n",
       "\n",
       "   _SolventAccessibilityC1  _SolventAccessibilityC2  _SolventAccessibilityC3  \\\n",
       "0                    0.258                    0.372                    0.369   \n",
       "1                    0.433                    0.365                    0.202   \n",
       "2                    0.445                    0.301                    0.254   \n",
       "3                    0.258                    0.372                    0.369   \n",
       "4                    0.433                    0.365                    0.202   \n",
       "\n",
       "   _SecondaryStrC1  _SecondaryStrC2  ...  _HydrophobicityD2075  \\\n",
       "0            0.500            0.164  ...                72.687   \n",
       "1            0.524            0.282  ...                74.713   \n",
       "2            0.468            0.268  ...                70.811   \n",
       "3            0.500            0.164  ...                72.687   \n",
       "4            0.524            0.282  ...                74.713   \n",
       "\n",
       "   _HydrophobicityD2100  _HydrophobicityD3001  _HydrophobicityD3025  \\\n",
       "0                99.486                 0.073                19.677   \n",
       "1                99.836                 0.164                22.989   \n",
       "2                99.640                 0.180                25.225   \n",
       "3                99.486                 0.073                19.677   \n",
       "4                99.836                 0.164                22.989   \n",
       "\n",
       "   _HydrophobicityD3050  _HydrophobicityD3075  _HydrophobicityD3100  \\\n",
       "0                45.668                69.604                 100.0   \n",
       "1                49.754                74.548                 100.0   \n",
       "2                48.829                74.955                 100.0   \n",
       "3                45.668                69.604                 100.0   \n",
       "4                49.754                74.548                 100.0   \n",
       "\n",
       "   protein_name_BRD4  protein_name_HSA  protein_name_sEH  \n",
       "0                1.0               0.0               0.0  \n",
       "1                0.0               1.0               0.0  \n",
       "2                0.0               0.0               1.0  \n",
       "3                1.0               0.0               0.0  \n",
       "4                0.0               1.0               0.0  \n",
       "\n",
       "[5 rows x 152 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(b0_fp_files[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(b1_fp_files[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Descriptor Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "for b0_desc_file, b1_desc_file in zip(b0_desc_files, b1_desc_files):\n",
    "    print(b0_desc_file, b1_desc_file)\n",
    "    b0_df = pd.read_csv(b0_desc_file)\n",
    "    b1_df = pd.read_csv(b1_desc_file)\n",
    "    print(b0_df.shape)\n",
    "    print(b1_df.shape)\n",
    "    \n",
    "    combined_df = pd.concat([b0_df, b1_df], ignore_index=True)\n",
    "    combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    non_scaler_columns = [col for col in combined_df.columns if col.startswith('protein_name_')] + ['binds']\n",
    "    scaler_columns = [col for col in combined_df.columns if col not in non_scaler_columns]\n",
    "    \n",
    "    features = combined_df[scaler_columns]\n",
    "    features = scaler.fit_transform(features)\n",
    "    features_df = pd.DataFrame(features, columns=scaler_columns)\n",
    "    features_df = pd.concat([features_df, combined_df[non_scaler_columns].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    binds_counts = features_df['binds'].value_counts()\n",
    "    print(f\"binds=0 count: {binds_counts.get(0, 0)}\")\n",
    "    print(f\"binds=1 count: {binds_counts.get(1, 0)}\")\n",
    "    \n",
    "    scaler_filename = f\"{save_dir}/utils/desc_scaler_{os.path.basename(b0_desc_file)}.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    \n",
    "    X = features_df.drop(columns=['binds'])\n",
    "    y = features_df['binds']\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores for {scores}\")\n",
    "    print(f\"Mean cross-validation score: {scores.mean()}\\n\")\n",
    "    \n",
    "    # 모델 학습 및 저장\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    model_filename = f\"{save_dir}/weights/desc_model_{os.path.basename(b0_desc_file)}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.FingerPrint Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(b0_fp_files[0])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "for b0_fp_file, b1_fp_file in zip(b0_fp_files, b1_fp_files):\n",
    "    print(b0_fp_file, b1_fp_file)\n",
    "    b0_df = pd.read_csv(b0_fp_file)\n",
    "    b1_df = pd.read_csv(b1_fp_file)\n",
    "    print(b0_df.shape, b1_df.shape)\n",
    "    \n",
    "    combined_df = pd.concat([b0_df, b1_df], ignore_index=True)\n",
    "    combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    max_len = combined_df['0'].str.len().max()\n",
    "    split_columns = pd.DataFrame(combined_df['0'].apply(lambda x: list(x)).tolist(), columns=[f'char_{i}' for i in range(max_len)])\n",
    "    combined_df = pd.concat([split_columns, combined_df.drop(columns=['0'])], axis=1)\n",
    "\n",
    "    columns_to_exclude = [col for col in combined_df.columns if col.startswith('binds') or col.startswith('protein_') or col.startswith('char_')]\n",
    "    columns_to_scale = [col for col in combined_df.columns if col not in columns_to_exclude]\n",
    "    combined_df[columns_to_scale] = scaler.fit_transform(combined_df[columns_to_scale])\n",
    "\n",
    "    binds_counts = combined_df['binds'].value_counts()\n",
    "    print(combined_df.shape)\n",
    "    print(f\"binds=0 count: {binds_counts.get(0, 0)}\")\n",
    "    print(f\"binds=1 count: {binds_counts.get(1, 0)}\")\n",
    "\n",
    "    print(combined_df.shape)\n",
    "    X = combined_df.drop(columns=['binds'])\n",
    "    y = combined_df['binds']\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    model_filename = f\"{save_dir}/weights/fp_model_{os.path.basename(b0_fp_file).split('.')[0]}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "    scaler_filename = f\"{save_dir}/utils/fp_scaler_{os.path.basename(b0_fp_file).split('.')[0]}.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
