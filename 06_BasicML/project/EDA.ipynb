{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.DataStructs import BitVectToText\n",
    "from rdkit.Chem import Descriptors, rdFingerprintGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/leash-bio\"\n",
    "save_dir = f\"{data_dir}/split_sets\"\n",
    "\n",
    "train_csv = f\"{data_dir}/train.csv\"\n",
    "test_csv = f\"{data_dir}/test.csv\"\n",
    "\n",
    "train_parquet = f\"{data_dir}/train.parquet\"\n",
    "test_parquet = f'{data_dir}/test.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터셋 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "## binds=0인 데이터만 load\n",
    "count_binds_0 = con.query(f\"\"\"SELECT COUNT(*) \n",
    "                              FROM parquet_scan('{train_parquet}') \n",
    "                              WHERE binds = 0\"\"\").fetchone()[0]\n",
    "print(f\"Total binds=0 : {count_binds_0}\")\n",
    "\n",
    "## binds=1인 데이터만 load\n",
    "count_binds_1 = con.query(f\"\"\"SELECT COUNT(*) \n",
    "                              FROM parquet_scan('{train_parquet}') \n",
    "                              WHERE binds = 1\"\"\").fetchone()[0]\n",
    "print(f\"Total binds=1 : {count_binds_1}\")\n",
    "\n",
    "## 전체 데이터 수\n",
    "total_count = count_binds_0 + count_binds_1\n",
    "print(f\"Total data : {total_count}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 컬럼별 고유한 값, 갯수 파악\n",
    "\n",
    "columns = [\n",
    "    'buildingblock1_smiles', \n",
    "    'buildingblock2_smiles', \n",
    "    'buildingblock3_smiles', \n",
    "    'molecule_smiles', \n",
    "    'protein_name'\n",
    "]\n",
    "\n",
    "con = duckdb.connect()\n",
    "for column in columns:\n",
    "    query = f\"SELECT {column}, COUNT(*) as count FROM parquet_scan('{train_parquet}') GROUP BY {column}\"\n",
    "    df = con.query(query).df()\n",
    "\n",
    "    df.to_csv(f\"{data_dir}/info/{column}_info.csv\", index=False)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 컬럼별 중복 데이터 확인\n",
    "\n",
    "con = duckdb.connect()\n",
    "bb1_query = f\"SELECT DISTINCT buildingblock1_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "bb2_query = f\"SELECT DISTINCT buildingblock2_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "bb3_query = f\"SELECT DISTINCT buildingblock3_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "\n",
    "bb1_set = set(con.query(bb1_query).df()['buildingblock1_smiles'])\n",
    "bb2_set = set(con.query(bb2_query).df()['buildingblock2_smiles'])\n",
    "bb3_set = set(con.query(bb3_query).df()['buildingblock3_smiles'])\n",
    "\n",
    "bb1_bb2_intersection = bb1_set.intersection(bb2_set)\n",
    "bb1_bb3_intersection = bb1_set.intersection(bb3_set)\n",
    "bb2_bb3_intersection = bb2_set.intersection(bb3_set)\n",
    "\n",
    "print(f\"Building block 1 & 2 중복 : {'있음' if bb1_bb2_intersection else '없음'}\")\n",
    "print(f\"Building block 1 & 3 중복 : {'있음' if bb1_bb3_intersection else '없음'}\")\n",
    "print(f\"Building block 2 & 3 중복 : {'있음' if bb2_bb3_intersection else '없음'}\")\n",
    "\n",
    "print(f\"Building block 1과 2 사이의 중복된 값: {bb1_bb2_intersection}\")\n",
    "print(f\"Building block 1과 3 사이의 중복된 값: {bb1_bb3_intersection}\")\n",
    "print(f\"Building block 2와 3 사이의 중복된 값: {bb2_bb3_intersection}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.RDKit을 활용한 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(binds, offset, chunk_size):\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM parquet_scan('{train_parquet}')\n",
    "    WHERE binds = {binds}\n",
    "    ORDER BY random()\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    return con.query(query).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpg = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "def compute_fingerprint(mol):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    return BitVectToText(fp)  # Convert to BitString for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {}\n",
    "    descriptors = Descriptors.CalcMolDescriptors(mol)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제외된 descriptors: ['MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW']\n",
      "사용된 descriptors: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "Processed offset: 0 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 10000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 20000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 30000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 40000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 50000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 60000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 70000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 80000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 90000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 100000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 110000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 120000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 130000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 140000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 150000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 160000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 170000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 180000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 190000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 200000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 210000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 220000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n",
      "Processed offset: 230000 saved to /home/pervinco/Datasets/leash-bio/preprocessed/train.parquet\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "molecule_smiles는 building block들로 조합된 약물 분자.\n",
    "모델 학습을 위해서는 인코딩을 적용해 컴퓨터가 이해할 수 있는 형태로 변환해야함.\n",
    "\"\"\"\n",
    "\n",
    "OFFSET = 0\n",
    "CHUNK_SIZE = 10000\n",
    "train_parquet = f'{data_dir}/train.parquet'\n",
    "con = duckdb.connect()\n",
    "\n",
    "output_dir = f\"{data_dir}/preprocessed\"\n",
    "output_file = f\"{output_dir}/train.parquet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "first_chunk = True\n",
    "while True:\n",
    "    chunk = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM parquet_scan('{train_parquet}')\n",
    "    LIMIT {CHUNK_SIZE} OFFSET {OFFSET}\n",
    "    \"\"\").fetch_df()\n",
    "\n",
    "    if chunk.empty:\n",
    "        break\n",
    "\n",
    "    ## 1.SMILES 문자열을 RDKit 객체로 변환.\n",
    "    chunk['molecule'] = chunk['molecule_smiles'].apply(Chem.MolFromSmiles)\n",
    "\n",
    "    ## 2.FingerPrint(해시 기반 이진 벡터) 생성.\n",
    "    chunk['fingerprints'] = chunk['molecule'].apply(compute_fingerprint)\n",
    "\n",
    "    ## 3.분자식으로부터 추가적인 특징들을 계산.\n",
    "    descriptors_list = chunk['molecule_smiles'].apply(calculate_descriptors).tolist()\n",
    "    descriptor_df = pd.DataFrame(descriptors_list)\n",
    "    excluded_descriptors = descriptor_df.columns[descriptor_df.isna().any()].tolist()\n",
    "    descriptor_df.drop(columns=excluded_descriptors, inplace=True)\n",
    "    used_descriptor = descriptor_df.columns.tolist()\n",
    "\n",
    "    if first_chunk:\n",
    "        print(f\"제외된 descriptors: {excluded_descriptors}\")\n",
    "        print(f\"사용된 descriptors: {used_descriptor}\")\n",
    "\n",
    "    ## molecule 컬럼을 데이터프레임에서 제외\n",
    "    chunk.drop(columns=['molecule'], inplace=True)\n",
    "\n",
    "    ## 원래 데이터와 합치기.\n",
    "    chunk = pd.concat([chunk, descriptor_df], axis=1)\n",
    "\n",
    "    ## Parquet 파일로 저장.\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "\n",
    "    if first_chunk:\n",
    "        writer = pq.ParquetWriter(output_file, table.schema)\n",
    "        first_chunk = False\n",
    "    \n",
    "    writer.write_table(table)\n",
    "    \n",
    "    print(f\"Processed offset: {OFFSET} saved to {output_file}\")\n",
    "    OFFSET += CHUNK_SIZE\n",
    "\n",
    "    # break\n",
    "\n",
    "writer.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(output_file, engine='pyarrow')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
