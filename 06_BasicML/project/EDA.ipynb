{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.DataStructs import BitVectToText\n",
    "from rdkit.Chem import Descriptors, rdFingerprintGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/leash-bio\"\n",
    "save_dir = f\"{data_dir}/split_sets\"\n",
    "\n",
    "train_csv = f\"{data_dir}/train.csv\"\n",
    "test_csv = f\"{data_dir}/test.csv\"\n",
    "\n",
    "train_parquet = f\"{data_dir}/train.parquet\"\n",
    "test_parquet = f'{data_dir}/test.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터셋 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "## binds=0인 데이터만 load\n",
    "count_binds_0 = con.query(f\"\"\"SELECT COUNT(*) \n",
    "                              FROM parquet_scan('{train_parquet}') \n",
    "                              WHERE binds = 0\"\"\").fetchone()[0]\n",
    "print(f\"Total binds=0 : {count_binds_0}\")\n",
    "\n",
    "## binds=1인 데이터만 load\n",
    "count_binds_1 = con.query(f\"\"\"SELECT COUNT(*) \n",
    "                              FROM parquet_scan('{train_parquet}') \n",
    "                              WHERE binds = 1\"\"\").fetchone()[0]\n",
    "print(f\"Total binds=1 : {count_binds_1}\")\n",
    "\n",
    "## 전체 데이터 수\n",
    "total_count = count_binds_0 + count_binds_1\n",
    "print(f\"Total data : {total_count}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 컬럼별 고유한 값, 갯수 파악\n",
    "\n",
    "columns = [\n",
    "    'buildingblock1_smiles', \n",
    "    'buildingblock2_smiles', \n",
    "    'buildingblock3_smiles', \n",
    "    'molecule_smiles', \n",
    "    'protein_name'\n",
    "]\n",
    "\n",
    "con = duckdb.connect()\n",
    "for column in columns:\n",
    "    query = f\"SELECT {column}, COUNT(*) as count FROM parquet_scan('{train_parquet}') GROUP BY {column}\"\n",
    "    df = con.query(query).df()\n",
    "\n",
    "    df.to_csv(f\"{data_dir}/info/{column}_info.csv\", index=False)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 컬럼별 중복 데이터 확인\n",
    "\n",
    "con = duckdb.connect()\n",
    "bb1_query = f\"SELECT DISTINCT buildingblock1_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "bb2_query = f\"SELECT DISTINCT buildingblock2_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "bb3_query = f\"SELECT DISTINCT buildingblock3_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "\n",
    "bb1_set = set(con.query(bb1_query).df()['buildingblock1_smiles'])\n",
    "bb2_set = set(con.query(bb2_query).df()['buildingblock2_smiles'])\n",
    "bb3_set = set(con.query(bb3_query).df()['buildingblock3_smiles'])\n",
    "\n",
    "bb1_bb2_intersection = bb1_set.intersection(bb2_set)\n",
    "bb1_bb3_intersection = bb1_set.intersection(bb3_set)\n",
    "bb2_bb3_intersection = bb2_set.intersection(bb3_set)\n",
    "\n",
    "print(f\"Building block 1 & 2 중복 : {'있음' if bb1_bb2_intersection else '없음'}\")\n",
    "print(f\"Building block 1 & 3 중복 : {'있음' if bb1_bb3_intersection else '없음'}\")\n",
    "print(f\"Building block 2 & 3 중복 : {'있음' if bb2_bb3_intersection else '없음'}\")\n",
    "\n",
    "print(f\"Building block 1과 2 사이의 중복된 값: {bb1_bb2_intersection}\")\n",
    "print(f\"Building block 1과 3 사이의 중복된 값: {bb1_bb3_intersection}\")\n",
    "print(f\"Building block 2와 3 사이의 중복된 값: {bb2_bb3_intersection}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.RDKit을 활용한 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(binds, offset, chunk_size):\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM parquet_scan('{train_parquet}')\n",
    "    WHERE binds = {binds}\n",
    "    ORDER BY random()\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    return con.query(query).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpg = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "def compute_fingerprint(mol):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    return BitVectToText(fp)  # Convert to BitString for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {}\n",
    "    descriptors = Descriptors.CalcMolDescriptors(mol)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "molecule_smiles는 building block들로 조합된 약물 분자.\n",
    "모델 학습을 위해서는 인코딩을 적용해 컴퓨터가 이해할 수 있는 형태로 변환해야함.\n",
    "\"\"\"\n",
    "\n",
    "OFFSET = 0\n",
    "CHUNK_SIZE = 100000\n",
    "OUTPUT_PATH = f\"{data_dir}/preprocessed/train.parquet\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "con = duckdb.connect()\n",
    "\n",
    "first_chunk = True\n",
    "while True:\n",
    "    chunk = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM parquet_scan('{train_parquet}')\n",
    "    LIMIT {CHUNK_SIZE} OFFSET {OFFSET}\n",
    "    \"\"\").fetch_df()\n",
    "\n",
    "    if chunk.empty:\n",
    "        break\n",
    "\n",
    "    ## 1.SMILES 문자열을 RDKit 객체로 변환.\n",
    "    chunk['molecule'] = chunk['molecule_smiles'].apply(Chem.MolFromSmiles)\n",
    "\n",
    "    ## 2.FingerPrint(해시 기반 이진 벡터) 생성.\n",
    "    chunk['fingerprints'] = chunk['molecule'].apply(compute_fingerprint)\n",
    "\n",
    "    ## 3.분자식으로부터 추가적인 특징들을 계산.\n",
    "    descriptors_list = chunk['molecule_smiles'].apply(calculate_descriptors).tolist()\n",
    "    descriptor_df = pd.DataFrame(descriptors_list)\n",
    "    excluded_descriptors = descriptor_df.columns[descriptor_df.isna().any()].tolist()\n",
    "    descriptor_df.drop(columns=excluded_descriptors, inplace=True)\n",
    "    used_descriptor = descriptor_df.columns.tolist()\n",
    "\n",
    "    if first_chunk:\n",
    "        print(f\"제외된 descriptors: {excluded_descriptors}\")\n",
    "        print(f\"사용된 descriptors: {used_descriptor}\")\n",
    "\n",
    "    ## molecule 컬럼을 데이터프레임에서 제외\n",
    "    chunk.drop(columns=['molecule'], inplace=True)\n",
    "\n",
    "    ## 원래 데이터와 합치기.\n",
    "    chunk = pd.concat([chunk, descriptor_df], axis=1)\n",
    "\n",
    "    ## Parquet 파일로 저장.\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "\n",
    "    if first_chunk:\n",
    "        writer = pq.ParquetWriter(OUTPUT_PATH, table.schema)\n",
    "        first_chunk = False\n",
    "    \n",
    "    writer.write_table(table)\n",
    "    \n",
    "    print(f\"Processed offset: {OFFSET}\")\n",
    "    OFFSET += CHUNK_SIZE\n",
    "\n",
    "writer.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
