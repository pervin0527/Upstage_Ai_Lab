{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import duckdb\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from glob import glob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/leash-bio/preprocessed\"\n",
    "save_dir = \"/home/pervinco/Models/leash_bio\"\n",
    "os.makedirs(f\"{save_dir}/weights\", exist_ok=True)\n",
    "os.makedirs(f\"{save_dir}/utils\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_desc_files = sorted(glob(f\"{data_dir}/binds1/descriptors/*.csv\"))\n",
    "b1_fp_files = sorted(glob(f\"{data_dir}/binds1/fingerprints/*.csv\"))\n",
    "print(len(b1_desc_files), len(b1_fp_files))\n",
    "print(b1_fp_files)\n",
    "print(b1_desc_files, '\\n')\n",
    "\n",
    "b0_desc_files = sorted(glob(f\"{data_dir}/binds0/descriptors/*.csv\"))\n",
    "b0_fp_files = sorted(glob(f\"{data_dir}/binds0/fingerprints/*.csv\"))\n",
    "print(len(b0_desc_files), len(b0_fp_files))\n",
    "print(b0_fp_files)\n",
    "print(b0_desc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(b1_fp_files[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(b0_fp_files[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Descriptor Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "for b0_desc_file, b1_desc_file in zip(b0_desc_files, b1_desc_files):\n",
    "    print(b0_desc_file, b1_desc_file)\n",
    "    b0_df = pd.read_csv(b0_desc_file)\n",
    "    b1_df = pd.read_csv(b1_desc_file)\n",
    "    print(b0_df.shape, b1_df.shape)\n",
    "    \n",
    "    combined_df = pd.concat([b0_df, b1_df], ignore_index=True)\n",
    "    combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    non_scaler_columns = [col for col in combined_df.columns if col.startswith('protein_name_')] + ['binds']\n",
    "    scaler_columns = [col for col in combined_df.columns if col not in non_scaler_columns]\n",
    "    \n",
    "    features = combined_df[scaler_columns]\n",
    "    features = scaler.fit_transform(features)\n",
    "    features_df = pd.DataFrame(features, columns=scaler_columns)\n",
    "    features_df = pd.concat([features_df, combined_df[non_scaler_columns].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    binds_counts = features_df['binds'].value_counts()\n",
    "    print(f\"binds=0 count: {binds_counts.get(0, 0)}\")\n",
    "    print(f\"binds=1 count: {binds_counts.get(1, 0)}\")\n",
    "    \n",
    "    X = features_df.drop(columns=['binds'])\n",
    "    y = features_df['binds']\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores for {scores}\")\n",
    "    print(f\"Mean cross-validation score: {scores.mean()}\\n\")\n",
    "    \n",
    "    # 모델 학습 및 저장\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    scaler_filename = f\"{save_dir}/utils/desc_scaler_{os.path.basename(b0_desc_file)}.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    \n",
    "    model_filename = f\"{save_dir}/weights/desc_model_{os.path.basename(b0_desc_file)}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(scaler_filename, model_filename)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.FingerPrint Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "for b0_fp_file, b1_fp_file in zip(b0_fp_files, b1_fp_files):\n",
    "    print(b0_fp_file, b1_fp_file)\n",
    "    b0_df = pd.read_csv(b0_fp_file)\n",
    "    b1_df = pd.read_csv(b1_fp_file)\n",
    "    print(b0_df.shape, b1_df.shape)\n",
    "    \n",
    "    combined_df = pd.concat([b0_df, b1_df], ignore_index=True)\n",
    "    combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    max_len = combined_df['0'].str.len().max()\n",
    "    split_columns = pd.DataFrame(combined_df['0'].apply(lambda x: list(x)).tolist(), columns=[f'char_{i}' for i in range(max_len)])\n",
    "    combined_df = pd.concat([split_columns, combined_df.drop(columns=['0'])], axis=1)\n",
    "\n",
    "    columns_to_exclude = [col for col in combined_df.columns if col.startswith('binds') or col.startswith('protein_') or col.startswith('char_')]\n",
    "    columns_to_scale = [col for col in combined_df.columns if col not in columns_to_exclude]\n",
    "    combined_df[columns_to_scale] = scaler.fit_transform(combined_df[columns_to_scale])\n",
    "\n",
    "    binds_counts = combined_df['binds'].value_counts()\n",
    "    print(combined_df.shape)\n",
    "    print(f\"binds=0 count: {binds_counts.get(0, 0)}\")\n",
    "    print(f\"binds=1 count: {binds_counts.get(1, 0)}\")\n",
    "\n",
    "    print(combined_df.shape)\n",
    "    X = combined_df.drop(columns=['binds'])\n",
    "    y = combined_df['binds']\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores for {scores}\")\n",
    "    print(f\"Mean cross-validation score: {scores.mean()}\\n\")\n",
    "    \n",
    "    # 모델 학습 및 저장\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    scaler_filename = f\"{save_dir}/utils/fp_scaler_{os.path.basename(b0_desc_file)}.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    \n",
    "    model_filename = f\"{save_dir}/weights/fp_model_{os.path.basename(b0_desc_file)}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(scaler_filename, model_filename)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/home/pervinco/Datasets/leash-bio/for_test\"\n",
    "saved_dir = \"/home/pervinco/Models/leash_bio\"\n",
    "\n",
    "desc_files = sorted(glob(f\"{test_dir}/descriptors/*.csv\"))\n",
    "fp_files = sorted(glob(f\"{test_dir}/fingerprints/*.csv\"))\n",
    "print(len(fp_files), len(desc_files))\n",
    "\n",
    "saved_weight_files = sorted(glob(f\"{saved_dir}/weigths/*\"))\n",
    "saved_util_files = sorted(glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc_file, fp_file in zip(desc_files, fp_files):\n",
    "    desc_df = pd.read_csv(desc_file)\n",
    "    fp_df = pd.read_csv(fp_file)\n",
    "\n",
    "    max_len = fp_df['fingerprints'].str.len().max()\n",
    "    split_columns = pd.DataFrame(fp_df['fingerprints'].apply(lambda x: list(x)).tolist(), columns=[f'char_{i}' for i in range(max_len)])\n",
    "    fp_df = pd.concat([split_columns, fp_df.drop(columns=['fingerprints'])], axis=1)\n",
    "\n",
    "    columns_to_exclude = [col for col in fp_df.columns if col.startswith('protein_') or col.startswith('char_')]\n",
    "    columns_to_scale = [col for col in fp_df.columns if col not in columns_to_exclude]\n",
    "\n",
    "    fp_df[columns_to_scale] = scaler.fit_transform(fp_df[columns_to_scale])\n",
    "    desc_df[columns_to_scale] = scaler.fit_transform(fp_df[columns_to_scale])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
