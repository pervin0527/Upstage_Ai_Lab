{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import duckdb\n",
    "import requests\n",
    "import PyBioMed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.DataStructs import BitVectToText\n",
    "from rdkit.Chem import Descriptors, rdFingerprintGenerator\n",
    "\n",
    "from PyBioMed.Pyprotein import PyProtein\n",
    "from PyBioMed.PyGetMol import GetProtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/leash-bio\"\n",
    "save_dir = f\"{data_dir}/split_sets\"\n",
    "\n",
    "train_csv = f\"{data_dir}/train.csv\"\n",
    "test_csv = f\"{data_dir}/test.csv\"\n",
    "\n",
    "train_parquet = f\"{data_dir}/train.parquet\"\n",
    "test_parquet = f'{data_dir}/test.parquet'\n",
    "\n",
    "target_proteins = ['sEH', 'HSA', 'BRD4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_csv, nrows=100)\n",
    "df.to_csv('./train_samples.csv', index=False)\n",
    "\n",
    "df = pd.read_csv(test_csv, nrows=100)\n",
    "df.to_csv('./test_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터셋 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "## binds=0인 데이터만 load\n",
    "count_binds_0 = con.query(f\"\"\"SELECT COUNT(*) \n",
    "                              FROM parquet_scan('{train_parquet}') \n",
    "                              WHERE binds = 0\"\"\").fetchone()[0]\n",
    "print(f\"Total binds=0 : {count_binds_0}\")\n",
    "\n",
    "## binds=1인 데이터만 load\n",
    "count_binds_1 = con.query(f\"\"\"SELECT COUNT(*) \n",
    "                              FROM parquet_scan('{train_parquet}') \n",
    "                              WHERE binds = 1\"\"\").fetchone()[0]\n",
    "print(f\"Total binds=1 : {count_binds_1}\")\n",
    "\n",
    "## 전체 데이터 수\n",
    "total_count = count_binds_0 + count_binds_1\n",
    "print(f\"Total data : {total_count}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 컬럼별 고유한 값, 갯수 파악\n",
    "\n",
    "columns = [\n",
    "    'buildingblock1_smiles', \n",
    "    'buildingblock2_smiles', \n",
    "    'buildingblock3_smiles', \n",
    "    'molecule_smiles', \n",
    "    'protein_name'\n",
    "]\n",
    "\n",
    "con = duckdb.connect()\n",
    "for column in columns:\n",
    "    query = f\"SELECT {column}, COUNT(*) as count FROM parquet_scan('{train_parquet}') GROUP BY {column}\"\n",
    "    df = con.query(query).df()\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 컬럼별 중복 데이터 확인\n",
    "\n",
    "con = duckdb.connect()\n",
    "bb1_query = f\"SELECT DISTINCT buildingblock1_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "bb2_query = f\"SELECT DISTINCT buildingblock2_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "bb3_query = f\"SELECT DISTINCT buildingblock3_smiles FROM parquet_scan('{train_parquet}')\"\n",
    "\n",
    "bb1_set = set(con.query(bb1_query).df()['buildingblock1_smiles'])\n",
    "bb2_set = set(con.query(bb2_query).df()['buildingblock2_smiles'])\n",
    "bb3_set = set(con.query(bb3_query).df()['buildingblock3_smiles'])\n",
    "\n",
    "bb1_bb2_intersection = bb1_set.intersection(bb2_set)\n",
    "bb1_bb3_intersection = bb1_set.intersection(bb3_set)\n",
    "bb2_bb3_intersection = bb2_set.intersection(bb3_set)\n",
    "\n",
    "print(f\"Building block 1 & 2 중복 : {'있음' if bb1_bb2_intersection else '없음'}\")\n",
    "print(f\"Building block 1 & 3 중복 : {'있음' if bb1_bb3_intersection else '없음'}\")\n",
    "print(f\"Building block 2 & 3 중복 : {'있음' if bb2_bb3_intersection else '없음'}\")\n",
    "\n",
    "print(f\"Building block 1과 2 사이의 중복된 값: {bb1_bb2_intersection}\")\n",
    "print(f\"Building block 1과 3 사이의 중복된 값: {bb1_bb3_intersection}\")\n",
    "print(f\"Building block 2와 3 사이의 중복된 값: {bb2_bb3_intersection}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 2000\n",
    "\n",
    "con = duckdb.connect()\n",
    "data = con.query(f\"\"\"(SELECT *\n",
    "                      FROM parquet_scan('{train_parquet}')\n",
    "                      WHERE binds = 0\n",
    "                      LIMIT {limit})\n",
    "                      UNION ALL\n",
    "                      (SELECT *\n",
    "                      FROM parquet_scan('{train_parquet}')\n",
    "                      WHERE binds = 1\n",
    "                      LIMIT {limit})\"\"\").df()\n",
    "\n",
    "con.close()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.RDKit을 활용한 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## molecule을 Morgan FingerPrint로 변환.\n",
    "def compute_fingerprint(mol):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    return BitVectToText(fp)  # Convert to BitString for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## molecule로부터 descriptor 계산.\n",
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {}\n",
    "    descriptors = Descriptors.CalcMolDescriptors(mol)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {'fingerprint': None, 'descriptors': {}}\n",
    "    fingerprint = compute_fingerprint(mol)\n",
    "    descriptors = calculate_descriptors(smiles)\n",
    "    return {'fingerprint': fingerprint, 'descriptors': descriptors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "molecule_smiles는 building block들로 조합된 약물 분자.\n",
    "모델 학습을 위해서는 인코딩을 적용해 컴퓨터가 이해할 수 있는 형태로 변환해야함.\n",
    "\"\"\"\n",
    "\n",
    "OFFSET = 0\n",
    "CHUNK_SIZE = 1000\n",
    "train_parquet = f'{data_dir}/train.parquet'\n",
    "con = duckdb.connect()\n",
    "\n",
    "output_dir = f\"{data_dir}/preprocessed\"\n",
    "output_file = f\"{output_dir}/train.parquet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "num_workers = cpu_count()\n",
    "pool = Pool(num_workers)\n",
    "\n",
    "first_chunk = True\n",
    "while OFFSET < 3000:\n",
    "    chunk = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM parquet_scan('{train_parquet}')\n",
    "    LIMIT {CHUNK_SIZE} OFFSET {OFFSET}\n",
    "    \"\"\").fetch_df()\n",
    "\n",
    "    if chunk.empty:\n",
    "        break\n",
    "\n",
    "    smiles_list = chunk['molecule_smiles'].tolist()\n",
    "    \n",
    "    ## 병렬로 데이터 처리\n",
    "    results = pool.map(process_row, smiles_list)\n",
    "\n",
    "    ## 결과를 데이터프레임으로 변환\n",
    "    fingerprints = [result['fingerprint'] for result in results]\n",
    "    descriptors_list = [result['descriptors'] for result in results]\n",
    "    \n",
    "    chunk['fingerprints'] = fingerprints\n",
    "    descriptor_df = pd.DataFrame(descriptors_list)\n",
    "    excluded_descriptors = descriptor_df.columns[descriptor_df.isna().any()].tolist()\n",
    "    descriptor_df.drop(columns=excluded_descriptors, inplace=True)\n",
    "    used_descriptor = descriptor_df.columns.tolist()\n",
    "\n",
    "    if first_chunk:\n",
    "        print(f\"제외된 descriptors: {excluded_descriptors}\")\n",
    "        print(f\"사용된 descriptors: {used_descriptor}\")\n",
    "\n",
    "    chunk = pd.concat([chunk, descriptor_df], axis=1)\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "\n",
    "    if first_chunk:\n",
    "        writer = pq.ParquetWriter(output_file, table.schema)\n",
    "        first_chunk = False\n",
    "\n",
    "    writer.write_table(table)\n",
    "    print(f\"Processed offset: {OFFSET} saved to {output_file}\")\n",
    "    OFFSET += CHUNK_SIZE\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "writer.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(output_file, engine='pyarrow')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Target Protein Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_dicts = {\"sEH\": \"P34913\", \"BRD4\": \"O60885\", \"HSA\": \"P02768\"}\n",
    "output_dir = f\"{data_dir}/protein_desc\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def get_protein_sequence(uniprot_id):\n",
    "    url = f\"https://www.uniprot.org/uniprot/{uniprot_id}.fasta\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        response_text = response.text\n",
    "        lines = response_text.splitlines()\n",
    "        seq = \"\".join(lines[1:])\n",
    "        return seq\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "protein_seq_dicts = {}\n",
    "for protein_name, uniprot_id in uniprot_dicts.items():\n",
    "    protein_sequence = get_protein_sequence(uniprot_id)\n",
    "    if protein_sequence:\n",
    "        protein_seq_dicts[protein_name] = protein_sequence\n",
    "    else:\n",
    "        print(f\"Failed to retrieve sequence for {protein_name} ({uniprot_id})\")\n",
    "\n",
    "ctd_features = []\n",
    "for protein_name, sequence in protein_seq_dicts.items():\n",
    "    protein_class = PyProtein(sequence)\n",
    "    CTD = protein_class.GetCTD()\n",
    "    CTD = {'protein_name': protein_name, **CTD}\n",
    "    ctd_features.append(CTD)\n",
    "\n",
    "ctd_df = pd.DataFrame(ctd_features)\n",
    "ctd_df = ctd_df[['protein_name'] + [col for col in ctd_df.columns if col != 'protein_name']]  # Ensure 'protein_name' is the first column\n",
    "ctd_df.to_csv(f\"{output_dir}/protein_descriptors.csv\", index=False)\n",
    "print(ctd_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.전체 데이터에 대한 FingerPrint, Descriptor 계산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## molecule을 Morgan FingerPrint로 변환.\n",
    "def compute_fingerprint(mol):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    return BitVectToText(fp)  # Convert to BitString for storage\n",
    "\n",
    "\n",
    "## molecule로부터 descriptor 계산.\n",
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {}\n",
    "    descriptors = Descriptors.CalcMolDescriptors(mol)\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def process_row(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {'fingerprint': None, 'descriptors': {}}\n",
    "    fingerprint = compute_fingerprint(mol)\n",
    "    descriptors = calculate_descriptors(smiles)\n",
    "    return {'fingerprint': fingerprint, 'descriptors': descriptors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSET = 0\n",
    "CHUNK_SIZE = 10000\n",
    "train_parquet = f'{data_dir}/train.parquet'\n",
    "con = duckdb.connect()\n",
    "\n",
    "output_dir = f\"{data_dir}/preprocessed\"\n",
    "output_file = f\"{output_dir}/train.parquet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "ctd_df = pd.read_csv(f\"{data_dir}/protein_desc/protein_descriptors.csv\")\n",
    "\n",
    "num_workers = cpu_count()\n",
    "pool = Pool(num_workers)\n",
    "\n",
    "first_chunk = True\n",
    "# while OFFSET < 20000:\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    chunk = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM parquet_scan('{train_parquet}')\n",
    "    LIMIT {CHUNK_SIZE} OFFSET {OFFSET}\n",
    "    \"\"\").fetch_df()\n",
    "\n",
    "    if chunk.empty:\n",
    "        break\n",
    "\n",
    "    smiles_list = chunk['molecule_smiles'].tolist()\n",
    "    \n",
    "    # 병렬로 데이터 처리\n",
    "    results = pool.map(process_row, smiles_list)\n",
    "\n",
    "    # 결과를 데이터프레임으로 변환\n",
    "    fingerprints = [result['fingerprint'] for result in results]\n",
    "    descriptors_list = [result['descriptors'] for result in results]\n",
    "    \n",
    "    chunk['fingerprints'] = fingerprints\n",
    "    descriptor_df = pd.DataFrame(descriptors_list)\n",
    "    excluded_descriptors = descriptor_df.columns[descriptor_df.isna().any()].tolist()\n",
    "    descriptor_df.drop(columns=excluded_descriptors, inplace=True)\n",
    "    used_descriptor = descriptor_df.columns.tolist()\n",
    "\n",
    "    if first_chunk:\n",
    "        print(f\"제외된 descriptors: {excluded_descriptors}\")\n",
    "        print(f\"사용된 descriptors: {used_descriptor}\")\n",
    "\n",
    "    # CTD 데이터 병합 (protein_name 기준)\n",
    "    merged_chunk = pd.merge(chunk, ctd_df, on='protein_name', how='left')\n",
    "    merged_chunk = pd.concat([merged_chunk, descriptor_df], axis=1)\n",
    "    \n",
    "    table = pa.Table.from_pandas(merged_chunk)\n",
    "\n",
    "    if first_chunk:\n",
    "        writer = pq.ParquetWriter(output_file, table.schema)\n",
    "        first_chunk = False\n",
    "\n",
    "    writer.write_table(table)\n",
    "    OFFSET += CHUNK_SIZE\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processed offset: {OFFSET} saved to {output_file}. Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "writer.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.binds=1인 데이터의 총량으로 10 fold 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## molecule을 Morgan FingerPrint로 변환.\n",
    "def compute_fingerprint(mol):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    return BitVectToText(fp)  # Convert to BitString for storage\n",
    "\n",
    "\n",
    "## molecule로부터 descriptor 계산.\n",
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {}\n",
    "    descriptors = Descriptors.CalcMolDescriptors(mol)\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def process_row(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {'fingerprint': None, 'descriptors': {}}\n",
    "    fingerprint = compute_fingerprint(mol)\n",
    "    descriptors = calculate_descriptors(smiles)\n",
    "    return {'fingerprint': fingerprint, 'descriptors': descriptors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "## binds=0인 데이터만 load\n",
    "count_binds_0 = con.query(f\"\"\"SELECT COUNT(*) \n",
    "                              FROM parquet_scan('{train_parquet}') \n",
    "                              WHERE binds = 0\"\"\").fetchone()[0]\n",
    "print(f\"Total binds=0 : {count_binds_0}\")\n",
    "\n",
    "## binds=1인 데이터만 load\n",
    "count_binds_1 = con.query(f\"\"\"SELECT COUNT(*) \n",
    "                              FROM parquet_scan('{train_parquet}') \n",
    "                              WHERE binds = 1\"\"\").fetchone()[0]\n",
    "print(f\"Total binds=1 : {count_binds_1}\")\n",
    "\n",
    "## 전체 데이터 수\n",
    "total_count = count_binds_0 + count_binds_1\n",
    "print(f\"Total data : {total_count}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_chunk_size = count_binds_0 // 1800\n",
    "b1_chunk_size = count_binds_1 // 10\n",
    "print(b0_chunk_size)\n",
    "print(b1_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{data_dir}/preprocessed\"\n",
    "ctd_df = pd.read_csv(f\"{data_dir}/protein_desc/protein_descriptors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_per_data_spliter(binds, chunk_size, parquet_path, ctd_df, process_row, output_path, max_chunk=0):\n",
    "    offset = 0\n",
    "    chunk_number = 0\n",
    "\n",
    "    num_workers = cpu_count()\n",
    "    pool = Pool(num_workers)\n",
    "\n",
    "    fingerprints_folder = os.path.join(output_path, 'fingerprints')\n",
    "    descriptors_folder = os.path.join(output_path, 'descriptors')\n",
    "    os.makedirs(fingerprints_folder, exist_ok=True)\n",
    "    os.makedirs(descriptors_folder, exist_ok=True)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoder.fit(ctd_df[['protein_name']])  # ctd_df를 이용해 fit\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    while True:\n",
    "        if max_chunk and chunk_number == max_chunk:\n",
    "            break\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Phase 1: 데이터 읽기\n",
    "        phase1_start = time.time()\n",
    "        chunk = con.execute(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM parquet_scan('{parquet_path}')\n",
    "        WHERE binds = {binds}\n",
    "        LIMIT {chunk_size}\n",
    "        OFFSET {offset}\n",
    "        \"\"\").fetch_df()\n",
    "        phase1_end = time.time()\n",
    "        print(f\"Phase 1: Data read - Time taken: {phase1_end - phase1_start:.2f} seconds\")\n",
    "\n",
    "        if chunk.empty:\n",
    "            break\n",
    "\n",
    "        smiles_list = chunk['molecule_smiles'].tolist()\n",
    "        protein_names = chunk['protein_name'].tolist()\n",
    "\n",
    "        # Phase 2: 병렬 데이터 처리\n",
    "        phase2_start = time.time()\n",
    "        results = pool.map(process_row, smiles_list)\n",
    "        phase2_end = time.time()\n",
    "        print(f\"Phase 2: Parallel processing - Time taken: {phase2_end - phase2_start:.2f} seconds\")\n",
    "\n",
    "        # Phase 3: 결과를 데이터프레임으로 변환\n",
    "        phase3_start = time.time()\n",
    "        fingerprints = [result['fingerprint'] for result in results]\n",
    "        descriptors_list = [result['descriptors'] for result in results]\n",
    "        phase3_end = time.time()\n",
    "        print(f\"Phase 3: Convert results to DataFrame - Time taken: {phase3_end - phase3_start:.2f} seconds\")\n",
    "\n",
    "        # Phase 4: fingerprints 저장\n",
    "        phase4_start = time.time()\n",
    "        fingerprints_df = pd.DataFrame(fingerprints)\n",
    "        fingerprints_df['protein_name'] = protein_names\n",
    "        fingerprints_df['binds'] = binds\n",
    "\n",
    "        # CTD 데이터 병합 (fingerprints와 연결)\n",
    "        merged_fingerprints = pd.merge(fingerprints_df, ctd_df, on='protein_name', how='left')\n",
    "\n",
    "        # 원핫 인코딩 적용 (fingerprints)\n",
    "        protein_name_encoded_fingerprints = encoder.transform(merged_fingerprints[['protein_name']])\n",
    "        protein_name_encoded_df_fingerprints = pd.DataFrame(protein_name_encoded_fingerprints, columns=encoder.get_feature_names_out(['protein_name']))\n",
    "        merged_fingerprints = pd.concat([merged_fingerprints, protein_name_encoded_df_fingerprints], axis=1).drop(columns=['protein_name'])\n",
    "\n",
    "        # 제거할 컬럼이 존재하는지 확인 후 삭제\n",
    "        columns_to_drop = ['buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles', 'molecule_smiles']\n",
    "        merged_fingerprints = merged_fingerprints.drop(columns=[col for col in columns_to_drop if col in merged_fingerprints.columns])\n",
    "\n",
    "        fingerprints_file = os.path.join(fingerprints_folder, f'fingerprints_b{binds}_chunk_{chunk_number}.csv')\n",
    "        merged_fingerprints.to_csv(fingerprints_file, index=False)\n",
    "        phase4_end = time.time()\n",
    "        print(f\"Phase 4: Save fingerprints - Time taken: {phase4_end - phase4_start:.2f} seconds\")\n",
    "        print(fingerprints_file)\n",
    "\n",
    "        # Phase 5: descriptors 데이터프레임 생성 및 protein_name 추가\n",
    "        phase5_start = time.time()\n",
    "        descriptor_df = pd.DataFrame(descriptors_list)\n",
    "        descriptor_df['protein_name'] = protein_names\n",
    "        descriptor_df['binds'] = binds\n",
    "        excluded_descriptors = descriptor_df.columns[descriptor_df.isna().any()].tolist()\n",
    "        descriptor_df.drop(columns=excluded_descriptors, inplace=True)\n",
    "        used_descriptor = descriptor_df.columns.tolist()\n",
    "\n",
    "        if chunk_number == 0:\n",
    "            print(f\"제외된 descriptors: {excluded_descriptors}\")\n",
    "            print(f\"사용된 descriptors: {used_descriptor}\")\n",
    "\n",
    "        # CTD 데이터 병합 (descriptor와 연결)\n",
    "        merged_descriptors = pd.merge(descriptor_df, ctd_df, on='protein_name', how='left')\n",
    "\n",
    "        # 원핫 인코딩 적용 (descriptors)\n",
    "        protein_name_encoded_descriptors = encoder.transform(merged_descriptors[['protein_name']])\n",
    "        protein_name_encoded_df_descriptors = pd.DataFrame(protein_name_encoded_descriptors, columns=encoder.get_feature_names_out(['protein_name']))\n",
    "        merged_descriptors = pd.concat([merged_descriptors, protein_name_encoded_df_descriptors], axis=1).drop(columns=['protein_name'])\n",
    "\n",
    "        descriptors_file = os.path.join(descriptors_folder, f'descriptors_b{binds}_chunk_{chunk_number}.csv')\n",
    "        merged_descriptors.to_csv(descriptors_file, index=False)\n",
    "        phase5_end = time.time()\n",
    "        print(f\"Phase 5: Save descriptors - Time taken: {phase5_end - phase5_start:.2f} seconds\")\n",
    "        print(descriptors_file)\n",
    "\n",
    "        offset += chunk_size\n",
    "        chunk_number += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Processed chunk: {chunk_number}, offset: {offset}, Total Time taken: {elapsed_time:.2f} seconds \\n\")\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_per_data_spliter(binds=1, \n",
    "                       chunk_size=b1_chunk_size, \n",
    "                       parquet_path=train_parquet, \n",
    "                       ctd_df=ctd_df, \n",
    "                       process_row=process_row, \n",
    "                       output_path=f\"{output_dir}/binds1\",\n",
    "                       max_chunk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_per_data_spliter(binds=0, \n",
    "                       chunk_size=b0_chunk_size, \n",
    "                       parquet_path=train_parquet, \n",
    "                       ctd_df=ctd_df, \n",
    "                       process_row=process_row, \n",
    "                       output_path=f\"{output_dir}/binds0\",\n",
    "                       max_chunk=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## molecule을 Morgan FingerPrint로 변환.\n",
    "def compute_fingerprint(mol):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    return BitVectToText(fp)  # Convert to BitString for storage\n",
    "\n",
    "\n",
    "## molecule로부터 descriptor 계산.\n",
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {}\n",
    "    descriptors = Descriptors.CalcMolDescriptors(mol)\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def process_row(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {'fingerprint': None, 'descriptors': {}}\n",
    "    fingerprint = compute_fingerprint(mol)\n",
    "    descriptors = calculate_descriptors(smiles)\n",
    "    return {'fingerprint': fingerprint, 'descriptors': descriptors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "chunk_size = 10000\n",
    "con = duckdb.connect()\n",
    "\n",
    "output_dir = f\"{data_dir}/for_test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "ctd_df = pd.read_csv(f\"{data_dir}/protein_desc/protein_descriptors.csv\")\n",
    "\n",
    "num_workers = cpu_count()\n",
    "pool = Pool(num_workers)\n",
    "\n",
    "fingerprints_folder = os.path.join(output_dir, 'fingerprints')\n",
    "descriptors_folder = os.path.join(output_dir, 'descriptors')\n",
    "os.makedirs(fingerprints_folder, exist_ok=True)\n",
    "os.makedirs(descriptors_folder, exist_ok=True)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(ctd_df[['protein_name']])  # ctd_df를 이용해 fit\n",
    "\n",
    "chunk_number = 0\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    phase1_start = time.time()\n",
    "    chunk = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM parquet_scan('{test_parquet}')\n",
    "    LIMIT {chunk_size}\n",
    "    OFFSET {offset}\n",
    "    \"\"\").fetch_df()\n",
    "    phase1_end = time.time()\n",
    "    print(f\"Phase 1: Data read - Time taken: {phase1_end - phase1_start:.2f} seconds\")\n",
    "\n",
    "    if chunk.empty:\n",
    "        break\n",
    "\n",
    "    smiles_list = chunk['molecule_smiles'].tolist()\n",
    "    protein_names = chunk['protein_name'].tolist()\n",
    "    ids = chunk['id'].tolist()\n",
    "    buildingblock1_smiles = chunk['buildingblock1_smiles'].tolist()\n",
    "    buildingblock2_smiles = chunk['buildingblock2_smiles'].tolist()\n",
    "    buildingblock3_smiles = chunk['buildingblock3_smiles'].tolist()\n",
    "\n",
    "    # Phase 2: 병렬 데이터 처리\n",
    "    phase2_start = time.time()\n",
    "    results = pool.map(process_row, smiles_list)\n",
    "    phase2_end = time.time()\n",
    "    print(f\"Phase 2: Parallel processing - Time taken: {phase2_end - phase2_start:.2f} seconds\")\n",
    "\n",
    "    # Phase 3: 결과를 데이터프레임으로 변환\n",
    "    phase3_start = time.time()\n",
    "    fingerprints = [result['fingerprint'] for result in results]\n",
    "    descriptors_list = [result['descriptors'] for result in results]\n",
    "    phase3_end = time.time()\n",
    "    print(f\"Phase 3: Convert results to DataFrame - Time taken: {phase3_end - phase3_start:.2f} seconds\")\n",
    "\n",
    "    # Phase 4: fingerprints 저장\n",
    "    phase4_start = time.time()\n",
    "    fingerprints_df = pd.DataFrame({'fingerprints': fingerprints, 'protein_name': protein_names, 'id': ids,\n",
    "                                    'buildingblock1_smiles': buildingblock1_smiles, 'buildingblock2_smiles': buildingblock2_smiles, 'buildingblock3_smiles': buildingblock3_smiles})\n",
    "\n",
    "    # CTD 데이터 병합 (fingerprints와 연결)\n",
    "    merged_fingerprints = pd.merge(fingerprints_df, ctd_df, on='protein_name', how='left')\n",
    "\n",
    "    # 원핫 인코딩 적용 (fingerprints)\n",
    "    protein_name_encoded_fingerprints = encoder.transform(merged_fingerprints[['protein_name']])\n",
    "    protein_name_encoded_df_fingerprints = pd.DataFrame(protein_name_encoded_fingerprints, columns=encoder.get_feature_names_out(['protein_name']))\n",
    "    merged_fingerprints = pd.concat([merged_fingerprints, protein_name_encoded_df_fingerprints], axis=1).drop(columns=['protein_name'])\n",
    "\n",
    "    # 기존 데이터를 불러와서 병합\n",
    "    if chunk_number > 0:\n",
    "        existing_fingerprints_file = os.path.join(fingerprints_folder, f'fingerprints_chunk_{chunk_number - 1}.csv')\n",
    "        existing_fingerprints_df = pd.read_csv(existing_fingerprints_file)\n",
    "        merged_fingerprints = pd.concat([existing_fingerprints_df, merged_fingerprints], ignore_index=True)\n",
    "\n",
    "    fingerprints_file = os.path.join(fingerprints_folder, f'fingerprints_chunk_{chunk_number}.csv')\n",
    "    merged_fingerprints.to_csv(fingerprints_file, index=False)\n",
    "    phase4_end = time.time()\n",
    "    print(f\"Phase 4: Save fingerprints - Time taken: {phase4_end - phase4_start:.2f} seconds\")\n",
    "    print(fingerprints_file)\n",
    "\n",
    "    # Phase 5: descriptors 데이터프레임 생성 및 protein_name 추가\n",
    "    phase5_start = time.time()\n",
    "    descriptor_df = pd.DataFrame(descriptors_list)\n",
    "    descriptor_df['protein_name'] = protein_names\n",
    "    descriptor_df['id'] = ids\n",
    "    descriptor_df['buildingblock1_smiles'] = buildingblock1_smiles\n",
    "    descriptor_df['buildingblock2_smiles'] = buildingblock2_smiles\n",
    "    descriptor_df['buildingblock3_smiles'] = buildingblock3_smiles\n",
    "\n",
    "    excluded_descriptors = descriptor_df.columns[descriptor_df.isna().any()].tolist()\n",
    "    descriptor_df.drop(columns=excluded_descriptors, inplace=True)\n",
    "    used_descriptor = descriptor_df.columns.tolist()\n",
    "\n",
    "    if chunk_number == 0:\n",
    "        print(f\"제외된 descriptors: {excluded_descriptors}\")\n",
    "        print(f\"사용된 descriptors: {used_descriptor}\")\n",
    "\n",
    "    # CTD 데이터 병합 (descriptor와 연결)\n",
    "    merged_descriptors = pd.merge(descriptor_df, ctd_df, on='protein_name', how='left')\n",
    "\n",
    "    # 원핫 인코딩 적용 (descriptors)\n",
    "    protein_name_encoded_descriptors = encoder.transform(merged_descriptors[['protein_name']])\n",
    "    protein_name_encoded_df_descriptors = pd.DataFrame(protein_name_encoded_descriptors, columns=encoder.get_feature_names_out(['protein_name']))\n",
    "    merged_descriptors = pd.concat([merged_descriptors, protein_name_encoded_df_descriptors], axis=1).drop(columns=['protein_name'])\n",
    "\n",
    "    # 기존 데이터를 불러와서 병합\n",
    "    if chunk_number > 0:\n",
    "        existing_descriptors_file = os.path.join(descriptors_folder, f'descriptors_chunk_{chunk_number - 1}.csv')\n",
    "        existing_descriptors_df = pd.read_csv(existing_descriptors_file)\n",
    "        merged_descriptors = pd.concat([existing_descriptors_df, merged_descriptors], ignore_index=True)\n",
    "\n",
    "    descriptors_file = os.path.join(descriptors_folder, f'descriptors_chunk_{chunk_number}.csv')\n",
    "    merged_descriptors.to_csv(descriptors_file, index=False)\n",
    "    phase5_end = time.time()\n",
    "    print(f\"Phase 5: Save descriptors - Time taken: {phase5_end - phase5_start:.2f} seconds\")\n",
    "    print(descriptors_file)\n",
    "\n",
    "    offset += chunk_size\n",
    "    chunk_number += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processed chunk: {chunk_number}, offset: {offset}, Total Time taken: {elapsed_time:.2f} seconds \\n\")\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
