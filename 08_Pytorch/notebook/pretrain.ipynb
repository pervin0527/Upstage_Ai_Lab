{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전이학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset](https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.backends import cudnn\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed_num):\n",
    "    torch.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed_all(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed_num)\n",
    "    \n",
    "random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models() # timm이 지원하는 모든 모델 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models('resnet*') # 쿼리를 통해 모델을 검색할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models('resnet50', pretrained=True) # resnet 모델 중 pretrained weight 가 있는 모델 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50', pretrained=True) # resnet50을 imagenet으로 pretrain한 모델 불러오기, 첫번째 모델로 불러옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.default_cfg # resnet50 모델의 기본 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model # 모델의 아키텍쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = timm.create_model('resnet50', pretrained = True, num_classes = 10) # 마지막 output class 개수 10개로 조정\n",
    "model2 # num class 를 임의로 조정하면 fc layer 의 weight가 초기화됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "cifar_transform = T.Compose([\n",
    "    T.ToTensor(), # 텐서 형식으로 변환\n",
    "])\n",
    "download_root = '/home/pervinco/Datasets/CIFAR10_DATASET'\n",
    "\n",
    "trainval_dataset = CIFAR10(download_root, transform=cifar_transform, train=True, download=True) # train dataset 다운로드\n",
    "test_dataset = CIFAR10(download_root, transform=cifar_transform, train=False, download=True) # test dataset 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num, valid_num = int(len(trainval_dataset) * 0.8), int(len(trainval_dataset) * 0.2) # 8 : 2 = train : valid\n",
    "print(\"Train dataset 개수 : \",train_num)\n",
    "print(\"Validation dataset 개수 : \",valid_num)\n",
    "train_dataset,val_dataset = torch.utils.data.random_split(trainval_dataset, [train_num, valid_num]) # train - valid set 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 배치사이즈 설정\n",
    "# 데이터로더 설정\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=False, num_workers = 8) # train dataloader 구성\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=False, num_workers = 8) # valid dataloader 구성\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=False, num_workers = 8) # test dataloader 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model 불러오기 (resnet50 불러오기)\n",
    "device = 'cuda:0' # gpu 설정\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes = 10).to(device) # 10개의 클래스 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 코드, evaluation 코드, training_loop 코드\n",
    "def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n",
    "  model.train()  # 모델을 학습 모드로 설정\n",
    "  train_loss = 0.0\n",
    "  train_accuracy = 0\n",
    "\n",
    "  tbar = tqdm(dataloader)\n",
    "  for images, labels in tbar:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # 순전파\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # 역전파 및 가중치 업데이트\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # 손실과 정확도 계산\n",
    "      train_loss += loss.item()\n",
    "      # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "      # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "      tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "  # 에폭별 학습 결과 출력\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "  return model, train_loss, train_accuracy\n",
    "\n",
    "def evaluation(model, dataloader, val_dataset, criterion, device, epoch, num_epochs):\n",
    "  model.eval()  # 모델을 평가 모드로 설정\n",
    "  valid_loss = 0.0\n",
    "  valid_accuracy = 0\n",
    "\n",
    "  with torch.no_grad(): # model의 업데이트 막기\n",
    "      tbar = tqdm(dataloader)\n",
    "      for images, labels in tbar:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          # 순전파\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          # 손실과 정확도 계산\n",
    "          valid_loss += loss.item()\n",
    "          # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "          # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "          tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "  valid_loss = valid_loss / len(dataloader)\n",
    "  valid_accuracy = valid_accuracy / len(val_dataset)\n",
    "\n",
    "  return model, valid_loss, valid_accuracy\n",
    "\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "          valid_max_accuracy = valid_accuracy\n",
    "\n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"./model_{model_name}.pt\")\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 전체 fine tuning\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "scores = dict()\n",
    "model_name = 'exp1'\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, val_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 layer 만 fine tuning\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "scores = dict()\n",
    "model_name = 'exp2'\n",
    "\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes= 10).to(device)\n",
    "\n",
    "for para in model.parameters(): # 모든 layer freeze 하기\n",
    "    para.requires_grad = False\n",
    "for para in model.fc.parameters(): # fc layer 만 학습하기\n",
    "    para.requires_grad = True\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, val_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate 에 따른 결과 비교\n",
    "\n",
    "model3 = timm.create_model('resnet50', pretrained=True, num_classes= 10).to(device)\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "scores = dict()\n",
    "model_name = 'exp3'\n",
    "\n",
    "lr = 1e-1 # learning rate 높게 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model3.parameters(), lr = lr)\n",
    "model, valid_max_accuracy = training_loop(model3, train_dataloader, val_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate 에 따른 결과 비교\n",
    "\n",
    "model4 = timm.create_model('resnet50', pretrained=True, num_classes= 10).to(device)\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "scores = dict()\n",
    "model_name = 'exp4'\n",
    "\n",
    "lr = 1e-5 # 기존보다 더 작게 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model4.parameters(), lr = lr)\n",
    "model, valid_max_accuracy = training_loop(model4, train_dataloader, val_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import random\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed_num):\n",
    "    torch.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed_all(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed_num)\n",
    "\n",
    "random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN 에서 갖고 오기\n",
    "data_csv = pd.read_csv('data/medium_data.csv')\n",
    "data = data_csv['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        seq = self.make_sequence(self.data, self.vocab, self.tokenizer) # next word prediction을 하기 위한 형태로 변환\n",
    "        self.seq = self.pre_zeropadding(seq, self.max_len) # zero padding으로 채워줌\n",
    "        self.X = torch.tensor(self.seq[:,:-1])\n",
    "        self.label = torch.tensor(self.seq[:,-1])\n",
    "\n",
    "    def make_sequence(self, data, vocab, tokenizer):\n",
    "        seq = []\n",
    "        for i in data:\n",
    "            token_id = vocab.lookup_indices(tokenizer(i))\n",
    "            for j in range(1, len(token_id)):\n",
    "                sequence = token_id[:j+1]\n",
    "                seq.append(sequence)\n",
    "        return seq\n",
    "\n",
    "    def pre_zeropadding(self, seq, max_len): # max_len 길이에 맞춰서 0 으로 padding 처리 (앞부분에 padding 처리)\n",
    "        return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n",
    "\n",
    "\n",
    "    def __len__(self): # dataset의 전체 길이 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx): # dataset 접근\n",
    "        X = self.X[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행합니다.\n",
    "    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n",
    "    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(cleaning_text, data))\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, data))\n",
    "vocab.insert_token('<pad>',0)\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set과 validation set, test set을 각각 나눕니다. 8 : 1 : 1 의 비율로 나눕니다.\n",
    "train, test = train_test_split(data, test_size = .2, random_state = 42)\n",
    "val, test = train_test_split(test, test_size = .5, random_state = 42)\n",
    "print(\"Train 개수: \", len(train))\n",
    "print(\"Validation 개수: \", len(val))\n",
    "print(\"Test 개수: \", len(test))\n",
    "\n",
    "train_dataset = CustomDataset(train, vocab, tokenizer, max_len)\n",
    "valid_dataset = CustomDataset(val, vocab, tokenizer, max_len)\n",
    "test_dataset = CustomDataset(test, vocab, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True) # batch_first=True는 입력의 첫 번째 차원이 batch 크기임을 나타냅니다.\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size) ## 마지막 RNN cell이 출력한 값을 입력으로 받아서 vocab_size 차원의 벡터를 출력한다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "           x: [batch_size, seq_len]\n",
    "        OUTPUT:\n",
    "           output: [batch_size, vocab_size]\n",
    "        '''\n",
    "        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n",
    "\n",
    "        # 첫 번째 리턴값인 output은 모든 time step의 hidden state를 포함한 출력입니다.\n",
    "        # 두 번째 리턴값인 h_0 는 마지막 time step의 hidden state를 의미합니다.\n",
    "        output, h_0 = self.rnn(x) # output: [batch_size, seq_len, hidden_dim] / h_0: [1, batch_size, hidden_dim]\n",
    "        return self.fc(output[:,-1,:]) ## [batch_size, vocab_size] 가장 마지막 hidden state만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 코드, evaluation 코드, training_loop 코드\n",
    "def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    tbar = tqdm(dataloader)\n",
    "    for texts, labels in tbar:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 순전파\n",
    "\n",
    "        outputs = model(texts)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실과 정확도 계산\n",
    "        train_loss += loss.item()\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "        tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 에폭별 학습 결과 출력\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "    return model, train_loss, train_accuracy\n",
    "\n",
    "def evaluation(model, dataloader, valid_dataset, criterion, device, epoch, num_epochs):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # model의 업데이트 막기\n",
    "        tbar = tqdm(dataloader)\n",
    "        for texts, labels in tbar:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 순전파\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 손실과 정확도 계산\n",
    "            valid_loss += loss.item()\n",
    "            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # _, true_labels = torch.max(labels, dim=1)\n",
    "            valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "            # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "    valid_loss = valid_loss / len(dataloader)\n",
    "    valid_accuracy = valid_accuracy / len(valid_dataset)\n",
    "\n",
    "    return model, valid_loss, valid_accuracy\n",
    "\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "            valid_max_accuracy = valid_accuracy\n",
    "\n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"./model_{model_name}.pt\")\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "patience = 3\n",
    "model_name = 'RNN'\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 512\n",
    "hidden_size = 256\n",
    "model = RNN(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model_RNN.pt\")) # 모델 불러오기\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "total_labels = []\n",
    "total_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_dataloader):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(texts)\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_preds.extend(predicted.detach().cpu().tolist())\n",
    "        total_labels.extend(labels.tolist())\n",
    "\n",
    "total_preds = np.array(total_preds)\n",
    "total_labels = np.array(total_labels)\n",
    "nwp_rnn_acc = accuracy_score(total_labels, total_preds) # 정확도 계산\n",
    "print(\"Next word prediction RNN model accuracy : \", nwp_rnn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "           x: [batch_size, seq_len]\n",
    "        OUTPUT:\n",
    "           output: [batch_size, vocab_size]\n",
    "        '''\n",
    "        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n",
    "        # lstm에선 마지막 time step에서의 cell state (c_n)도 반환합니다.\n",
    "        output, h_0, c_0 = self.lstm(x) # output : [batch_size, seq_len, hidden_dim] # h_n: [1, batch_size, hidden_dim] # c_n: [1, batch_size, hidden_dim]\n",
    "        return self.fc(output[:,-1,:]) # [batch_size, num_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "patience = 3\n",
    "model_name = 'LSTM'\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 512\n",
    "hidden_size = 256\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model_LSTM.pt\")) # 모델 불러오기\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "total_labels = []\n",
    "total_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_dataloader):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(texts)\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_preds.extend(predicted.detach().cpu().tolist())\n",
    "        total_labels.extend(labels.tolist())\n",
    "\n",
    "total_preds = np.array(total_preds)\n",
    "total_labels = np.array(total_labels)\n",
    "nwp_lstm_acc = accuracy_score(total_labels, total_preds) # 정확도 계산\n",
    "print(\"Next word prediction LSTM model accuracy : \", nwp_lstm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "           x: [batch_size, seq_len]\n",
    "        OUTPUT:\n",
    "           output: [batch_size, vocab_size]\n",
    "        '''\n",
    "        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n",
    "        output, h_0 = self.gru(x) # output : [batch_size, seq_len, hidden_dim] # h_0 는 [1, batch_size, hidden_dim]\n",
    "        return self.fc(output[:,-1,:]) # [batch_size, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "patience = 3\n",
    "model_name = 'GRU'\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 512\n",
    "hidden_size = 256\n",
    "model = GRU(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model_GRU.pt\")) # 모델 불러오기\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "total_labels = []\n",
    "total_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_dataloader):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(texts)\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_preds.extend(predicted.detach().cpu().tolist())\n",
    "        total_labels.extend(labels.tolist())\n",
    "\n",
    "total_preds = np.array(total_preds)\n",
    "total_labels = np.array(total_labels)\n",
    "nwp_gru_acc = accuracy_score(total_labels, total_preds) # 정확도 계산\n",
    "print(\"Next word prediction GRU model accuracy : \", nwp_gru_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
