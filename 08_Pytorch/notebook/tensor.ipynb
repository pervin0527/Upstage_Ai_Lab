{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1],\n",
       "                       [1, 0]]),\n",
       "       values=tensor([2., 3.]),\n",
       "       size=(2, 2), nnz=2, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0, 2.], [3, 0]])\n",
    "\n",
    "a.to_sparse() # COO Sparse tensor 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 1]]),\n",
      "       values=tensor([4, 5, 6]),\n",
      "       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n",
      "\n",
      "\n",
      "tensor([[0, 0, 4],\n",
      "        [5, 6, 0]])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.tensor([[0, 1, 1],[2, 0, 1]]) # 0이 아닌 값의 (index, column) pairs\n",
    "values = torch.tensor([4, 5, 6]) # 0 이 아닌 값의 values, values의 사이\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices = indices, values = values, size=(2, 3)) # (2,3)의 sparse tensor\n",
    "\n",
    "print(sparse_tensor)\n",
    "print('\\n')\n",
    "print(sparse_tensor.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape :  torch.Size([2, 4])\n",
      "tensor([[0, 0, 4, 3],\n",
      "        [5, 6, 0, 0]])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2305940/418198422.py:7: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971093/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  t.to_sparse_csr()  # Dense Tensor를 CSR Sparse Tensor 형식으로 변환\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([0, 2, 4]),\n",
       "       col_indices=tensor([2, 3, 0, 1]),\n",
       "       values=tensor([4, 3, 5, 6]), size=(2, 4), nnz=4,\n",
       "       layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0, 0, 4, 3], [5, 6, 0, 0]])\n",
    "print(\"Shape : \", t.size())\n",
    "print(t)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "t.to_sparse_csr()  # Dense Tensor를 CSR Sparse Tensor 형식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape :  torch.Size([2, 4])\n",
      "tensor([[0, 0, 4, 3],\n",
      "        [5, 6, 0, 0]])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(ccol_indices=tensor([0, 1, 2, 3, 4]),\n",
       "       row_indices=tensor([1, 1, 0, 0]),\n",
       "       values=tensor([5, 6, 4, 3]), size=(2, 4), nnz=4,\n",
       "       layout=torch.sparse_csc)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0, 0, 4, 3], [5, 6, 0, 0]])\n",
    "print(\"Shape : \", t.size())\n",
    "print(t)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "t.to_sparse_csc()  # Dense Tensor 를 CSC Spare tensor 형식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(crow_indices=tensor([0, 2, 2]),\n",
      "       col_indices=tensor([0, 1]),\n",
      "       values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csr)\n",
      "\n",
      "\n",
      "tensor([[1, 2],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "crow_indices = torch.tensor([0, 2, 2]) # 0이 아닌 행의 위치 (첫번쨰는 무조건 0), 즉 row_pointer\n",
    "col_indices = torch.tensor([0, 1]) # 0이 아닌 열의 위치\n",
    "values = torch.tensor([1, 2]) # 0이 아닌 값\n",
    "csr = torch.sparse_csr_tensor(crow_indices = crow_indices, col_indices = col_indices, values = values)\n",
    "\n",
    "print(csr)\n",
    "print('\\n')\n",
    "print(csr.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(ccol_indices=tensor([0, 2, 2]),\n",
      "       row_indices=tensor([0, 1]),\n",
      "       values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csc)\n",
      "\n",
      "\n",
      "tensor([[1, 0],\n",
      "        [2, 0]])\n"
     ]
    }
   ],
   "source": [
    "ccol_indices = torch.tensor([0, 2, 2]) # 0이 아닌 열의 위치 (첫번쨰는 무조건 0), 즉 column_pointer\n",
    "row_indices = torch.tensor([0, 1]) # 0이 아닌 행의 위치\n",
    "values = torch.tensor([1, 2]) # 0이 아닌 값\n",
    "csc = torch.sparse_csc_tensor(ccol_indices = ccol_indices, row_indices = row_indices, values = values)\n",
    "\n",
    "print(csc)\n",
    "print('\\n')\n",
    "print(csc.to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# MNIST 분류를 위한 CNN 모델\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_ratio):\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5),  # [BATCH_SIZE, 1, 28, 28] -> [BATCH_SIZE, 16, 24, 24]\n",
    "            nn.ReLU(),  # ReLU 활성화 함수 적용\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5), # [BATCH_SIZE, 16, 24, 24] -> [BATCH_SIZE, 32, 20, 20]\n",
    "            nn.ReLU(),  # ReLU 활성화 함수 적용\n",
    "            nn.MaxPool2d(kernel_size=2), # [BATCH_SIZE, 32, 20, 20] -> [BATCH_SIZE, 32, 10, 10]\n",
    "            nn.Dropout(dropout_ratio),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5), # [BATCH_SIZE, 32, 10, 10] -> [BATCH_SIZE, 64, 6, 6]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # 크기를 1/2로 줄입니다. [BATCH_SIZE, 64, 6, 6] -> [BATCH_SIZE, 64, 3, 3]\n",
    "            nn.Dropout(dropout_ratio),\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Linear(64*3*3, self.num_classes) # [BATCH_SIZE, 64*3*3] -> [BATCH_SIZE, num_classes]\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Input and Output Summary\n",
    "\n",
    "        Input :\n",
    "            x : [batch_size, channel, height, width]\n",
    "        Output :\n",
    "            pred : [batch_size, num_classes]\n",
    "        '''\n",
    "        out = self.layer(x) # self.layer에 정의한 Sequential의 연산을 차례대로 다 실행합니다. [BATCH_SIZE, 64, 3, 3]\n",
    "        out = out.view(x.size(0), -1)  # [BATCH_SIZE, 64*3*3]\n",
    "        pred = self.fc_layer(out) # [BATCH_SIZE, num_classes]\n",
    "        pred = self.softmax(pred) # [BATCH_SIZE, num_classes]\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def weight_initialization(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# training 코드, evaluation 코드, training_loop 코드\n",
    "def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n",
    "  model.train()  # 모델을 학습 모드로 설정\n",
    "  train_loss = 0.0\n",
    "  train_accuracy = 0\n",
    "\n",
    "  tbar = tqdm(dataloader)\n",
    "  for images, labels in tbar:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # 순전파\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # 역전파 및 가중치 업데이트\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # 손실과 정확도 계산\n",
    "      train_loss += loss.item()\n",
    "      # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "      # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "      tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "  # 에폭별 학습 결과 출력\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "  return model, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, val_dataset, criterion, device, epoch, num_epochs):\n",
    "  model.eval()  # 모델을 평가 모드로 설정\n",
    "  valid_loss = 0.0\n",
    "  valid_accuracy = 0\n",
    "\n",
    "  with torch.no_grad(): # model의 업데이트 막기\n",
    "      tbar = tqdm(dataloader)\n",
    "      for images, labels in tbar:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          # 순전파\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          # 손실과 정확도 계산\n",
    "          valid_loss += loss.item()\n",
    "          # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "          # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "          tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "  valid_loss = valid_loss / len(dataloader)\n",
    "  valid_accuracy = valid_accuracy / len(val_dataset)\n",
    "\n",
    "  return model, valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "          valid_max_accuracy = valid_accuracy\n",
    "\n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"./model_{model_name}.pt\")\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, valid_max_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        seq = self.make_sequence(self.data, self.vocab, self.tokenizer) # next word prediction을 하기 위한 형태로 변환\n",
    "        self.seq = self.pre_zeropadding(seq, self.max_len) # zero padding으로 채워줌\n",
    "        self.X = torch.tensor(self.seq[:,:-1])\n",
    "        self.label = torch.tensor(self.seq[:,-1])\n",
    "\n",
    "    def make_sequence(self, data, vocab, tokenizer):\n",
    "        seq = []\n",
    "        for i in data:\n",
    "            token_id = vocab.lookup_indices(tokenizer(i))\n",
    "            for j in range(1, len(token_id)):\n",
    "                sequence = token_id[:j+1]\n",
    "                seq.append(sequence)\n",
    "        return seq\n",
    "\n",
    "    def pre_zeropadding(self, seq, max_len): # max_len 길이에 맞춰서 0 으로 padding 처리 (앞부분에 padding 처리)\n",
    "        return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n",
    "\n",
    "\n",
    "    def __len__(self): # dataset의 전체 길이 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx): # dataset 접근\n",
    "        X = self.X[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        return X, label\n",
    "    \n",
    "\n",
    "def cleaning_text(text):\n",
    "    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행합니다.\n",
    "    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n",
    "    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchtext\n",
    "\n",
    "# DNN 에서 갖고 오기\n",
    "data_csv = pd.read_csv('data/medium_data.csv')\n",
    "data = data_csv['title']\n",
    "\n",
    "data = list(map(cleaning_text, data))\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, data))\n",
    "vocab.insert_token('<pad>',0)\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set과 validation set, test set을 각각 나눕니다. 8 : 1 : 1 의 비율로 나눕니다.\n",
    "train, test = train_test_split(data, test_size = .2, random_state = 42)\n",
    "val, test = train_test_split(test, test_size = .5, random_state = 42)\n",
    "print(\"Train 개수: \", len(train))\n",
    "print(\"Validation 개수: \", len(val))\n",
    "print(\"Test 개수: \", len(test))\n",
    "\n",
    "train_dataset = CustomDataset(train, vocab, tokenizer, max_len)\n",
    "valid_dataset = CustomDataset(val, vocab, tokenizer, max_len)\n",
    "test_dataset = CustomDataset(test, vocab, tokenizer, max_len)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True) # batch_first=True는 입력의 첫 번째 차원이 batch 크기임을 나타냅니다.\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size) ## 마지막 RNN cell이 출력한 값을 입력으로 받아서 vocab_size 차원의 벡터를 출력한다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "           x: [batch_size, seq_len]\n",
    "        OUTPUT:\n",
    "           output: [batch_size, vocab_size]\n",
    "        '''\n",
    "        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n",
    "\n",
    "        # 첫 번째 리턴값인 output은 모든 time step의 hidden state를 포함한 출력입니다.\n",
    "        # 두 번째 리턴값인 h_0 는 마지막 time step의 hidden state를 의미합니다.\n",
    "        output, h_0 = self.rnn(x) # output: [batch_size, seq_len, hidden_dim] / h_0: [1, batch_size, hidden_dim]\n",
    "        return self.fc(output[:,-1,:]) ## [batch_size, vocab_size] 가장 마지막 hidden state만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 코드, evaluation 코드, training_loop 코드\n",
    "def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    tbar = tqdm(dataloader)\n",
    "    for texts, labels in tbar:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 순전파\n",
    "\n",
    "        outputs = model(texts)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실과 정확도 계산\n",
    "        train_loss += loss.item()\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "        tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 에폭별 학습 결과 출력\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "    return model, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, valid_dataset, criterion, device, epoch, num_epochs):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # model의 업데이트 막기\n",
    "        tbar = tqdm(dataloader)\n",
    "        for texts, labels in tbar:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 순전파\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 손실과 정확도 계산\n",
    "            valid_loss += loss.item()\n",
    "            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # _, true_labels = torch.max(labels, dim=1)\n",
    "            valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "            # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "    valid_loss = valid_loss / len(dataloader)\n",
    "    valid_accuracy = valid_accuracy / len(valid_dataset)\n",
    "\n",
    "    return model, valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "            valid_max_accuracy = valid_accuracy\n",
    "\n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"./model_{model_name}.pt\")\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "patience = 3\n",
    "model_name = 'RNN'\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 512\n",
    "hidden_size = 256\n",
    "model = RNN(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "           x: [batch_size, seq_len]\n",
    "        OUTPUT:\n",
    "           output: [batch_size, vocab_size]\n",
    "        '''\n",
    "        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n",
    "        # lstm에선 마지막 time step에서의 cell state (c_n)도 반환합니다.\n",
    "        output, h_0, c_0 = self.lstm(x) # output : [batch_size, seq_len, hidden_dim] # h_n: [1, batch_size, hidden_dim] # c_n: [1, batch_size, hidden_dim]\n",
    "        return self.fc(output[:,-1,:]) # [batch_size, num_classes]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
