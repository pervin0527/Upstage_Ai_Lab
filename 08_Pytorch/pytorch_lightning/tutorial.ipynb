{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "torch.set_float32_matmul_precision('medium') ## 'high'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvision dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "data_dir = \"/home/pervinco/Datasets/MNIST\"\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root=data_dir, download=True, train=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=data_dir, download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(train_dataset) * 0.8)\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=32)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers=32)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```LightningDataModule``` 클래스를 상속받아서 사용자 데이터셋 클래스를 정의한다.\n",
    "\n",
    "- prepare_data 메서드에서는 데이터를 다운로드 하는 등의 준비를 명시.\n",
    "- setup은 데이터를 로드하고 처리하는 과정을 정의.\n",
    "    - count number of classes\n",
    "    - build vocabulary\n",
    "    - perform train/val/test splits\n",
    "    - create datasets\n",
    "    - apply transforms (defined explicitly in your datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 data_dir=\"path/to/dir\", \n",
    "                 batch_size=32, \n",
    "                 num_workers=4,\n",
    "                 transform=transforms.ToTensor()):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform\n",
    "\n",
    "    def prepare_data(self):\n",
    "        ## 데이터 다운로드(준비)\n",
    "        datasets.MNIST(root=self.data_dir, download=True, train=True, transform=None)\n",
    "        datasets.MNIST(root=self.data_dir, download=True, train=False, transform=None)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            full_train_dataset = datasets.MNIST(root=self.data_dir, download=True, train=True, transform=self.transform)\n",
    "            self.train_dataset, self.valid_dataset = random_split(full_train_dataset, [55000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        if stage == 'test' or stage == 'predict':\n",
    "            self.test_dataset = datasets.MNIST(root=self.data_dir, download=True, train=False, transform=self.transform)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(data_dir=data_dir, batch_size=32, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 정의할 때는 pytorch_lightning의 ```LightningModule```을 상속받도록 한다.  \n",
    "클래스에는 다음과 같은 메서드들이 필수적으로 구현되어야 한다.\n",
    "- ```__init__```\n",
    "- forward\n",
    "- configure_optimizers : 최적화 알고리즘을 명시하며, 필요에 따라 스케쥴러도 적용 가능.\n",
    "- training_step : 모델 학습으로, validation_step, test_step도 동일한 방식으로 정의할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "class CNN(LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate, dropout_ratio, use_shceduler):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.use_scheduler = use_shceduler\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5)  # [BATCH_SIZE, 1, 28, 28] -> [BATCH_SIZE, 16, 24, 24]\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5) # [BATCH_SIZE, 16, 24, 24] -> [BATCH_SIZE, 32, 20, 20]\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2) # [BATCH_SIZE, 32, 20, 20] -> [BATCH_SIZE, 32, 10, 10]\n",
    "        self.dropout2 = nn.Dropout(dropout_ratio)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5) # [BATCH_SIZE, 32, 10, 10] -> [BATCH_SIZE, 64, 6, 6]\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2) # 크기를 1/2로 줄입니다. [BATCH_SIZE, 64, 6, 6] -> [BATCH_SIZE, 64, 3, 3]\n",
    "        self.dropout3 = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.output = nn.Linear(64 * 3 * 3, self.num_classes)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=10, factor=0.1, verbose=True)\n",
    "            return [optimizer], [scheduler]\n",
    "        else:\n",
    "            return optimizer\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ## model.train()을 생략.\n",
    "        x, y = batch ## to(device)를 생략.\n",
    "        y_pred = self(x) ## outputs = model(images)\n",
    "\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        ## loss.backward(), optimizer.step()은 생략한다. 라이트닝이 자동으로 수행.\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ## valid, test에서 사용하던 model.eval()과 torch.no_grad()를 생략한다. 자동으로 수행함.\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        \n",
    "        _, preds = torch.max(y_pred, dim=1) ## [batch_size, num_classes]. num_classes 중 최고 확률 하나 선택.\n",
    "        acc = self.accuracy(preds, y)\n",
    "\n",
    "        self.log(\"valid_loss\", loss, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(\"valid_acc\", acc, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        \n",
    "        _, preds = torch.max(y_pred, dim=1) ## [batch_size, num_classes]. num_classes 중 최고 확률 하나 선택.\n",
    "        acc = self.accuracy(preds, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        predictions = self(x)\n",
    "        _, preds = torch.max(predictions, dim=1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def weight_initialization(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 인스턴스 생성\n",
    "model = CNN(num_classes=10, learning_rate=0.01, dropout_ratio=0.2, use_shceduler=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#trainer](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#trainer)  \n",
    "\n",
    "- Trainer 클래스의 인스턴스를 생성한다. 여기에는 epoch, gpu 사용, 콜백 기능, 로깅 기능 등을 명시한다.\n",
    "- trainer 인스턴스에서 fit 함수를 이용해 학습을 수행. model, train_loader, valid_loader 를 반영한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pervinco/miniconda3/envs/DL/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory ./runs/weights exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type               | Params\n",
      "--------------------------------------------------\n",
      "0  | criterion | CrossEntropyLoss   | 0     \n",
      "1  | accuracy  | MulticlassAccuracy | 0     \n",
      "2  | conv1     | Conv2d             | 416   \n",
      "3  | relu1     | ReLU               | 0     \n",
      "4  | conv2     | Conv2d             | 12.8 K\n",
      "5  | relu2     | ReLU               | 0     \n",
      "6  | pool2     | MaxPool2d          | 0     \n",
      "7  | dropout2  | Dropout            | 0     \n",
      "8  | conv3     | Conv2d             | 51.3 K\n",
      "9  | relu3     | ReLU               | 0     \n",
      "10 | pool3     | MaxPool2d          | 0     \n",
      "11 | dropout3  | Dropout            | 0     \n",
      "12 | output    | Linear             | 5.8 K \n",
      "--------------------------------------------------\n",
      "70.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "70.3 K    Total params\n",
      "0.281     Total estimated model params size (MB)\n",
      "/home/pervinco/miniconda3/envs/DL/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory ./runs/logs/train_csv/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1719/1719 [00:05<00:00, 288.35it/s, v_num=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1719: 'valid_loss' reached 0.06565 (best 0.06565), saving model to './runs/weights/epoch=0-valid_loss=0.07.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1719/1719 [00:06<00:00, 284.23it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 3438: 'valid_loss' reached 0.04522 (best 0.04522), saving model to './runs/weights/epoch=1-valid_loss=0.05.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1719/1719 [00:06<00:00, 278.30it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 5157: 'valid_loss' reached 0.03241 (best 0.03241), saving model to './runs/weights/epoch=2-valid_loss=0.03.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1719/1719 [00:05<00:00, 288.40it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 6876: 'valid_loss' reached 0.04063 (best 0.03241), saving model to './runs/weights/epoch=3-valid_loss=0.04.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1719/1719 [00:06<00:00, 277.53it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 8595: 'valid_loss' reached 0.03073 (best 0.03073), saving model to './runs/weights/epoch=4-valid_loss=0.03.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1719/1719 [00:06<00:00, 277.73it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 10314: 'valid_loss' reached 0.03573 (best 0.03073), saving model to './runs/weights/epoch=5-valid_loss=0.04.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1719/1719 [00:06<00:00, 285.62it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 12033: 'valid_loss' reached 0.03432 (best 0.03073), saving model to './runs/weights/epoch=6-valid_loss=0.03.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1719/1719 [00:06<00:00, 279.07it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 13752: 'valid_loss' reached 0.03140 (best 0.03073), saving model to './runs/weights/epoch=7-valid_loss=0.03.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1719/1719 [00:06<00:00, 278.85it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "save_dir = \"./runs\"\n",
    "csv_logger = CSVLogger(save_dir=f\"{save_dir}/logs\", name=\"train_csv\")\n",
    "tb_logger = TensorBoardLogger(save_dir=f\"{save_dir}/logs\", name=\"train_tb\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"valid_loss\", mode='min')\n",
    "save_ckpt_callback = ModelCheckpoint(\n",
    "    dirpath=f\"{save_dir}/weights\",\n",
    "    monitor=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    filename=\"{epoch}-{valid_loss:.2f}\",  # 모델 체크포인트 파일 이름 설정\n",
    "    save_last=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=True,\n",
    "    save_top_k=3  # 가장 좋은 3개의 모델만 저장\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\", ## 또는 auto로 설정하면 알아서 선택함.\n",
    "    callbacks=[early_stop_callback, save_ckpt_callback],\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    default_root_dir=\"./runs\" ## 저장경로\n",
    ")\n",
    "\n",
    "# trainer.fit(\n",
    "#     model, \n",
    "#     train_dataloader, \n",
    "#     valid_dataloader,\n",
    "#     ckpt_path=None, ## resume할 가중치 파일 경로\n",
    "# )\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at last.ckpt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Checkpoint at last.ckpt not found. Aborting training.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:706\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    704\u001b[0m     model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:749\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(model, test_dataloaders\u001b[38;5;241m=\u001b[39mdataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n\u001b[1;32m    746\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    748\u001b[0m )\n\u001b[0;32m--> 749\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    751\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:901\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mrestore_checkpoint_after_setup:\n\u001b[1;32m    900\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: restoring module and callbacks from checkpoint path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_modules_and_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: configuring sharded model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    904\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_configure_sharded_model(\u001b[38;5;28mself\u001b[39m)  \u001b[38;5;66;03m# allow user to setup in model sharded environment\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:395\u001b[0m, in \u001b[0;36m_CheckpointConnector._restore_modules_and_callbacks\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_restore_modules_and_callbacks\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# restore modules after setup\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_model()\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_datamodule()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:82\u001b[0m, in \u001b[0;36m_CheckpointConnector.resume_start\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m     80\u001b[0m rank_zero_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestoring states from the checkpoint path at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 82\u001b[0m     loaded_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:347\u001b[0m, in \u001b[0;36mStrategy.load_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: _PATH) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    346\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.9/site-packages/lightning_fabric/plugins/io/torch_io.py:87\u001b[0m, in \u001b[0;36mTorchCheckpointIO.load_checkpoint\u001b[0;34m(self, path, map_location)\u001b[0m\n\u001b[1;32m     85\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Aborting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pl_load(path, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Checkpoint at last.ckpt not found. Aborting training."
     ]
    }
   ],
   "source": [
    "trainer.test(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    ckpt_path=\"last.ckpt\",\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
