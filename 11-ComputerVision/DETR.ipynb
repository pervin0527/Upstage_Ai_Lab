{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from typing import Optional, Union, Tuple, List\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETR 모델 불러오기 및 프로세서 초기화\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference를 위한 테스트 이미지\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor를 사용하여 이미지 전처리\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# inference 결과\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor 내부 함수를 사용하여 결과 이미지 후처리\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측된 bounding boxes\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Label: {model.config.id2label[label.item()]} | Score: {round(score.item(), 4)} | Box: {box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "def plot_results(pil_img, scores, labels, boxes):\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(scores)))\n",
    "\n",
    "    for score, label, (xmin, ymin, xmax, ymax), c in zip(scores, labels, boxes.tolist(), colors):\n",
    "        ax.add_patch(patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n",
    "        text = f\"{model.config.id2label[label.item()]}: {score.item():0.4f}\"\n",
    "        if ymin < 20: ymin = ymin + 20\n",
    "        ax.text(xmin, ymin, text, bbox={\"facecolor\": c, \"alpha\": 0.6}, clip_box=ax.clipbox, clip_on=True)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_results(image, results[\"scores\"], results[\"labels\"], results[\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO 2017 validation set의 이미지와 annotation 다운로드\n",
    "%mkdir -p data\n",
    "%wget http://images.cocodataset.org/zips/val2017.zip -P data/\n",
    "%wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P data/\n",
    "%unzip -q data/val2017.zip -d data/\n",
    "%unzip -q data/annotations_trainval2017.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지와 annotation 정보를 로드하기 위한 Dataset 클래스 (processor 전처리 내장)\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_directory_path: str,\n",
    "        image_processor,\n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path.split('/')[0], 'annotations/instances_val2017.json')\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CocoDetection(\n",
    "    image_directory_path='data/val2017',\n",
    "    image_processor=processor,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 DataLoader\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "print(\"Batch number of train loader:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning을 위한 engine 함수\n",
    "def finetune_model(model, train_loader, num_epochs, learning_rate):\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        t = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Loss: ---\")\n",
    "\n",
    "        for i, batch in t:\n",
    "            # pixel_values: 이미지, targets['boxes']: bbox 좌표 gt, targets['class_labels']: class gt\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels']\n",
    "            targets = [{'boxes': label['boxes'].to(device), 'class_labels': label['class_labels'].to(device)} for label in labels]\n",
    "\n",
    "            # inference (prediction + loss 등 포함) 결과\n",
    "            outputs = model(pixel_values=pixel_values, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                avg_loss = total_loss / (i+1)\n",
    "                print(f\"\\n  ##### Iteration {i}, Average Loss: {avg_loss:.4f}\")\n",
    "            t.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            t.refresh()\n",
    "\n",
    "            # 빠른 실습을 위해\n",
    "            if i == 30:\n",
    "                break\n",
    "\n",
    "        print(f\"\\n----- Epoch {epoch}, loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')\n",
    "\n",
    "finetune_model(model.to(device), train_loader, num_epochs=1, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.detr.configuration_detr import DetrConfig\n",
    "\n",
    "# config 수정을 통해 모델 customize\n",
    "detr_config = DetrConfig(\n",
    "    d_model = 256,                   # layer들의 dimension\n",
    "    dropout = 0.1,                   # embeddings, encoder, pooler에 사용될 dropout 비율\n",
    "    activation_function = 'relu',    # encoder와 pooler에 사용될 activation function\n",
    "    num_labels = 91,                 # 학습 데이터셋의 num_classes\n",
    "\n",
    "    # encoder configs\n",
    "    encoder_layers = 6,              # encoder layer의 개수\n",
    "    encoder_attention_heads = 8,     # encoder 내부 attention layer의 attention head 개수\n",
    "    encoder_ffn_dim = 2048,          # encoder 내부 FFN layer의 dimension\n",
    "\n",
    "    # decoder configs\n",
    "    decoder_layers = 6,              # decoder layer의 개수\n",
    "    decoder_attention_heads = 8,     # decoder 내부 attention layer의 attention head 개수\n",
    "    decoder_ffn_dim = 2048           # decoder 내부 FFN layer의 dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.detr.modeling_detr import DetrEncoder, DetrDecoder\n",
    "\n",
    "# config에 맞는 Encoder과 Decoder\n",
    "encoder = DetrEncoder(detr_config).to(device)\n",
    "decoder = DetrDecoder(detr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional embedding\n",
    "from transformers.models.detr.modeling_detr import DetrSinePositionEmbedding\n",
    "\n",
    "def build_position_encoding(config):\n",
    "    # Transformer의 positional enocding을 2D로 일반화 -> 각 축에 d_model//2씩 생성 후 concat하여 d_model\n",
    "    n_steps = config.d_model // 2\n",
    "    position_embedding = DetrSinePositionEmbedding(n_steps, normalize=True)\n",
    "\n",
    "    return position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN backbone + positional embedding\n",
    "from transformers.models.detr.modeling_detr import DetrConvEncoder, DetrConvModel\n",
    "\n",
    "backbone_cnn = DetrConvEncoder(detr_config)\n",
    "position_embedding = build_position_encoding(detr_config)\n",
    "\n",
    "backbone = DetrConvModel(backbone_cnn, position_embedding).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transforms = T.Compose([\n",
    "    T.Resize((800, 800)),  # Resize the image\n",
    "    T.ToTensor(),  # Convert image to tensor\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "input = transforms(image).unsqueeze(0).to(device)\n",
    "\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 pixel 처리\n",
    "pixel_mask = torch.ones(((input.shape[0], input.shape[2], input.shape[3])), device=device)\n",
    "print(f'Shape of input image: {input.shape}')\n",
    "print(f'Shape of pixel mask: {pixel_mask.shape}')\n",
    "\n",
    "# backbone forward\n",
    "features, pos_embs = backbone(input, pixel_mask)\n",
    "print(f'Num of output features: {len(features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone의 final feature map과 downsampled mask\n",
    "feature_map, mask = features[-1]\n",
    "print(f'Shape of feature map: {feature_map.shape}')\n",
    "print(f'Shape of downsampled mask: {mask.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel 조절해주는 1x1 convolution\n",
    "input_projection = nn.Conv2d(\n",
    "    in_channels=feature_map.shape[1],\n",
    "    out_channels=256,\n",
    "    kernel_size=1\n",
    "    ).to(device)\n",
    "\n",
    "projected_feature_map = input_projection(feature_map)\n",
    "print(f'Shape of projected feature map: {projected_feature_map.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature map,\n",
    "flattened_features = projected_feature_map.flatten(2).permute(0, 2, 1)\n",
    "pos_emb = pos_embs[-1].flatten(2).permute(0, 2, 1)\n",
    "flattened_mask = mask.flatten(1)\n",
    "\n",
    "print(f'Shape of flattened_features: {flattened_features.shape}')\n",
    "print(f'Shape of position embeddings: {pos_emb.shape}')\n",
    "print(f'Shape of flattened_mask: {flattened_mask.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = encoder(\n",
    "    inputs_embeds=flattened_features,\n",
    "    attention_mask=flattened_mask,\n",
    "    object_queries=pos_emb\n",
    "    )\n",
    "\n",
    "# encoder_outputs[0]: last_hidden_state\n",
    "print(f'Shape of encoder_outputs: {encoder_outputs[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 탐지를 위한 N개의 object queries 생성\n",
    "N = 10\n",
    "query_position_embeddings = nn.Embedding(N, 256)    # 임의의 embedding 생성\n",
    "query_position_embeddings = query_position_embeddings.weight.unsqueeze(0).repeat(1, 1, 1).to(device)    # batch_size에 맞게 조정(현재는 1)\n",
    "queries = torch.zeros_like(query_position_embeddings).to(device)    # Zero initialize\n",
    "\n",
    "print(f'Shape of queries: {queries.shape}')\n",
    "print(f'Query: {queries[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = decoder(\n",
    "    inputs_embeds=queries,\n",
    "    object_queries=pos_emb,                              # Cross-attn에서 사용되는 positional embedding\n",
    "    query_position_embeddings=query_position_embeddings, # Self-attn에서 사용되는 positional embedding\n",
    "    encoder_hidden_states=encoder_outputs[0],\n",
    "    encoder_attention_mask=flattened_mask\n",
    "    )\n",
    "\n",
    "# decoder_outputs[0]: last_hidden_state\n",
    "print(f'Shape of decoder_outputs: {encoder_outputs[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 전체 코드\n",
    "\n",
    "from transformers.models.detr.modeling_detr import DetrPreTrainedModel\n",
    "\n",
    "class DetrModel(DetrPreTrainedModel):\n",
    "    def __init__(self, config: DetrConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # backbone + positional encoding 생성\n",
    "        backbone_cnn = DetrConvEncoder(config)\n",
    "        position_embedding = build_position_encoding(config)\n",
    "        self.backbone = DetrConvModel(backbone_cnn, position_embedding)\n",
    "\n",
    "        # Input의 차원을 변경하는 projection layer\n",
    "        self.input_projection = nn.Conv2d(backbone_cnn.intermediate_channel_sizes[-1], config.d_model, kernel_size=1)\n",
    "\n",
    "        # N개의 object query 생성\n",
    "        self.query_position_embeddings = nn.Embedding(config.num_queries, config.d_model)\n",
    "\n",
    "        # 모델 내부 encoder와 decoder 생성\n",
    "        self.encoder = DetrEncoder(config)\n",
    "        self.decoder = DetrDecoder(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        pixel_mask = None,\n",
    "        decoder_attention_mask = None,\n",
    "        encoder_outputs = None,\n",
    "        inputs_embeds = None,\n",
    "        decoder_inputs_embeds = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        device = pixel_values.device\n",
    "\n",
    "        if pixel_mask is None:\n",
    "            pixel_mask = torch.ones(((batch_size, height, width)), device=device)\n",
    "\n",
    "        # 1. pixel_values + pixel mask를 Backbone에 넣어 feature 생성\n",
    "        features, pos_embs = self.backbone(pixel_values, pixel_mask)\n",
    "\n",
    "        # Backbone의 final feature map과 downsampled mask\n",
    "        feature_map, mask = features[-1]\n",
    "\n",
    "        # 2. 1x1 convolution으로 channel을 d_model로 압축\n",
    "        projected_feature_map = self.input_projection(feature_map)\n",
    "\n",
    "        # 3. features, queries, mask flatten\n",
    "        flattened_features = projected_feature_map.flatten(2).permute(0, 2, 1)\n",
    "        pos_emb = pos_embs[-1].flatten(2).permute(0, 2, 1)\n",
    "        flattened_mask = mask.flatten(1)\n",
    "\n",
    "        # 4. flattened_features + flattened_mask + position embeddings -> encoder 통과\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=flattened_features,\n",
    "            attention_mask=flattened_mask,\n",
    "            object_queries=pos_emb\n",
    "        )\n",
    "\n",
    "        # 5. 객체 탐지를 위한 object query 초기화\n",
    "        query_position_embeddings = self.query_position_embeddings.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        queries = torch.zeros_like(query_position_embeddings)\n",
    "\n",
    "        # 6. query embeddings + pos_emb -> decoder 통과\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs_embeds=queries,\n",
    "            attention_mask=None,\n",
    "            object_queries=pos_emb,\n",
    "            query_position_embeddings=query_position_embeddings,\n",
    "            encoder_hidden_states=encoder_outputs[0],\n",
    "            encoder_attention_mask=flattened_mask\n",
    "        )\n",
    "\n",
    "        return decoder_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 box의 영역 계산\n",
    "def box_area(boxes):\n",
    "    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "\n",
    "# 두 박스 사이의 IoU(Intersection of Union) 계산\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2]) # [N,M,2]\n",
    "    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:]) # [N,M,2]\n",
    "\n",
    "    width_height = (right_bottom - left_top).clamp(min=0) # [N,M,2]\n",
    "    inter = width_height[:, :, 0] * width_height[:, :, 1] # [N,M]\n",
    "\n",
    "    # union = (box1의 영역 넓이) + (box2의 영역 넓이) - (교집합 영역 넓이)\n",
    "    union = area1[:, None] + area2 - inter\n",
    "\n",
    "    # IoU = (Area of Overlap) / (Area of Union)\n",
    "    iou = inter / union\n",
    "    return iou, union\n",
    "\n",
    "\n",
    "# 두 박스 사이의 GIoU(Generalized Intersection of Union) 계산\n",
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    iou, union = box_iou(boxes1, boxes2)\n",
    "\n",
    "    # 새로운 box C의 좌표와 생성\n",
    "    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "    # 새로운 box C의 width, height 계산 (넓이 = width*height)\n",
    "    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n",
    "    area = width_height[:, :, 0] * width_height[:, :, 1]\n",
    "\n",
    "    # GIoU = IoU - ((Area of C) - (Area of Union)) / (Area of C)\n",
    "    return iou - (area - union) / area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hungarian algorithm으로 최적의 조합을 찾아주는 matcher\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from transformers.image_transforms import center_to_corners_format\n",
    "\n",
    "class DetrHungarianMatcher(nn.Module):\n",
    "    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n",
    "        super().__init__()\n",
    "        # 세 가지 weight의 가중치 (matching cost의 weighted sum)\n",
    "        self.class_cost = class_cost\n",
    "        self.bbox_cost = bbox_cost\n",
    "        self.giou_cost = giou_cost\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n",
    "\n",
    "        # batch로 연산하기 위해 flatten\n",
    "        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # target label과 bbox도 concat\n",
    "        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n",
    "        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # 1. Classification loss\n",
    "        # -(class probability)로 근사하여 계산\n",
    "        class_cost = -out_prob[:, target_ids]\n",
    "\n",
    "        # 2-(1). Bbox l1-loss\n",
    "        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n",
    "\n",
    "        # 2-(2). Bbox GIoU loss\n",
    "        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n",
    "\n",
    "        # 3. Matrix 값을 채울 matching cost = 세 가지 loss의 Weighted sum\n",
    "        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n",
    "        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        # scipy의 linear_sum_assignment로 hungarian algorithm 연산\n",
    "        # 최적의 cost 조합인 indices 반환\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n",
    "\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 조합 indices로 최종 hungarian loss 계산\n",
    "class DetrLoss(nn.Module):\n",
    "    def __init__(self, matcher, num_classes, eos_coef, losses):\n",
    "        super().__init__()\n",
    "        self.matcher = matcher\n",
    "        self.num_classes = num_classes\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer(\"empty_weight\", empty_weight)\n",
    "\n",
    "    # 1. classification loss\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
    "        # 모델의 예측 확률값\n",
    "        source_logits = outputs[\"logits\"]\n",
    "\n",
    "        # 정답 박스와 매칭된 예측 박스의 인덱스\n",
    "        idx = self._get_source_permutation_idx(indices)\n",
    "\n",
    "        # Construct label\n",
    "        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(\n",
    "            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n",
    "        )\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        # cross entropy로 negative log likelihood 계산\n",
    "        loss_ce = nn.functional.cross_entropy(source_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        losses = {\"loss_ce\": loss_ce}\n",
    "\n",
    "        return losses\n",
    "\n",
    "    # 2. bbox loss\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        idx = self._get_source_permutation_idx(indices)\n",
    "        source_boxes = outputs[\"pred_boxes\"][idx]\n",
    "        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        # 2-(1). l1-loss 계산\n",
    "        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n",
    "        losses = {}\n",
    "        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        # 2-(2). GIoU loss 계산\n",
    "        loss_giou = 1 - torch.diag(\n",
    "            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n",
    "        )\n",
    "        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    # 최적의 매칭을 기준으로 pair를 조합하여 재정렬하는 함수\n",
    "    def _get_source_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n",
    "        source_idx = torch.cat([source for (source, _) in indices])\n",
    "        return batch_idx, source_idx\n",
    "\n",
    "    # classification loss와 bbox loss 반환\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n",
    "        loss_map = {\n",
    "            \"labels\": self.loss_labels,\n",
    "            \"boxes\": self.loss_boxes\n",
    "        }\n",
    "        return loss_map[loss](outputs, targets, indices, num_boxes)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # 마지막 layer의 출력과 target 사이의 최적의 매칭\n",
    "        indices = self.matcher(outputs, targets)\n",
    "\n",
    "        # 모든 node에 걸쳐 target bbox의 평균 개수를 계산 -> 정규화 목적\n",
    "        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        num_boxes = torch.clamp(num_boxes, min=1).item()\n",
    "\n",
    "        # 두 가지의 loss 모두 계산\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox 좌표 예측을 위한 MLP prediction head\n",
    "class DetrMLPPredictionHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrForObjectDetection(DetrPreTrainedModel):\n",
    "    def __init__(self, config: DetrConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # DETR의 기본 encoder-decoder 모델\n",
    "        self.model = DetrModel(config)\n",
    "\n",
    "        # Prediction heads\n",
    "        # Class label을 예측하는 linear head (\"no object\"를 포함하여 num_classes+1)\n",
    "        self.class_labels_classifier = nn.Linear(\n",
    "            config.d_model, config.num_labels + 1\n",
    "        )\n",
    "        # bbox 좌표를 예측하는 MLP head\n",
    "        self.bbox_predictor = DetrMLPPredictionHead(\n",
    "            input_dim=config.d_model, hidden_dim=config.d_model, output_dim=4, num_layers=3\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        pixel_mask = None,\n",
    "        decoder_attention_mask = None,\n",
    "        encoder_outputs = None,\n",
    "        inputs_embeds = None,\n",
    "        decoder_inputs_embeds = None,\n",
    "        labels = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "        # 1.이미지를 DETR 기본 모델을 통해 전달하여 encoder + decoder 출력 반환\n",
    "        sequence_output = self.model(\n",
    "            pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # 2. Prediction heads를 통과하여 class logit과 pred bbox 생성\n",
    "        logits = self.class_labels_classifier(sequence_output)\n",
    "        pred_boxes = self.bbox_predictor(sequence_output).sigmoid()\n",
    "\n",
    "        loss, loss_dict = None, None\n",
    "        if labels is not None:\n",
    "            # 3. Prediction과 Ground Truth 사이의 최적의 매칭 생성\n",
    "            matcher = DetrHungarianMatcher(\n",
    "                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n",
    "            )\n",
    "            # 4. 최적의 조합을 기반으로 하는 classification loss, bbox loss 계산\n",
    "            losses = [\"labels\", \"boxes\"]\n",
    "            criterion = DetrLoss(\n",
    "                matcher=matcher,\n",
    "                num_classes=self.config.num_labels,\n",
    "                eos_coef=self.config.eos_coefficient,\n",
    "                losses=losses,\n",
    "            )\n",
    "            criterion.to(self.device)\n",
    "            outputs_loss = {}\n",
    "            outputs_loss[\"logits\"] = logits\n",
    "            outputs_loss[\"pred_boxes\"] = pred_boxes\n",
    "\n",
    "            # (실제 3,4번 동시에 연산되는 구간)\n",
    "            loss_dict = criterion(outputs_loss, labels)\n",
    "\n",
    "            # 5. 모든 loss의 weighted sum으로 최종 loss 계산\n",
    "            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n",
    "            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n",
    "            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetrForObjectDetection(detr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training DETR detector\n",
    "def train_model(model, train_loader, num_epochs=1, learning_rate=1e-4):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        t = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Loss: ---\")\n",
    "        for i, batch in t:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels']\n",
    "            targets = [{'boxes': label['boxes'].to(device), 'class_labels': label['class_labels'].to(device)} for label in labels]\n",
    "\n",
    "            loss = model(pixel_values=pixel_values, labels=targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                avg_loss = total_loss / (i+1)\n",
    "                print(f\"\\n  ##### Iteration {i}, Average Loss: {avg_loss:.4f}\")\n",
    "            t.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            t.refresh()\n",
    "\n",
    "            if i == 20:\n",
    "               break\n",
    "\n",
    "        print(f\"\\n----- Epoch {epoch}, loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')\n",
    "\n",
    "model = DetrForObjectDetection(detr_config).to(device)\n",
    "train_model(model, train_loader, num_epochs=1, learning_rate=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
