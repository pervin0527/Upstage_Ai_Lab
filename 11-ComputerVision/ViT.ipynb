{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# !wget https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.init import trunc_normal_\n",
    "from transformers import ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerWithLinear(nn.Module):\n",
    "    def __init__(self, base_vit, embed_dim=384, num_classes=10, **kwargs):\n",
    "        \"\"\"Vision Transformer with an additional Linear Classifier.\n",
    "        ViT Encoder를 외부에서 받았을 때 MLP layer(출력층)와 연결하여 output을 획득한다.\n",
    "\n",
    "        Args:\n",
    "            base_vit (nn.Module): 기존 Vision Transformer 모델.\n",
    "            embed_dim (int, optional): 임베딩 차원의 크기. 기본값은 384.\n",
    "            num_classes (int, optional): 출력 클래스의 개수. 기본값은 10.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_vit = base_vit\n",
    "\n",
    "        # ViT encoder로부터 얻어진 잠재적 특징 맵을 분류를 위한 최종 출력으로 투영\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass를 위한 함수.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 이미지.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 분류기의 로짓 결과.\n",
    "        \"\"\"\n",
    "\n",
    "        # 이미지를 인코더로 입력\n",
    "        features = self.base_vit(x) ## Encoder의 output\n",
    "\n",
    "        # 인코더의 출력을 정규화\n",
    "        features = torch.nn.functional.normalize(features, dim=-1)\n",
    "\n",
    "        # 분류 헤드를 거쳐 최종 로짓을 계산\n",
    "        logits = self.fc(features)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 이미지를 Patch로 나누기 위한 class\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        # Convolution filter와 stride를 이용하여 이미지를 patch화 --> 패치로 분할함과 동시에 Linear Projection 가능.\n",
    "        #   - filter의 크기 = patch의 크기\n",
    "        #   - stride = patch의 크기\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention 연산을 수행하는 클래스\"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        # Attention score를 계산할 때 softmax의 분모에 해당하는 부분\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        # Embedding된 token을 각각 Q, K, V로 mapping\n",
    "        self.qkv = nn.Linear(dim, dim * 3) ## Q, K, V로 한 번에 projection하기 위함.\n",
    "\n",
    "        # Attention에 dropout 적용\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x): ## x : input embedding\n",
    "        B, N, C = x.shape ## batch_size, num_patches, (p * p * embed_dim)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Attention score 계산 (softmax 부분) 후 dropout 적용\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # Value를 반영한 최종 Attention score 계산 후 projection + dropout\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Dropout과 같이 overfitting을 막기위한 조치, training 단계에서만 적용\"\"\"\n",
    "\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1) # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_() # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"Transformer encoder의 feed forward\"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer의 encoder block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads,  attn_drop=attn_drop, proj_drop=drop\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Positional embedding까지 적용된 latent feature를 input으로 받음\"\"\"\n",
    "\n",
    "        # normalization 후 attention score 계산\n",
    "        y = self.attn(self.norm1(x))\n",
    "        x = x + self.drop_path(y) ## skip-connection을 적용할 것인지\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=[224],\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            num_classes=0,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            num_heads=12,\n",
    "            mlp_ratio=4.,\n",
    "            drop_rate=0.,\n",
    "            attn_drop_rate=0.,\n",
    "            drop_path_rate=0.,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            **kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size: 이미지 크기.\n",
    "                정사각형으로 resize된 이미지가 사용되기 때문에, 가로 또는 세로 크기 하나만 입력.\n",
    "            mlp_ratio: Transformer의 feed forward 부분의 hidden dimension을 결정.\n",
    "                hidden_dim = embedding_size * mlp_ratio\n",
    "            drop_path_rate: Dropout과 같이 과적합을 막기위함.\n",
    "              Transformer encoder의 skip connection 부분에서 새로운 value vector를 랜덤하게 제거.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # From image to patch\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "\n",
    "        # 인풋 image와 patch 크기로부터 계산된 patch 갯수 (224/16 = 14 -> 가로, 세로 14개씩 patch로 총 196개의 patch가 생성)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # CLS Token 생성, 학습가능한 파라미터로 설정 (1개 patch)\n",
    "        # 일단 batch size를 1로 설정 후 이후에 batch size에 맞게 broadcast\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        # Positional embedding 생성, 학습가능한 파라미터 설정 (각 patch + class token)\n",
    "        # +1은 cls 토큰이 추가된 임베딩에 더해주기 위함.\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        # Postional embedidng 결과에 dropout을 적용하기 위함\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Drop 확률을 depth에 따라 차등적용\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Attention blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Class token과 positional embedding의 parameter 초기화\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        # Layernorm과 classification head의 parameter 초기화\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Layernorm과 classification head의 parameter 초기화하는 함수\"\"\"\n",
    "\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        \"\"\"Pre-trained된 positional encoding을 interpolation을 사용하여 고해상도 이미지에 적용하기 위한 함수\"\"\"\n",
    "\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "\n",
    "        # Interpolation 할 필요가 없으면 interpoltation 없이 postional embedding 적용\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "\n",
    "        # Interpolation을 위한 코드\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        \n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x, interpolate_pos_encoding=False):\n",
    "        \"\"\"이미지 embedding후 classification token과 positional embedding을 추가로 적용하는 함수\"\"\"\n",
    "\n",
    "        # (batch, number of channel, width, height)\n",
    "        B, nc, w, h = x.shape\n",
    "\n",
    "        # 이미지를 패치들로 분할하고 임베딩.\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "       # [CLS] 토큰을 batch size에 맞게 확장\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        # [CLS] 토큰을 embedding patch에 추가\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        # positional embedding 결과에 dropout 적용\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, return_all_patches=False):\n",
    "        # 이미지 -> token -> embedding\n",
    "        # 이미지(x)를 패치들로 나누고, 이를 flatten하고 embedding한다.\n",
    "        x = self.prepare_tokens(x)\n",
    "\n",
    "        # 정해진 depth만큼 encoder block 적용\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # encoder output을 normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 학습된 모든 patch가 필요한 경우 모든 patch의 latent feature map을 반환\n",
    "        if return_all_patches:\n",
    "            return x\n",
    "        # CLS Token의 latent feature map만 반환\n",
    "        else:\n",
    "            return x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 dataset 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # ViT expects images of size 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# CIFAR10의 training, test set 다운로드\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 정의\n",
    "method = 'custom' # or hugging (hugging으로 하면 huggingfac에 있는 모델 클래스를 사용하는 것.)\n",
    "pretrain = True\n",
    "\n",
    "if method == 'custom':\n",
    "    # 위에서 정의한 ViT 모델\n",
    "    encoder = vit_small()\n",
    "    if pretrain:\n",
    "        state_dict = torch.load(\"./pretrained/dino_deitsmall16_pretrain.pth\")\n",
    "        encoder.load_state_dict(state_dict)\n",
    "        model = VisionTransformerWithLinear(encoder).cuda()\n",
    "    else :\n",
    "        model = VisionTransformerWithLinear(encoder).cuda()\n",
    "\n",
    "elif method == 'hugging':\n",
    "    # huggingface에 준비된 모델\n",
    "    if pretrain:\n",
    "        model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=10).cuda()\n",
    "    else:\n",
    "        model = ViTForImageClassification().cuda()\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "# start training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    cnt = 0\n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if method == \"custom\":\n",
    "            outputs = model(inputs)\n",
    "        else:\n",
    "            outputs = model(inputs).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        cnt += len(inputs)\n",
    "        if i==10:\n",
    "            break\n",
    "    if epoch % 1 == 0:  # print every 5 epochs\n",
    "        print(f\"[Training loss at epoch {epoch + 1}]: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in tqdm(test_loader):\n",
    "        images, labels = data[0].cuda(), data[1].cuda()\n",
    "        outputs = model(images)\n",
    "\n",
    "        if method == \"custom\":\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        break\n",
    "print(f'Accuracy on the 10000 test images: {100 * correct / total}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
