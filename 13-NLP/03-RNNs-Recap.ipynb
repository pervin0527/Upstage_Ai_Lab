{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "[https://pytorch.org/docs/stable/generated/torch.nn.RNN.html](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 256])\n",
      "torch.Size([1, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "# 배치 크기: 3, 시퀀스 길이: 5, 입력 크기: 256의 더미 입력 생성\n",
    "input_tensor = torch.randn(3, 5, 256)\n",
    "\n",
    "## d는 bidirectional=True일 때 2\n",
    "## batch_first = False -> [seq_len, batch_size, d * output_dim]\n",
    "## batch_first = True -> [batch_size, seq_len, d * output_dim]\n",
    "rnn = nn.RNN(input_size=256,\n",
    "             hidden_size=256,\n",
    "             nonlinearity=\"tanh\",\n",
    "             batch_first=True)\n",
    "\n",
    "output, h_n = rnn(input_tensor) ## output은 각 t마다 얻은 output feature에 해당.\n",
    "\n",
    "## output 텐서는 RNN의 마지막 레이어에서 각 타임스텝(t)마다 생성된 출력 특징들을 포함.\n",
    "## 길이가 5인 시퀀스 데이터를 rnn에 입력했을 때 0~4까지의 hidden_state가 발생하니까 output의 두번째 차원이 5\n",
    "print(output.shape) ## [3, 5, 256], [batch, seq_len, hidden_dim]\n",
    "\n",
    "## h_n 텐서는 RNN의 마지막 타임스텝에서 각 배치 항목에 대한 최종 은닉 상태를 포함.\n",
    "## 배치별로 마지막 t번째 hidden state를 가져오니까 [1, 3, 256]\n",
    "print(h_n.shape) ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, nonlinearity='tanh', batch_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=self.input_dim,\n",
    "                          hidden_size=self.hidden_dim,\n",
    "                          nonlinearity=self.nonlinearity,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x, device):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(device) ## hidden_state 초기화.\n",
    "\n",
    "        out, h_n = self.rnn(x, h0)\n",
    "\n",
    "        out = self.output_layer(out[:, -1, :]) ## 마지막 시간 단계의 hidden_state만 가져와 선형변환.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_dim=2, hidden_dim=20, output_dim=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Loss: 0.0973\n",
      "Epoch [60/300], Loss: 0.0303\n",
      "Epoch [90/300], Loss: 0.0155\n",
      "Epoch [120/300], Loss: 0.0100\n",
      "Epoch [150/300], Loss: 0.0072\n",
      "Epoch [180/300], Loss: 0.0056\n",
      "Epoch [210/300], Loss: 0.0045\n",
      "Epoch [240/300], Loss: 0.0037\n",
      "Epoch [270/300], Loss: 0.0031\n",
      "Epoch [300/300], Loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(np.array([[[1, 2], [3, 4], [5, 6]]], dtype=np.float32)).to(device)\n",
    "\n",
    "for epoch in range(300): # 300번의 에폭 동안 학습을 진행\n",
    "    model.zero_grad() # 기울기를 0으로 초기화\n",
    "    outputs = model(inputs, device) # 모델에 입력을 전달하고 출력을 받음\n",
    "\n",
    "    loss = criterion(outputs, torch.tensor([1]).to(device))  # 더미 타겟 데이터로 손실(loss)을 계산\n",
    "    loss.backward() # 역전파를 통해 기울기를 계산\n",
    "\n",
    "    optimizer.step() # 최적화 알고리즘을 통해 파라미터를 업데이트\n",
    "\n",
    "    if (epoch+1) % 30 == 0: # 30 에폭마다 손실을 출력\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "[https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)\n",
    "\n",
    "- LSTM(Long Short-Term Memory)에서의 cell state는 hidden state를 보완하고, 더 나은 기억과 정보를 유지하기 위한 역할.\n",
    "- 일반적인 RNN은 시퀀스가 길어질수록 이전 정보가 제대로 유지되지 않는 문제가 있다.\n",
    "- forgot gate, input gate, output gate를 통해 어떤 정보를 기억하고 어떤 정보를 잊을지 결정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "# 배치 크기: 3, 시퀀스 길이: 5, 입력 크기: 256의 더미 입력 생성\n",
    "input_tensor = torch.randn(3, 5, 256)\n",
    "\n",
    "## d는 bidirectional=True일 때 2\n",
    "## batch_first = True -> [batch_size, seq_len, d * output_dim]\n",
    "lstm = nn.LSTM(input_size=256,\n",
    "               hidden_size=256,\n",
    "               batch_first=True)\n",
    "\n",
    "output, (h_n, c_n) = lstm(input_tensor)\n",
    "\n",
    "## output 텐서는 RNN의 마지막 레이어에서 각 타임스텝(t)마다 생성된 출력 특징들을 포함.\n",
    "## 길이가 5인 시퀀스 데이터를 rnn에 입력했을 때 0~4까지의 hidden_state가 발생하니까 output의 두번째 차원이 5\n",
    "print(output.shape) ## [3, 5, 256], [batch, seq_len, hidden_dim]\n",
    "\n",
    "## h_n 텐서는 RNN의 마지막 타임스텝에서 각 배치 항목에 대한 최종 은닉 상태를 포함.\n",
    "## 배치별로 마지막 t번째 hidden state를 가져오니까 [1, 3, 256]\n",
    "print(h_n.shape)\n",
    "\n",
    "## c_n 텐서는 시퀀스의 마지막 타임스텝에서 각 LSTM 레이어의 최종 cell state를 포함.\n",
    "## 배치별로 마지막 t번째 cell state를 가져오니까 [1, 3, 256]\n",
    "print(c_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, nonlinearity='tanh', batch_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=self.input_dim,\n",
    "                          hidden_size=self.hidden_dim,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x, device):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(device) ## hidden_state 초기화.\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_dim).to(device) ## cell_state 초기화.\n",
    "\n",
    "        out, (h_n, cn) = self.rnn(x, (h0, c0))\n",
    "\n",
    "        out = self.output_layer(out[:, -1, :]) ## 마지막 timestemp의 hidden_state만 가져와 선형변환.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Loss: 0.4644\n",
      "Epoch [60/300], Loss: 0.1944\n",
      "Epoch [90/300], Loss: 0.0654\n",
      "Epoch [120/300], Loss: 0.0289\n",
      "Epoch [150/300], Loss: 0.0165\n",
      "Epoch [180/300], Loss: 0.0108\n",
      "Epoch [210/300], Loss: 0.0078\n",
      "Epoch [240/300], Loss: 0.0059\n",
      "Epoch [270/300], Loss: 0.0047\n",
      "Epoch [300/300], Loss: 0.0039\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(input_dim=2, hidden_dim=20, output_dim=2).to(device) # LSTM 모델을 생성\n",
    "criterion = nn.CrossEntropyLoss() # 손실 함수로 CrossEntropyLoss를 사용\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘으로 Adam을 사용\n",
    "\n",
    "for epoch in range(300): # 300회의 에포크동안 학습을 진행\n",
    "    model.zero_grad() # 기울기를 0으로 초기화\n",
    "    outputs = model(inputs, device) # 모델에 입력을 전달하고 출력을 받음\n",
    "    loss = criterion(outputs, torch.tensor([1]).to(device))  # 예시로 사용할 목표 텐서 생성\n",
    "    loss.backward() # 역전파를 수행하여 기울기를 계산\n",
    "    optimizer.step() # 최적화 알고리즘을 통해 파라미터 업데이트\n",
    "\n",
    "    if (epoch+1) % 30 == 0: # 30 에포크마다 손실을 출력\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "\n",
    "[https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 256])\n",
      "torch.Size([1, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "# 배치 크기: 3, 시퀀스 길이: 5, 입력 크기: 256의 더미 입력 생성\n",
    "input_tensor = torch.randn(3, 5, 256)\n",
    "\n",
    "## d는 bidirectional=True일 때 2\n",
    "## batch_first = True -> [batch_size, seq_len, d * output_dim]\n",
    "rnn = nn.GRU(input_size=256,\n",
    "             hidden_size=256,\n",
    "             batch_first=True)\n",
    "\n",
    "output, h_n = rnn(input_tensor)\n",
    "\n",
    "## output 텐서는 RNN의 마지막 레이어에서 각 타임스텝(t)마다 생성된 출력 특징들을 포함.\n",
    "## 길이가 5인 시퀀스 데이터를 rnn에 입력했을 때 0~4까지의 hidden_state가 발생하니까 output의 두번째 차원이 5\n",
    "print(output.shape) ## [3, 5, 256], [batch, seq_len, hidden_dim]\n",
    "\n",
    "## h_n 텐서는 RNN의 마지막 타임스텝에서 각 배치 항목에 대한 최종 은닉 상태를 포함.\n",
    "## 배치별로 마지막 t번째 hidden state를 가져오니까 [1, 3, 256]\n",
    "print(h_n.shape) ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, nonlinearity='tanh', batch_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=self.input_dim,\n",
    "                          hidden_size=self.hidden_dim,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x, device):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(device) ## hidden_state 초기화.\n",
    "\n",
    "        out, h_n = self.rnn(x, h0)\n",
    "\n",
    "        out = self.output_layer(out[:, -1, :]) ## 마지막 timestemp의 hidden_state만 가져와 선형변환.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Loss: 0.2199\n",
      "Epoch [60/300], Loss: 0.0783\n",
      "Epoch [90/300], Loss: 0.0301\n",
      "Epoch [120/300], Loss: 0.0150\n",
      "Epoch [150/300], Loss: 0.0097\n",
      "Epoch [180/300], Loss: 0.0071\n",
      "Epoch [210/300], Loss: 0.0056\n",
      "Epoch [240/300], Loss: 0.0045\n",
      "Epoch [270/300], Loss: 0.0038\n",
      "Epoch [300/300], Loss: 0.0032\n"
     ]
    }
   ],
   "source": [
    "# GRU 학습\n",
    "model = GRU(input_dim=2, hidden_dim=20, output_dim=2).to(device) # GRU 모델 인스턴스 생성\n",
    "criterion = nn.CrossEntropyLoss() # 손실 함수로 CrossEntropyLoss를 사용\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘으로 Adam을 사용\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.zero_grad()\n",
    "    outputs = model(inputs, device)\n",
    "    loss = criterion(outputs, torch.tensor([1]).to(device))  # A dummy target example\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 30 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
