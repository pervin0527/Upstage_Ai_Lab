{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 목차\n",
    "* 1. RNN 실습\n",
    "  * 1-1. RNN 구현\n",
    "  * 1-2. RNN 학습\n",
    "* 2. LSTM과 GRU 실습\n",
    "  * 2-1. LSTM과 GRU 구현\n",
    "  * 2-2. LSTM과 GRU 학습\n",
    "* 3. One-to-one / One-to-many / Many-to-one / Many-to-many RNNs 구현 및 실습\n",
    "  * 3-1. One-to-one RNN 실습\n",
    "  * 3-2. One-to-Many RNN 실습\n",
    "  * 3-3. Many-to-One RNN 실습\n",
    "  * 3-4. Many-to-Many RNN 실습\n",
    "* 4. Gradient Vanishing / Exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module): # SimpleRNN 클래스 선언\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super().__init__() # nn.Module의 초기화 함수 상속\n",
    "        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n",
    "        self.D = n_inputs # 입력 차원의 크기 지정\n",
    "        self.K = n_outputs # 출력 차원의 크기 지정\n",
    "\n",
    "        # RNN 모듈을 생성\n",
    "        self.rnn = nn.RNN(input_size=self.D, # 입력 차원의 크기 지정\n",
    "                          hidden_size=self.M, # 은닉 상태의 크기 지정\n",
    "                          nonlinearity='tanh', # 활성화 함수로 tanh를 사용\n",
    "                          batch_first=True) # 배치 차원이 먼저 오도록 설정\n",
    "        \n",
    "        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n",
    "\n",
    "    def forward(self, X): # 순전파 함수를 정의\n",
    "        # initial hidden states\n",
    "        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 hidden state를 0으로 초기화\n",
    "\n",
    "        # get RNN unit output\n",
    "        out, _ = self.rnn(X, h0) # RNN에 입력을 전달하고 출력을 받음\n",
    "\n",
    "        # we only want h(T) at the final time step\n",
    "        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(n_inputs=2, n_hidden=20, n_outputs=2).to(device) # SimpleRNN 모델을 생성하고, GPU로 전송\n",
    "criterion = nn.CrossEntropyLoss() # 손실 함수로 CrossEntropyLoss를 사용\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘으로 Adam을 사용\n",
    "\n",
    "# 더미 입력 데이터 예제\n",
    "inputs = torch.from_numpy(np.array([[[1, 2], [3, 4], [5, 6]]], dtype=np.float32)).to(device)\n",
    "\n",
    "for epoch in range(300): # 300번의 에폭 동안 학습을 진행\n",
    "    model.zero_grad() # 기울기를 0으로 초기화\n",
    "    outputs = model(inputs) # 모델에 입력을 전달하고 출력을 받음\n",
    "    loss = criterion(outputs, torch.tensor([1]).to(device))  # 더미 타겟 데이터로 손실(loss)을 계산\n",
    "    loss.backward() # 역전파를 통해 기울기를 계산\n",
    "    optimizer.step() # 최적화 알고리즘을 통해 파라미터를 업데이트\n",
    "\n",
    "    if (epoch+1) % 10 == 0: # 10 에폭마다 손실을 출력\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module): # LSTM 클래스 선언\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super().__init__() # nn.Module의 초기화 함수 상속\n",
    "        self.D = n_inputs # 입력 차원의 크기 지정\n",
    "        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n",
    "        self.K = n_outputs # 출력 차원의 크기 지정\n",
    "\n",
    "        # LSTM 모듈을 생성\n",
    "        self.lstm = nn.LSTM(input_size=self.D, # 입력 차원의 크기 지정\n",
    "                            hidden_size=self.M, # 은닉 상태의 크기 지정\n",
    "                            batch_first=True) # 배치 차원이 먼저 오도록 설정\n",
    "        \n",
    "        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n",
    "\n",
    "    def forward(self, X): # 순전파 함수를 정의\n",
    "        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 hidden state를 0으로 초기화\n",
    "        c0 = torch.zeros(1, X.size(0), self.M).to(X.device) # LSTM의 초기 cell state를 0으로 초기화\n",
    "\n",
    "        # LSTM에 입력을 전달하고 출력을 받음\n",
    "        out, _ = self.lstm(X, (h0, c0)) ## LSTM은 hidden state와 cell state를 튜플로 반환함.\n",
    "\n",
    "        # we only want h(T) at the final time step\n",
    "        out = self.fc(out[:, -1, :]) # 마지막 hidden state만 사용하여 선형 변환을 수행\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 학습\n",
    "model = LSTM(n_inputs=2, n_hidden=20, n_outputs=2).to(device) # LSTM 모델을 생성\n",
    "criterion = nn.CrossEntropyLoss() # 손실 함수로 CrossEntropyLoss를 사용\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘으로 Adam을 사용\n",
    "\n",
    "for epoch in range(300): # 300회의 에포크동안 학습을 진행\n",
    "    model.zero_grad() # 기울기를 0으로 초기화\n",
    "    outputs = model(inputs) # 모델에 입력을 전달하고 출력을 받음\n",
    "    loss = criterion(outputs, torch.tensor([1]).to(device))  # 예시로 사용할 목표 텐서 생성\n",
    "    loss.backward() # 역전파를 수행하여 기울기를 계산\n",
    "    optimizer.step() # 최적화 알고리즘을 통해 파라미터 업데이트\n",
    "\n",
    "    if (epoch+1) % 30 == 0: # 30 에포크마다 손실을 출력\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module): # GRU 클래스 선언\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super().__init__() # nn.Module의 초기화 함수 상속\n",
    "        self.D = n_inputs # 입력 차원의 크기 지정\n",
    "        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n",
    "        self.K = n_outputs # 출력 차원의 크기 지정\n",
    "        \n",
    "        # GRU 모듈을 생성\n",
    "        self.gru = nn.GRU(input_size=self.D, # 입력 차원의 크기 지정\n",
    "                          hidden_size=self.M, # 은닉 상태의 크기 지정\n",
    "                          batch_first=True) # 배치 차원이 먼저 오도록 설정\n",
    "        \n",
    "        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n",
    "\n",
    "    def forward(self, X): # 순전파 함수를 정의\n",
    "        # initial hidden states\n",
    "        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n",
    "\n",
    "        # get RNN unit output\n",
    "        out, _ = self.gru(X, h0) ## output, hidden state\n",
    "\n",
    "        # we only want h(T) at the final time step\n",
    "        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU 학습\n",
    "model = GRU(n_inputs=2, n_hidden=20, n_outputs=2).to(device) # GRU 모델 인스턴스 생성\n",
    "criterion = nn.CrossEntropyLoss() # 손실 함수로 CrossEntropyLoss를 사용\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘으로 Adam을 사용\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, torch.tensor([1]).to(device))  # A dummy target example\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 30 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-One\n",
    "X = np.random.randint(1, 5, size=(1000, 1, 1)) # 입력 데이터 생성, 1~4 사이의 정수 1000개를 랜덤하게 생성\n",
    "Y = np.square(X) # 타겟 데이터 생성, 입력 데이터의 제곱을 타겟으로 설정\n",
    "\n",
    "X = torch.from_numpy(X.astype(np.float32)).to(device) # 입력 데이터를 텐서로 변환\n",
    "Y = torch.from_numpy(Y.astype(np.float32)).squeeze(-1).to(device) # 타겟 데이터를 텐서로 변환\n",
    "\n",
    "model = SimpleRNN(n_inputs=1, n_hidden=40, n_outputs=1).to(device) # 모델 생성\n",
    "criterion = nn.MSELoss() # 손실 함수 설정\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘 설정\n",
    "\n",
    "for epoch in range(4000): # 4000회 반복하여 학습\n",
    "    model.zero_grad() # 기울기 초기화\n",
    "    outputs = model(X) # 모델에 입력 데이터 전달하여 예측값 계산\n",
    "    loss = criterion(outputs, Y) # 예측값과 타겟값을 이용하여 손실 계산\n",
    "    loss.backward() # 역전파 수행\n",
    "    optimizer.step() # 가중치 업데이트\n",
    "\n",
    "    if (epoch+1) % 100 == 0: # 100회마다 손실 출력\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 4000, loss.item()))\n",
    "\n",
    "# Inference\n",
    "X_test = torch.tensor([[[2.0]]], dtype=torch.float32).to(device) # 테스트 데이터 생성\n",
    "print(f\"Input: 2.0, Output: {model(X_test).item()}, 정답: {np.square(2.0)}\") # 테스트 데이터에 대한 예측값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 1) (1000, 10)\n",
      "[[2]] [ 2  4  6  8 10 12 14 16 18 20]\n"
     ]
    }
   ],
   "source": [
    "#### 입력의 배수 10개를 타겟으로 설정\n",
    "# 아래의 One-to-Many는 하나의 숫자 입력 데이터를 받고, 입력된 데이터의 배수 10개를 예측하는 RNN 모델을 구현합니다.\n",
    "\n",
    "# One-to-Many\n",
    "class SimpleRNNOne2Many(nn.Module): # One-to-Many 모델 클래스 선언\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super(SimpleRNNOne2Many, self).__init__()\n",
    "        self.D = n_inputs\n",
    "        self.M = n_hidden\n",
    "        self.K = n_outputs\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.D,\n",
    "            hidden_size=self.M,\n",
    "            nonlinearity='tanh',\n",
    "            batch_first=True) # RNN 모듈 생성\n",
    "        self.fc = nn.Linear(self.M, self.K) # 선형 변환 정의\n",
    "\n",
    "    def forward(self, X): # 순전파 함수 정의\n",
    "        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n",
    "        out, _ = self.rnn(X, h0) # RNN에 입력을 전달하고 출력을 받음. [batch_size, seq_len, hidden_dim] = [1000, 1, 40]\n",
    "        out = self.fc(out) # 출력에 선형 변환을 수행 [batch_size, seq_len, output_dim] = [1000, 1, 10]\n",
    "        return out.view(-1, 10) # 출력을 적절한 형태로 변환 [1000, 10]\n",
    "\n",
    "X = np.random.randint(1, 5, size=(1000, 1, 1)) # 입력 데이터 생성, 1~4 사이의 정수 1000개를 랜덤하게 생성\n",
    "Y = np.array([[i*j for i in range(1, 11)] for j in X.squeeze()]) # 타겟 데이터 생성, 입력의 배수 10개를 타겟으로 설정\n",
    "print(X.shape, Y.shape)\n",
    "print(X[0], Y[0])\n",
    "\n",
    "X = torch.from_numpy(X.astype(np.float32)).to(device) # 입력 데이터를 텐서로 변환\n",
    "Y = torch.from_numpy(Y.astype(np.float32)).to(device) # 타겟 데이터를 텐서로 변환\n",
    "\n",
    "model = SimpleRNNOne2Many(n_inputs=1, n_hidden=40, n_outputs=10).to(device) # 모델 생성\n",
    "criterion = nn.MSELoss() # 손실 함수 설정\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘 설정\n",
    "\n",
    "for epoch in range(4000): # 4000회 반복하여 학습\n",
    "    model.zero_grad() # 기울기 초기화\n",
    "    outputs = model(X) # 모델에 입력 데이터 전달하여 예측값 계산\n",
    "    loss = criterion(outputs.squeeze(), Y) # 예측값과 타겟값을 이용하여 손실 계산\n",
    "    loss.backward() # 역전파 수행\n",
    "    optimizer.step() # 가중치 업데이트\n",
    "\n",
    "    if (epoch+1) % 100 == 0: # 100회마다 손실 출력\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 4000, loss.item()))\n",
    "\n",
    "# Inference\n",
    "X_test = torch.tensor([[[2.0]]], dtype=torch.float32).to(device) # 테스트 데이터 생성\n",
    "print('-' * 20, '추론 결과', '-' * 20)\n",
    "print(f\"Input: 2.0\")\n",
    "output = [round(num, 1) for num in model(X_test).squeeze().tolist()] # 테스트 데이터에 대한 예측값 계산\n",
    "answer = list(range(2, 21, 2)) # 정답 리스트 생성\n",
    "\n",
    "for o, a in zip(output, answer): # 예측값과 정답을 비교하여 출력\n",
    "    print(f\"Output: {o}, 정답: {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래의 Many-to-One은 여러 개의 숫자 입력 데이터를 받고, 입력된 데이터의 총 합을 예측하는 RNN 모델을 구현합니다.\n",
    "# Many-to-One\n",
    "class ManyToOneRNN(nn.Module): # Many-to-One 모델 클래스 선언\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ManyToOneRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # RNN 모듈 생성\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # 선형 변환 정의\n",
    "\n",
    "    def forward(self, x): # 순전파 함수 정의\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) # 초기 은닉 상태를 0으로 설정\n",
    "        out, _ = self.rnn(x, h0) # RNN에 입력을 전달하고 출력을 받음\n",
    "        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n",
    "\n",
    "        return out\n",
    "\n",
    "X = np.random.randint(1, 15, size=(50000, 6, 1)) # 입력 데이터 생성, 1~14 사이의 정수 50000개를 랜덤하게 생성\n",
    "Y = np.array([np.sum(x) for x in X]) # 타겟 데이터 생성, 모든 입력 데이터의 총합이 타겟\n",
    "\n",
    "X = torch.from_numpy(X.astype(np.float32)).to(device) # 입력 데이터를 텐서로 변환\n",
    "Y = torch.from_numpy(Y.astype(np.float32)).to(device) # 타겟 데이터를 텐서로 변환\n",
    "\n",
    "model = ManyToOneRNN(input_size=1, hidden_size=50, output_size=1).to(device) # 모델 생성\n",
    "criterion = nn.MSELoss() # 손실 함수 설정\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘 설정\n",
    "\n",
    "for epoch in range(4000):\n",
    "    model.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs.squeeze(), Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 4000, loss.item()))\n",
    "\n",
    "# Inference\n",
    "X_test = torch.tensor([[[2.0], [4.0], [6.0], [8.0], [10.0], [11.0]]], dtype=torch.float32).to(device)\n",
    "print('-' * 20, '추론 결과', '-' * 20)\n",
    "print(f\"Input: {X_test.squeeze().tolist()}\")\n",
    "output = round(model(X_test).item(), 1)\n",
    "answer = round(sum(X_test.squeeze().tolist()), 1)\n",
    "\n",
    "print(f\"Output: {output}, 정답: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 입력 데이터 리스트의 누적 합 리스트가 정답\n",
    "# 아래의 Many-to-Many 구조는 여러 개의 숫자 입력 데이터를 받고, 입력된 데이터의 누적합을 예측하는 RNN 모델을 구현합니다.\n",
    "# 예를 들어, 1,3,5를 입력으로 받으면 RNN 모델은 이들의 누적합인 1,4,9를 출력하도록 학습됩니다.\n",
    "\n",
    "# Many-to-Many\n",
    "class ManyToManyRNN(nn.Module): # ManyToManyRNN 클래스 선언\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ManyToManyRNN, self).__init__() # nn.Module의 초기화 함수 상속\n",
    "        self.hidden_size = hidden_size # 은닉 상태(hidden state)의 크기를 지정\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # RNN 모듈을 생성\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # 출력을 위한 선형 변환을 정의\n",
    "\n",
    "    def forward(self, x): # 순전파 함수를 정의\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) # 초기 은닉 상태를 0으로 설정\n",
    "        out, _ = self.rnn(x, h0) # RNN에 입력을 전달하고 출력을 받음\n",
    "        out = self.fc(out) # 모든 시간 단계의 출력에 대해 선형 변환을 수행\n",
    "\n",
    "        return out\n",
    "\n",
    "# 데이터 생성\n",
    "# 각각의 리스트는 0~30 사이의 랜덤한 정수를 가지는 5개의 정수로 구성, 이러한 리스트를 3000개 생성\n",
    "X = np.array([[[np.random.randint(0, 31)] for _ in range(5)] for _ in range(3000)])\n",
    "Y = np.array([np.cumsum(x) for x in X]) # 정답은 각 리스트의 누적합\n",
    "\n",
    "X = torch.from_numpy(X.astype(np.float32)) # numpy 배열을 PyTorch 텐서로 변환\n",
    "Y = torch.from_numpy(Y.astype(np.float32)) # numpy 배열을 PyTorch 텐서로 변환\n",
    "\n",
    "model = ManyToManyRNN(1, 60, 1) # 모델 생성\n",
    "\n",
    "criterion = nn.MSELoss() # 손실 함수 설정\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘 설정\n",
    "\n",
    "# 학습\n",
    "for epoch in range(4000): # 4000번의 에폭 동안 학습\n",
    "    model.zero_grad() # 기울기를 0으로 초기화\n",
    "    outputs = model(X) # 모델에 입력을 전달하고 출력을 받음\n",
    "    loss = criterion(outputs, Y.view_as(outputs)) # 손실 계산\n",
    "    loss.backward() # 역전파 수행\n",
    "    optimizer.step() # 가중치 갱신\n",
    "\n",
    "    if (epoch+1) % 100 == 0: # 100 에폭마다 손실 출력\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 4000, loss.item()))\n",
    "\n",
    "# 추론\n",
    "X_test = torch.tensor([[[i+10] for i in range(5)]], dtype=torch.float32) # 테스트 데이터 생성\n",
    "print('-' * 20, '추론 결과', '-' * 20)\n",
    "print(f\"Input: {list(range(10, 15))}\")\n",
    "output = [round(num, 1) for num in model(X_test).squeeze().tolist()] # 모델의 출력 계산\n",
    "answer = list(np.cumsum(range(10, 15))) # 정답 계산\n",
    "\n",
    "for o, a in zip(output, answer): # 모델의 출력과 정답 비교\n",
    "    print(f\"Output: {o}, 정답: {a}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
