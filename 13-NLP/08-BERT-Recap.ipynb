{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `bert-base-uncased` : Tokenize를 할 때, 대소문자를 구분하려면 `cased`, 모두 소문자로 변환하려면 `uncased` 사용합니다.\n",
    "- `add_pooling_layer` : BERT 위에 추가적으로 pooling layer를 쌓을 것인지 여부를 결정합니다. pooling layer는 `[CLS]` token의 embedding만을 뽑아 linear 연산과 activation 연산을 수행하는 layer로, BERT를 이용해 classification task를 수행할 때 주로 사용합니다.\n",
    "- `output_hidden_states` : BERT의 각 layer의 hidden state들을 출력할 것인지 여부를 결정합니다.(즉, 12개의 encoder layer가 출력한 hidden_state를 모두 반환.) False인 경우 마지막 encoder_layer의 hidden_state만 반환.\n",
    "- `ouput_attentions` : BERT의 각 layer의 attention weight들을 출력할 것인지 여부를 결정합니다.(12개의 encoder layer가 출력한 attention_weight를 모두 반환.) 여기서 attention_weight는 Attention distribution을 말함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c225804c186e438f9c0fc2c610891ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741881581c294fdb87ea94b57047a715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                  add_pooling_layer=False,\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108891648\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.num_parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "  (embeddings): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "    (position_embeddings): Embedding(512, 768)\n",
    "    (token_type_embeddings): Embedding(2, 768)\n",
    "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word_embeddings : 시퀀스 데이터의 각 토큰들 즉, 단어들을 768차원의 벡터로 맵핑하는 임베딩. vocab의 크기는 30522.\n",
    "- position_embeddings : 시퀀스 내 각 토큰들에 대한 위치정보를 반영. ```문장의 최대 길이는 512```. ```따라서 512개의 토큰들은 각각 768차원의 위치 임베딩 벡터로 맵핑된다.```\n",
    "- token_type_embeddings(segment embedding) : 각 토큰이 어떤 문장에 속하는지를 나타내는 임베딩 벡터. 한 번에 두 문장을 입력 받았을 때 문장 A에 속하는 토큰에는 0, B에 속하는 토큰에는 1을 부여한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(30522, 768, padding_idx=0)\n",
      "Embedding(512, 768)\n",
      "Embedding(2, 768)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings.word_embeddings)\n",
    "print(model.embeddings.position_embeddings)\n",
    "print(model.embeddings.token_type_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b27ed7820ef4497917f7caf5cebffd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce5621c73b444708d013e7f8354fcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037d412abc4a4cb1b731213cf9050b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 2293, 17953, 2361, 999, 102, 1045, 2293, 9932, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "input ids :  [101, 1045, 2293, 17953, 2361, 999, 102, 1045, 2293, 9932, 102]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "input_tokens :  ['[CLS]', 'i', 'love', 'nl', '##p', '!', '[SEP]', 'i', 'love', 'ai', '[SEP]']\n",
      "lenght of input_tokens :  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') ## BERT 토크나이저 불러오기\n",
    "\n",
    "tokens = tokenizer(\"I love NLP!\", \"I love ai\") ## 예제 문장 토큰화.\n",
    "print(tokens)\n",
    "\n",
    "print(\"input ids : \", tokens['input_ids']) ## 각 토큰이 가진 vocab내 idx\n",
    "print(\"token_type_ids: \", tokens['token_type_ids']) ## segment embedding\n",
    "print(\"input_tokens : \", tokenizer.convert_ids_to_tokens(tokens['input_ids'])) ## 토크나이저가 special tokens를 알아서 집어넣어준다.\n",
    "print(\"lenght of input_tokens : \", len(tokenizer.convert_ids_to_tokens(tokens['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 통해 양방향 문맥 정보가 반영된 임베딩을 만들어내는 층.\n",
    "\n",
    "첫번째 encoder layer는 embeddings 모듈의 출력값을 입력으로 받으며 총 12개의 encoder lauyer를 통과해 encoder_output이라는 출력값을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "BERT의 총 hidden_states 개수:  13\n",
      "Encoder layer의 출력 형태:  torch.Size([1, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "# bert encoder layer의 출력값 확인\n",
    "model_input = tokenizer(\"I love NLP!\" , \"I love ai\", return_tensors=\"pt\") ## 문장 토큰화 수행.\n",
    "output = model(**model_input) ## 모델에 입력\n",
    "print(len(output))\n",
    "\n",
    "print(\"BERT의 총 hidden_states 개수: \", len(output.hidden_states)) ## BERT Embedding의 hidden state 1개 + BERT Encoder의 hidden state 12개\n",
    "print(\"Encoder layer의 출력 형태: \", output.last_hidden_state.shape) ## 예시로 사용한 문장은 토크나이저에 의해 11개의 토큰으로 분할되었고 마지막 encoder의 출력이므로 (1, 11, 768)\n",
    "\n",
    "## Pretraining, Finetuning을 수행할 때는 last_hidden_state를 가지고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 26852,  2208,  2001,  2207,  4969,  2006,  2244,  2459,  1010,\n",
      "         25682,  1010,  2000,  4187, 10761,  1998,  2248,  3086,  1012,  2009,\n",
      "          2003, 20907,  1005,  1055,  2087,  1011,  3427,  2186,  1010,  3352,\n",
      "          1996,  2327,  1011,  7021,  2565,  1999,  6365,  3032,  1998, 15411,\n",
      "          2062,  2084, 16087,  2454,  2266,  3911,  1998, 25933, 18965,  1015,\n",
      "          1012,  3515,  4551, 10523,  2847,  2076,  2049,  2034,  2176,  3134,\n",
      "          2013,  4888,  1010, 27097,  2958, 11715,  2005,  1996,  2516,  1997,\n",
      "          2087,  3427,  2265,  1012,  1996,  2186,  2038,  2036,  2363,  3365,\n",
      "         27447,  1010,  2164,  1996,  3585,  7595,  2400,  2005,  2190,  4637,\n",
      "          3364,  1516,  2186,  1010, 13612,  2030,  2547,  2143,  2005,  1051,\n",
      "          6300,  5063,  1011, 10514,  1998,  1996,  3898,  5889,  9054,  2400,\n",
      "          2005,  5151,  2836,  2011,  1037,  3287,  3364,  1999,  1037,  3689,\n",
      "          2186,  1998,  5151,  2836,  2011,  1037,  2931,  3364,  1999,  1037,\n",
      "          3689,  2186,  2005,  3389, 11810,  1011, 22770,  1998,  7570,  6672,\n",
      "          2239, 11810,  1010,  4414,  1010,  2007,  2035,  2093,  2437,  2381,\n",
      "          2004,  1996,  2034,  4759,  5889,  2000,  2663,  1999,  2216,  7236,\n",
      "          1012,  1037,  2117,  2161,  2003,  1999,  2458,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}\n",
      "토큰화 된 결과:  ['[CLS]', 'squid', 'game', 'was', 'released', 'worldwide', 'on', 'september', '17', ',', '2021', ',', 'to', 'critical', 'acclaim', 'and', 'international', 'attention', '.', 'it', 'is', 'netflix', \"'\", 's', 'most', '-', 'watched', 'series', ',', 'becoming', 'the', 'top', '-', 'viewed', 'program', 'in', '94', 'countries', 'and', 'attracting', 'more', 'than', '142', 'million', 'member', 'households', 'and', 'ama', '##ssing', '1', '.', '65', 'billion', 'viewing', 'hours', 'during', 'its', 'first', 'four', 'weeks', 'from', 'launch', ',', 'surpassing', 'bridge', '##rton', 'for', 'the', 'title', 'of', 'most', 'watched', 'show', '.', 'the', 'series', 'has', 'also', 'received', 'numerous', 'accolades', ',', 'including', 'the', 'golden', 'globe', 'award', 'for', 'best', 'supporting', 'actor', '–', 'series', ',', 'miniseries', 'or', 'television', 'film', 'for', 'o', 'ye', '##ong', '-', 'su', 'and', 'the', 'screen', 'actors', 'guild', 'award', 'for', 'outstanding', 'performance', 'by', 'a', 'male', 'actor', 'in', 'a', 'drama', 'series', 'and', 'outstanding', 'performance', 'by', 'a', 'female', 'actor', 'in', 'a', 'drama', 'series', 'for', 'lee', 'jung', '-', 'jae', 'and', 'ho', '##ye', '##on', 'jung', ',', 'respectively', ',', 'with', 'all', 'three', 'making', 'history', 'as', 'the', 'first', 'korean', 'actors', 'to', 'win', 'in', 'those', 'categories', '.', 'a', 'second', 'season', 'is', 'in', 'development', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Squid Game was released worldwide on September 17, 2021, to critical acclaim and\n",
    "international attention. It is Netflix's most-watched series, becoming\n",
    "the top-viewed program in 94 countries and attracting more than 142 million\n",
    "member households and amassing 1.65 billion viewing hours during its first four\n",
    "weeks from launch, surpassing Bridgerton for the title of most watched show.\n",
    "The series has also received numerous accolades, including the Golden Globe\n",
    "Award for Best Supporting Actor – Series, Miniseries or Television Film for\n",
    "O Yeong-su and the Screen Actors Guild Award for Outstanding Performance by a\n",
    "Male Actor in a Drama Series and Outstanding Performance by a Female Actor in a\n",
    "Drama Series for Lee Jung-jae and HoYeon Jung, respectively, with all three making\n",
    "history as the first Korean actors to win in those categories. A second season is in development.\n",
    "\"\"\"\n",
    "\n",
    "## Attention Mask는 attention 계산시 유의미한 단어는 1, PAD 같이 무의미한 단어는 0으로 하는 mask를 의미한다.\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "print(inputs)\n",
    "print(\"토큰화 된 결과: \", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLM은 MASK 토큰에 있어야할 단어를 예측하는 것이므로 원래 단어가 곧 label에 해당. 따라서 기존 토큰을 그대로 복사하여 labels로 취급."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  5,   7,  16,  23,  33,  57,  74,  75,  82,  85,  89,  95, 104, 109,\n",
      "        112, 113, 114, 116, 131, 132, 137, 142, 155, 166])\n",
      "[MASK]로 교체될 토큰의 index:  tensor([  5,   7,  16,  57,  74,  82,  85,  89, 104, 109, 112, 114, 116, 131,\n",
      "        132, 137, 142, 166])\n",
      "다른 토큰으로 교체될 토큰의 index:  tensor([ 75, 155])\n",
      "tensor([[  101, 26852,  2208,  2001,  2207,   103,  2006,   103,  2459,  1010,\n",
      "         25682,  1010,  2000,  4187, 10761,  1998,   103,  3086,  1012,  2009,\n",
      "          2003, 20907,  1005,  1055,  2087,  1011,  3427,  2186,  1010,  3352,\n",
      "          1996,  2327,  1011,  7021,  2565,  1999,  6365,  3032,  1998, 15411,\n",
      "          2062,  2084, 16087,  2454,  2266,  3911,  1998, 25933, 18965,  1015,\n",
      "          1012,  3515,  4551, 10523,  2847,  2076,  2049,   103,  2176,  3134,\n",
      "          2013,  4888,  1010, 27097,  2958, 11715,  2005,  1996,  2516,  1997,\n",
      "          2087,  3427,  2265,  1012,   103, 21041,  2038,  2036,  2363,  3365,\n",
      "         27447,  1010,   103,  1996,  3585,   103,  2400,  2005,  2190,   103,\n",
      "          3364,  1516,  2186,  1010, 13612,  2030,  2547,  2143,  2005,  1051,\n",
      "          6300,  5063,  1011, 10514,   103,  1996,  3898,  5889,  9054,   103,\n",
      "          2005,  5151,   103,  2011,   103,  3287,   103,  1999,  1037,  3689,\n",
      "          2186,  1998,  5151,  2836,  2011,  1037,  2931,  3364,  1999,  1037,\n",
      "          3689,   103,   103,  3389, 11810,  1011, 22770,   103,  7570,  6672,\n",
      "          2239, 11810,   103,  4414,  1010,  2007,  2035,  2093,  2437,  2381,\n",
      "          2004,  1996,  2034,  4759,  5889, 29021,  2663,  1999,  2216,  7236,\n",
      "          1012,  1037,  2117,  2161,  2003,  1999,   103,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "\n",
    "# CLS : 101, SEP : 102번 토큰 제외하고 15% 위치 선별\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102)\n",
    "\n",
    "# 전체 토큰 중 15%를 대체할 토큰으로 선정\n",
    "selection = torch.flatten((mask_arr[0]).nonzero())\n",
    "\n",
    "# 선별된 위치의 인덱스 번호\n",
    "## 토큰의 정수값이 아니라 시퀀스 내에서 몇번째 idx의 토큰이 masking될 것인지를 말함.\n",
    "print(selection)\n",
    "\n",
    "# selection의 위치마다 0~1 값 부여\n",
    "selection_val = np.random.random(len(selection))\n",
    "\n",
    "# 80% : Mask 토큰 대체\n",
    "mask_selection = selection[np.where(selection_val >= 0.2)[0]]\n",
    "\n",
    "# 10% : 랜덤 토큰 대체\n",
    "random_selection = selection[np.where(selection_val < 0.1)[0]]\n",
    "\n",
    "print(\"[MASK]로 교체될 토큰의 index: \", mask_selection)\n",
    "print(\"다른 토큰으로 교체될 토큰의 index: \", random_selection)\n",
    "\n",
    "inputs.input_ids[0, mask_selection] = 103 ## Mask로 처리되는 위치의 토큰은 103번으로 맵핑\n",
    "inputs.input_ids[0, random_selection] = torch.randint(0, 30522, size = random_selection.shape) ## 임의의 단어로 바뀌는 위치의 토큰은 BERT의 단어 집합에서 랜덤으로 단어 추출\n",
    "\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer에 들어갈 데이터 셋 클래스\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "\n",
    "# 위에서 설정한 inputs를 데이터 셋 클래스로 지정\n",
    "dataset = Dataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning을 위한 argument 설정\n",
    "args = TrainingArguments(\n",
    "    output_dir='./', # 결과 정보를 받아볼 디렉토리\n",
    "    per_device_train_batch_size=16, # gpu 당 batch size 수\n",
    "    num_train_epochs=10 # epoch 수\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # 훈련을 수행할 모델\n",
    "    args=args, # 지정한 argument\n",
    "    train_dataset=dataset # 학습 데이터셋\n",
    ")\n",
    "\n",
    "# GPU 학습을 위해 모델을 선택된 device에 이동\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.train() # 학습 모드 설정\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Sentence Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikitext 데이터셋 불러오기\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Robert Boulter = \\n',\n",
       " '',\n",
       " ' Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \\n',\n",
       " ' In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . \\n',\n",
       " '',\n",
       " ' = = Career = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = 2000 – 2005 = = = \\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 예시를 위한 일부 데이터 선정\n",
    "data = dataset['test']['text'][:10]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 문장:  Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall .\n",
      "추출된 문장:  In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti .\n"
     ]
    }
   ],
   "source": [
    "# 모델 입력을 위해 데이터 전처리 수행\n",
    "text = []\n",
    "\n",
    "for line in data:\n",
    "  line = line.strip()\n",
    "  if line:\n",
    "    if line[0].isalpha():\n",
    "      print(\"추출된 문장: \", line)\n",
    "      text.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall .',\n",
       " 'In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti .']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "bag = [item for sentence in text for item in sentence.split('.') if item != '']\n",
    "bag_size = len(bag)\n",
    "print(bag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Robert Boulter is an English film , television and theatre actor ',\n",
       " ' He had a guest @-@ starring role on the television series The Bill in 2000 ',\n",
       " ' This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre ',\n",
       " ' He had a guest role in the television series Judge John Deed in 2002 ',\n",
       " ' In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi ',\n",
       " ' He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London ',\n",
       " ' He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall ',\n",
       " 'In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill ',\n",
       " ' He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke ',\n",
       " ' How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham ',\n",
       " ' Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn ',\n",
       " ' In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 ',\n",
       " ' He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" ',\n",
       " ' Boulter starred in the 2011 film Mercenaries directed by Paris Leonti ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = []\n",
    "sentence_b = []\n",
    "label = []\n",
    "\n",
    "for paragraph in text:\n",
    "    sentences = [sentence for sentence in paragraph.split('.') if sentence != '']\n",
    "    \n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences > 1:\n",
    "        start = random.randint(0, num_sentences-2)\n",
    "        # 50:50의 비율로 순서가 올바르거나 올바르지 않은 문장을 선정\n",
    "        if random.random() >= 0.5:\n",
    "            # this is IsNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(sentences[start+1])\n",
    "            label.append(0)\n",
    "        else:\n",
    "            index = random.randint(0, bag_size-1)\n",
    "            # this is NotNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(bag[index])\n",
    "            label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      " He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London \n",
      "---\n",
      " In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 \n",
      "\n",
      "1\n",
      " How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham \n",
      "---\n",
      " This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(label)):\n",
    "    print(label[i])\n",
    "    print(sentence_a[i] + '\\n---')\n",
    "    print(sentence_b[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs['labels'] = torch.LongTensor([label]).T\n",
    "inputs.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP 작업을 위한 모델 불러오기\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GPU 학습을 위해 모델을 선택된 device에 이동\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.train()  # 학습 모드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning을 위한 argument 설정\n",
    "args = TrainingArguments(\n",
    "    output_dir='./', # 결과 정보를 받아볼 디렉토리\n",
    "    per_device_train_batch_size=16, # gpu 당 batch size 수\n",
    "    num_train_epochs=10 # epoch 수\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # 훈련을 수행할 모델\n",
    "    args=args, # 지정한 argument\n",
    "    train_dataset=dataset # 학습 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-67611615ff7f>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.9699393272399902, metrics={'train_runtime': 2.2196, 'train_samples_per_second': 9.011, 'train_steps_per_second': 4.505, 'total_flos': 5262221107200.0, 'train_loss': 0.9699393272399902, 'epoch': 10.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
