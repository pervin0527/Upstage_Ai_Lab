{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서요약 fine-tuning하기\n",
    "- 전체 문서의 내용을 입력으로 줬을때, `생성 요약문 작성`\n",
    "- 법률 문서 : 법원 판결문 뉴스 텍스트 및 법원 주요 판결문 텍스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 실습 개요\n",
    "\n",
    "1) 실습 목적\n",
    "- 이번 실습에서는 GPT-2 모델을 PyTorch를 사용하여 구현하고 fine-tuning 해봅니다.\n",
    "- GPT 모델은 decoder-only 구조를 통해, 자연어 처리에서 뛰어난 성능을 보여주는 모델으로 GPT 모델의 구조와 downstream task 에서의 fine-tuning 방식을 이해하고, 학습이 이뤄지는 과정을 학습해봅니다\n",
    "\n",
    "2) 수강 목표\n",
    "  * GPT 모델의 구조와 fine-tuning 방법을 이해한다\n",
    "  * GPT 모델을 문서 요약을 위한 모델로 fine-tuning하고, 학습 과정에서 일어나는 연산과 input/output 형태에 대해 이해한다\n",
    "  * GPT 모델을 원하는 downstream task로 tuning 하여 사용할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습 목차\n",
    "\n",
    "1. Dataset & Tokenizing\n",
    "  * 1-1. summ_dataset class 정의\n",
    "  * 1-2. load_Data, tokenized_dataset 함수 정의\n",
    "  * 1-3. prepare_dataset 함수 정의\n",
    "2. Model & Trainer\n",
    "  * 2-1. compute_metrics 함수 정의 (rouge)\n",
    "  * 2-2. load_tokenizer_and_model_for_train 함수 정의\n",
    "  * 2-3. load_trainer_for_train 함수 정의\n",
    "  * 2-4. train 함수 정의\n",
    "  * 2-5. arguments 지정 및 학습 진행\n",
    "3. Inference & Evaluation\n",
    "  * 3-1. load_model_for_inference 함수 정의\n",
    "  * 3-2. inference 함수 정의\n",
    "  * 3-3. infer_and_eval 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /home/pervinco/miniconda3/envs/upstage/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/pervinco/.local/lib/python3.8/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /home/pervinco/.local/lib/python3.8/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/pervinco/.local/lib/python3.8/site-packages (from rouge_score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/pervinco/.local/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/pervinco/.local/lib/python3.8/site-packages (from nltk->rouge_score) (8.1.6)\n",
      "Requirement already satisfied: joblib in /home/pervinco/.local/lib/python3.8/site-packages (from nltk->rouge_score) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/pervinco/.local/lib/python3.8/site-packages (from nltk->rouge_score) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/pervinco/miniconda3/envs/upstage/lib/python3.8/site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "--2024-08-29 14:25:07--  http://xn--no-check-certificate-2t2l/\n",
      "Resolving xn--no-check-certificate-2t2l (xn--no-check-certificate-2t2l)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address ‘xn--no-check-certificate-2t2l’\n",
      "--2024-08-29 14:25:07--  https://docs.google.com/uc?export=download&id=14s5orP5j6nNOqdmGdy6DkdyhcIsn-6Zv\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.161.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=14s5orP5j6nNOqdmGdy6DkdyhcIsn-6Zv&export=download [following]\n",
      "--2024-08-29 14:25:08--  https://drive.usercontent.google.com/download?id=14s5orP5j6nNOqdmGdy6DkdyhcIsn-6Zv&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.206.193, 2404:6800:400a:813::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.206.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1074341 (1.0M) [application/octet-stream]\n",
      "Saving to: ‘./dataset/summarization//train.csv’\n",
      "\n",
      "./dataset/summariza 100%[===================>]   1.02M  3.87MB/s    in 0.3s    \n",
      "\n",
      "2024-08-29 14:25:11 (3.87 MB/s) - ‘./dataset/summarization//train.csv’ saved [1074341/1074341]\n",
      "\n",
      "FINISHED --2024-08-29 14:25:11--\n",
      "Total wall clock time: 3.6s\n",
      "Downloaded: 1 files, 1.0M in 0.3s (3.87 MB/s)\n",
      "--2024-08-29 14:25:11--  http://xn--no-check-certificate-2t2l/\n",
      "Resolving xn--no-check-certificate-2t2l (xn--no-check-certificate-2t2l)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address ‘xn--no-check-certificate-2t2l’\n",
      "--2024-08-29 14:25:11--  https://docs.google.com/uc?export=download&id=1wMhyqoJ0D7xQepW4y7Ireq2U2X61ndYz\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.161.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1wMhyqoJ0D7xQepW4y7Ireq2U2X61ndYz&export=download [following]\n",
      "--2024-08-29 14:25:11--  https://drive.usercontent.google.com/download?id=1wMhyqoJ0D7xQepW4y7Ireq2U2X61ndYz&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.207.97, 2404:6800:400a:813::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.207.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 858298 (838K) [application/octet-stream]\n",
      "Saving to: ‘./dataset/summarization/test.csv’\n",
      "\n",
      "./dataset/summariza 100%[===================>] 838.18K  3.37MB/s    in 0.2s    \n",
      "\n",
      "2024-08-29 14:25:14 (3.37 MB/s) - ‘./dataset/summarization/test.csv’ saved [858298/858298]\n",
      "\n",
      "FINISHED --2024-08-29 14:25:14--\n",
      "Total wall clock time: 2.9s\n",
      "Downloaded: 1 files, 838K in 0.2s (3.37 MB/s)\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score\n",
    "!wget –no-check-certificate 'https://docs.google.com/uc?export=download&id=14s5orP5j6nNOqdmGdy6DkdyhcIsn-6Zv' -O ./dataset/summarization//train.csv\n",
    "!wget –no-check-certificate 'https://docs.google.com/uc?export=download&id=1wMhyqoJ0D7xQepW4y7Ireq2U2X61ndYz' -O ./dataset/summarization/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:25:15.709127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-29 14:25:15.784892: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-29 14:25:16.170858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-08-29 14:25:16.170911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-08-29 14:25:16.170915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summ_dataset(Dataset):\n",
    "    \"\"\"dataframe을 torch dataset class로 변환\"\"\"\n",
    "    def __init__(self, document, tokenizer):\n",
    "      self.dataset = document\n",
    "      self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        input_ids=torch.LongTensor(self.dataset[\"input_ids\"][idx])\n",
    "        labels=torch.LongTensor(self.dataset[\"labels\"][idx])\n",
    "\n",
    "        attention_mask=input_ids.ne(self.tokenizer.pad_token_id) ## padding token은 attention 계산에 반영되면 안되니까 mask를 정의한다..\n",
    "\n",
    "        return dict(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2.load_data , construct_tokenized_dataset 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    \"\"\"csv file을 dataframe으로 load\"\"\"\n",
    "    dataset = pd.read_csv(dataset_dir)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, doc_tokenizer, sum_tokenizer, doc_max_length, sum_max_len, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    토크나이징을 위한 함수. training과 inference 단계에서의 토크나이징이 별도로 구축되어 있다.\n",
    "    - 학습일 때는 본문과 요약이 함께 입력된다. --> 본문 [SEP] 요약\n",
    "    - 반면 추론 단계에서는 본문만 입력되어 요약을 생성해야함.\n",
    "    \"\"\"\n",
    "    ## 추론 단계\n",
    "    if mode == \"infer\":\n",
    "      ## inference 시에는 document 만 주어지고, 마지막에 bos_token을 붙여 생성 시작하게 한다.\n",
    "      document_text = dataset['document']\n",
    "      summ_text = dataset['summary']\n",
    "\n",
    "      ## document + bos\n",
    "      ## <pad> <pad> d_1 d_2 d_3 ... d_n <bos>\n",
    "      document = [doc_tokenizer(documents, padding = 'max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for documents in document_text.values]\n",
    "      \n",
    "      # labels에는 요약문만큼의 빈칸으로 채워준 후 모델이 예측하도록 함\n",
    "      labels = [[-100] * sum_max_len for _ in document]\n",
    "\n",
    "      out = {\"input_ids\": document, \"labels\": labels}\n",
    "      print(\"inference을 위한 데이터에서 tokenizing 된 input 형태\")\n",
    "      print(document[-1])\n",
    "      print(doc_tokenizer.convert_ids_to_tokens(document[-1]))\n",
    "      print()\n",
    "\n",
    "    elif mode == \"train\":\n",
    "      document_text = dataset['document']\n",
    "      summary_text = dataset['summary']\n",
    "      ## document 와 summary를 이어 붙여서 모델 학습에 사용. \n",
    "      ## document 뒤에는 bos_token 을 붙여 생성 시작을 명시하고, summary 를 붙인 후 맨 뒤에는 eos_token 으로 생성의 끝을 명시.\n",
    "      ## ⭐️ document를 padding 할 때는 side를 left로 주고, summary를 padding 할 때는 side를 right로 줘서 연속된 문장이 생성될 수 있도록 한다.\n",
    "      ## ⭐️ <pad> <pad> d_1 d_2 d_3 ... d_n <bos> s_1 s_2 ... s_m <eos> <pad> <pad>\n",
    "      document = [doc_tokenizer(documents, padding='max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for documents in document_text.values]\n",
    "      summary = [sum_tokenizer(summaries + sum_tokenizer.eos_token, padding = 'max_length',truncation=True, max_length=sum_max_len, add_special_tokens=True)['input_ids'] for summaries in summary_text.values]\n",
    "\n",
    "      ## 구성해둔 document 와 summary를 결합하여 input 준비\n",
    "      tokenized_senetences = [document + summary for (document, summary) in zip(document, summary)]\n",
    "      ## document는 생성할 내용이 아니므로 -100으로 label을 부여한다.\n",
    "      # Input : <pad> <pad> d_1  d_2  d_3  ... d_n  <bos> s_1 s_2 ... s_m <eos> <pad> <pad>\n",
    "      # Label : -100  -100    -100 -100 -100  ... -100  -100  s_1 s_2 ... s_m <eos> -100 -100\n",
    "\n",
    "      labels = [[-100] * len(document) + summary for (document, summary) in zip(document, summary)]\n",
    "      ## ⭐️ Q. 다음에 올 Token을 생성하도록 학습해야 되니까 s_1의 label은 한 칸씩 밀린 s_2가 들어가야 되지 않나요?\n",
    "      # A. Transformer 라이브러리의 GPT 구현(https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1103-L1104)을 보면, \n",
    "      # 모델의 Logit을 [: -1]만 가져오고, Label은 [1: ]을 가져와서 Loss를 계산하게 됩니다.\n",
    "      # 즉, Input과 Label이 한 칸씩 밀린채로 입력을 넣지 않아도, 내부 구현에 의해 자동으로 밀린 채로 계산이 됩니다.\n",
    "\n",
    "      # padding 된 부분이 학습되지 않도록 -100 으로 치환\n",
    "      labels = [[-100 if token == sum_tokenizer.pad_token_id else token for token in l] for l in labels]\n",
    "      out = {\"input_ids\": tokenized_senetences, \"labels\": labels}\n",
    "\n",
    "      print(\"학습을 위한 데이터에서 tokenizing 된 input 형태\")\n",
    "      print(tokenized_senetences[-1])\n",
    "      print(doc_tokenizer.convert_ids_to_tokens(tokenized_senetences[-1]))\n",
    "      print()\n",
    "\n",
    "      print(\"학습을 위한 데이터에서 label의 형태\")\n",
    "      print(labels[-1])\n",
    "      print()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3. prepare_dataset 함수 정의\n",
    "- 앞서 정의한 함수들 기반으로 데이터셋 준비하는 함수\n",
    "\n",
    "** 전처리과정 ** \n",
    "1. train.csv / test.csv 파일을 pd.dataframe 로 다운로드 해준다. <br>\n",
    "2. train/validation set을 나눠준다. (7.5:2.5) <br>\n",
    "3. 전체 문서와 요약 데이터를 모두 tokenizing 해준다.  <br>\n",
    "4. 요약(label) 데이터는 Padding 된 부분은 loss 가 흐르지 않도록 -100 으로 치환해준다. <br>\n",
    "5. tokenizing 된 데이터를 summ_dataset class로 반환해준다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_dir, doc_tokenizer,sum_tokenizer,doc_max_len, sum_max_len):\n",
    "    \"\"\"학습(train)과 평가(test)를 위한 데이터셋을 준비\"\"\"\n",
    "    # load_data\n",
    "    train_dataset = load_data(os.path.join(dataset_dir, \"./dataset/summarization/train.csv\"))\n",
    "    test_dataset = load_data(os.path.join(dataset_dir, \"./dataset/summarization/test.csv\"))\n",
    "\n",
    "    # split train / validation = 7.5 : 2.5\n",
    "    train_dataset, val_dataset = train_test_split(train_dataset,test_size=0.2,random_state=42)\n",
    "\n",
    "    ### tokenizer 에 들어가기 전 데이터 형태\n",
    "    print(\"tokenizer 에 들어가는 데이터 형태\")\n",
    "    print(train_dataset.iloc[0])\n",
    "\n",
    "    # tokenizing\n",
    "    print(\"train tokenizing...\")\n",
    "    tokenized_train = tokenized_dataset(train_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len)\n",
    "    \n",
    "    print(\"valid tokenizing...\")\n",
    "    tokenized_val = tokenized_dataset(val_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len)\n",
    "    \n",
    "    print(\"test tokenizing...\")\n",
    "    tokenized_test = tokenized_dataset(test_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len, mode=\"infer\")\n",
    "\n",
    "    # make dataset for pytorch.\n",
    "    summ_train_dataset = summ_dataset(tokenized_train, doc_tokenizer)\n",
    "    summ_val_dataset = summ_dataset(tokenized_val, doc_tokenizer)\n",
    "    summ_test_dataset = summ_dataset(tokenized_test, doc_tokenizer)\n",
    "\n",
    "    print(\"--- dataset class Done ---\")\n",
    "\n",
    "    return summ_train_dataset , summ_val_dataset, summ_test_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2️⃣ Model & Trainer\n",
    "- huggingface 에서 사전학습된(pre-trained) 모델을 불러옵니다.\n",
    "- huggingface 의 Trainer 모듈을 정의하고 학습에 사용될 Arguments 들을 지정해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. compute_metrics 함수 정의\n",
    "- 학습 중 validation 할 때 사용될 평가지표 정의하는 함수\n",
    "- 해당 실습에서는 Rouge Score를 Metric으로 사용\n",
    "\n",
    "Q. Rouge Score?\n",
    "\n",
    "문서 요약, 기계 번역 등의 task 에서 모델이 생성한 문장을 평가하기 위한 지표.\n",
    "\n",
    "다양한 종류의 score 가 있지만, 이 중 ROUGE-N의 F1-score(recall 과 precisiond을 반영)를 사용합니다.\n",
    "\n",
    "- ROUGE-N : N gram에 기반하여, 모델이 생성한 요약문과 정답 요약문 중 겹치는 단어의 개수를 확인하여 생성한 요약문을 평가하는 평가지표\n",
    "- ROUGE-N recall : 두 요약문 중 겹치는 N-gram의 수 / 정답 요약문의 N-gram의 수\n",
    "- ROUGE-N precision : 두 요약문 중 겹치는 N-gram의 수 / 모델이 생성한 요약문의 N-gram의 수  \n",
    "\n",
    "예) ROUGE-1\n",
    "- 모델이 생성한 요약문(R) : \"the hello a cat dog fox jumps\"\n",
    "- 정답 요약문 (T) : \"the fox jumps\" \n",
    "- 두 요약문 중 겹치는 1-gram ['the', 'fox', 'jumps'] 이므로 ROUGE-1 recall 은 3/3, ROUGE-1 precision 은 3/7 = 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(args, pred):\n",
    "    # tokenizer load\n",
    "    MODEL_NAME = args.model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # 예측값과 정답\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    if isinstance(preds, tuple):\n",
    "      preds = preds[0]\n",
    "\n",
    "    # preds에서 document 이후부터 생성된 summary를 decode\n",
    "    ## GPT-2는 encoder-decoder 구조가 아니기 때문에 입력된 document에 이어서 summary를 생성하므로\n",
    "    ## document 부분을 별도로 잘라내야 한다.\n",
    "    decoded_preds = tokenizer.batch_decode(preds[:, args.doc_max_len:], skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels[:, args.doc_max_len:], skip_special_tokens=True)\n",
    "\n",
    "    # rouge score 계산\n",
    "    metric = datasets.load_metric(\"rouge\")\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # ROUGE 결과를 추출\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return {\n",
    "        'Rouge-2' : result['rouge2']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2.load_tokenizer_and_model_for_train 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model_for_train(args):\n",
    "    \"\"\"학습(train)을 위한 사전학습(pretrained) 토크나이저와 모델을 huggingface에서 load\"\"\"\n",
    "    # model과 tokenizer를 load\n",
    "    MODEL_NAME = args.model_name\n",
    "    doc_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    sum_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n",
    "\n",
    "    # model의 hyperparameter를 setting\n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    print(model_config)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, config=model_config)\n",
    "    print(\"--- Modeling Done ---\")\n",
    "    \n",
    "    return doc_tokenizer, sum_tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3.load_trainer_for_train 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer_for_train(args, model, summ_train_dataset, summ_val_dataset):\n",
    "    \"\"\"학습(train)을 위한 huggingface trainer 설정\"\"\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.save_path + \"results\",  # output directory\n",
    "        save_total_limit=args.save_limit,  # number of total save model.\n",
    "        save_steps=args.save_step,  # model saving step.\n",
    "        num_train_epochs=args.epochs,  # total number of training epochs\n",
    "        learning_rate=args.lr,  # learning_rate\n",
    "        per_device_train_batch_size=args.batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "        warmup_steps=args.warmup_steps,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=args.weight_decay,  # strength of weight decay\n",
    "        logging_dir=args.save_path + \"logs\",  # directory for storing logs\n",
    "        logging_steps=args.logging_steps,  # log saving step.\n",
    "        evaluation_strategy=\"steps\",  # evaluation strategy to adopt during training\n",
    "            # `no`: No evaluation during training.\n",
    "            # `steps`: Evaluate every `eval_steps`.\n",
    "            # `epoch`: Evaluate every end of epoch.\n",
    "        eval_steps=args.eval_steps,  # evaluation step.\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    ## Add callback & optimizer & scheduler\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5, early_stopping_threshold=0.001\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=args.weight_decay,\n",
    "        amsgrad=False,\n",
    "    )\n",
    "\n",
    "    print(\"--- Set training arguments Done ---\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=summ_train_dataset,  # training dataset\n",
    "        eval_dataset=summ_val_dataset,  # evaluation dataset\n",
    "        compute_metrics=lambda p: compute_metrics(args, p),\n",
    "        callbacks=[MyCallback],\n",
    "        optimizers=(\n",
    "            optimizer,\n",
    "            get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=args.warmup_steps,\n",
    "                    num_training_steps=len(summ_train_dataset) * args.epochs,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    print(\"--- Set Trainer Done ---\")\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-4.train 함수 정의\n",
    "\n",
    "** 학습동작과정 ** \n",
    "1. 실험에 영향을 주는 모든 seed를 고정해준다. <br>\n",
    "2. 사용할 gpu를 device에 할당해준다. <br>\n",
    "3. tokenizer와 model을 불러온후, model을 device에 할당해준다. <br>\n",
    "4. 학습에 사용될 summ_dataset 을 불러온다.<br>\n",
    "5. 학습에 사용될 Trainer 를 불러온다.<br>\n",
    "6. 학습을 진행한후에 best_model을 저장해준다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \"\"\"모델을 학습(train)하고 best model을 저장\"\"\"\n",
    "    # fix a seed\n",
    "    pl.seed_everything(seed=42, workers=False)\n",
    "\n",
    "    # set device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # set model and tokenizer\n",
    "    doc_tokenizer, sum_tokenizer , model = load_tokenizer_and_model_for_train(args)\n",
    "    model.to(device)\n",
    "\n",
    "    # set data\n",
    "    summ_train_dataset, summ_val_dataset, summ_test_dataset, test_dataset = prepare_dataset(args.dataset_dir,doc_tokenizer, sum_tokenizer,args.doc_max_len,args.sum_max_len)\n",
    "    # set trainer\n",
    "    trainer = load_trainer_for_train(args, model, summ_train_dataset, summ_val_dataset)\n",
    "\n",
    "    # train model\n",
    "    print(\"--- Start train ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Finish train ---\")\n",
    "    model.save_pretrained(\"./best_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-5.arguments 지정 및 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"MrBananaHuman/kogpt2_small\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/upstage/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Modeling Done ---\n",
      "tokenizer 에 들어가는 데이터 형태\n",
      "document    [1] 甲이 토지소유자 乙에게서 토지를 임차한 후 주유소 영업을 위하여 지하에 유류...\n",
      "summary     토지에 매설된 유류저장조는 토지와 일체를 이루는 구성 부분이 아니므로 토지 임차인이...\n",
      "Name: 249, dtype: object\n",
      "train tokenizing...\n",
      "학습을 위한 데이터에서 tokenizing 된 input 형태\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12712, 600, 979, 723, 1042, 545, 3772, 1314, 13479, 12712, 1450, 1171, 7832, 979, 135, 3500, 723, 1254, 23745, 11328, 12712, 600, 979, 7303, 1172, 4658, 1036, 476, 752, 2653, 12712, 600, 979, 1667, 16747, 7832, 694, 5242, 27950, 10509, 7491, 5208, 10703, 2867, 1377, 386, 8459, 7309, 8065, 1104, 319, 5295, 14660, 12712, 600, 979, 1042, 545, 16798, 11242, 1487, 10093, 1042, 545, 2336, 1681, 1249, 5490, 1377, 386, 8459, 6650, 549, 1673, 2792, 11242, 1487, 10093, 1042, 545, 2823, 18961, 12712, 600, 29958, 5626, 13821, 7540, 1264, 11242, 1487, 14660, 1042, 545, 727, 1325, 9295, 20622, 1042, 545, 7636, 1957, 10093, 5901, 12763, 901, 25168, 9087, 1219, 1027, 1006, 23171, 6694, 14421, 21415, 0, 12712, 600, 979, 723, 1042, 545, 3772, 1377, 386, 8459, 7309, 8065, 1104, 14660, 12712, 600, 979, 1042, 545, 16798, 47506, 10093, 1042, 545, 2336, 1681, 1249, 6694, 11242, 1487, 14660, 1042, 545, 727, 1325, 9295, 20622, 1042, 545, 7636, 1957, 10093, 5901, 12763, 1006, 23171, 12925, 11242, 1487, 10093, 1042, 545, 2823, 18961, 12712, 600, 29958, 5626, 1580, 1424, 2, 1, 1, 1]\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '▁소멸', '시', '효', '의', '▁기', '산', '일은', '▁채', '무의', '▁소멸', '이라고', '▁하는', '▁법률', '효', '과', '▁발생', '의', '▁요', '건에', '▁해당하는', '▁소멸', '시', '효', '▁기간', '▁계', '산의', '▁시', '발', '점', '으로서', '▁소멸', '시', '효', '▁항', '변의', '▁법률', '요', '건을', '▁구성하는', '▁구체적인', '▁사실에', '▁해당', '하므로', '▁이는', '▁변', '론', '주의의', '▁적용', '▁대상이', '고,', '따', '라서', '▁본래의', '▁소멸', '시', '효', '▁기', '산', '일과', '▁당사', '자가', '▁주장하는', '▁기', '산', '일이', '▁서로', '▁다른', '▁경우에는', '▁변', '론', '주의의', '▁원칙', '상', '▁법', '원은', '▁당사', '자가', '▁주장하는', '▁기', '산', '일을', '▁기준으로', '▁소멸', '시', '효를', '▁계산', '하여야', '▁하는데,', '이는', '▁당사', '자가', '▁본래의', '▁기', '산', '일', '보다', '▁뒤의', '▁날짜를', '▁기', '산', '일로', '▁하여', '▁주장하는', '▁경우는', '▁물론이고', '특', '별한', '▁사정이', '▁없는', '▁한', '▁그', '▁반대의', '▁경우에', '▁있어서도', '▁마찬가지이다.', '<s>', '▁소멸', '시', '효', '의', '▁기', '산', '일은', '▁변', '론', '주의의', '▁적용', '▁대상이', '고,', '▁본래의', '▁소멸', '시', '효', '▁기', '산', '일과', '▁당사자', '▁주장하는', '▁기', '산', '일이', '▁서로', '▁다른', '▁경우에', '▁당사', '자가', '▁본래의', '▁기', '산', '일', '보다', '▁뒤의', '▁날짜를', '▁기', '산', '일로', '▁하여', '▁주장하는', '▁경우는', '▁물론이고', '▁그', '▁반대의', '▁경우에도', '▁당사', '자가', '▁주장하는', '▁기', '산', '일을', '▁기준으로', '▁소멸', '시', '효를', '▁계산', '해야', '▁한다.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "학습을 위한 데이터에서 label의 형태\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 12712, 600, 979, 723, 1042, 545, 3772, 1377, 386, 8459, 7309, 8065, 1104, 14660, 12712, 600, 979, 1042, 545, 16798, 47506, 10093, 1042, 545, 2336, 1681, 1249, 6694, 11242, 1487, 14660, 1042, 545, 727, 1325, 9295, 20622, 1042, 545, 7636, 1957, 10093, 5901, 12763, 1006, 23171, 12925, 11242, 1487, 10093, 1042, 545, 2823, 18961, 12712, 600, 29958, 5626, 1580, 1424, 2, -100, -100, -100]\n",
      "\n",
      "valid tokenizing...\n",
      "학습을 위한 데이터에서 tokenizing 된 input 형태\n",
      "[1018, 15, 1029, 5269, 21716, 14045, 2456, 14525, 1220, 162, 37267, 4255, 6204, 25400, 1565, 2330, 1315, 10092, 1016, 1503, 25170, 7513, 492, 1056, 2932, 1231, 492, 941, 1565, 286, 499, 123, 952, 32525, 1096, 5169, 1339, 1249, 1565, 3397, 25689, 1045, 8086, 155, 1614, 1208, 10092, 723, 6239, 10837, 1006, 8435, 13691, 696, 286, 499, 123, 2539, 15419, 2927, 784, 572, 712, 5023, 663, 137, 941, 492, 397, 723, 7309, 272, 2153, 1627, 2927, 2329, 29840, 3763, 15991, 513, 452, 723, 3423, 687, 8990, 13, 766, 761, 17608, 1290, 39055, 766, 776, 745, 4658, 8990, 18781, 2422, 445, 1283, 5158, 557, 3848, 15002, 3685, 6445, 15591, 40912, 5778, 1080, 492, 726, 10185, 726, 1356, 776, 4198, 143, 2927, 784, 572, 712, 5023, 663, 137, 941, 492, 397, 600, 950, 382, 41776, 10805, 13336, 7198, 15, 3027, 382, 47988, 23075, 19, 14517, 29396, 5258, 6567, 1010, 11, 42516, 760, 5250, 44972, 9368, 1055, 13983, 3, 34111, 7203, 1120, 725, 543, 2989, 24880, 1055, 1673, 24534, 1136, 1019, 1024, 12029, 885, 784, 572, 712, 5023, 663, 137, 941, 492, 397, 10834, 2351, 1356, 8150, 572, 712, 1187, 39316, 1198, 452, 582, 967, 4350, 2416, 6962, 14045, 0, 1220, 162, 37267, 4255, 6204, 1087, 1565, 2330, 1315, 10092, 1016, 1503, 25170, 7513, 492, 1056, 2932, 1231, 492, 941, 1565, 286, 499, 123, 952, 32525, 1096, 5169, 1339, 1249, 1565, 3397, 25689, 1045, 8086, 1006, 1614, 1208, 10092, 723, 6239, 10837, 2927, 784, 572, 712, 5023, 663, 137, 941, 7832, 723, 7309, 272, 4033, 1627, 2927, 2329, 5208, 2060, 5158, 557, 3848, 15002]\n",
      "['▁가', '.', '▁지', '목이', '▁대인', '▁토지', '상의', '▁건물이', '▁등', '기', '부나', '▁관리', '대장', '상으로는', '▁용', '도가', '▁모두', '▁주택', '으로', '▁되어', '▁있음에도', '▁건축', '법', '▁소', '정의', '▁적', '법', '한', '▁용', '도', '변', '경', '허', '가나', '▁신', '고도', '▁없이', '▁다른', '▁용', '도에', '▁사용되고', '▁있는', '▁경우,', '그', '▁토', '지는', '▁주택', '의', '▁부지', '이므로', '▁그', '▁후의', '▁불법', '용', '도', '변', '경', '과는', '▁상관없이', '▁택', '지', '소', '유', '상한', '에', '관', '한', '법', '률', '의', '▁적용', '대', '상이', '▁되는', '▁택', '지에', '▁해당한다.', '나.', '▁선조', '분', '묘', '의', '▁유지', '와', '▁보존', ',', '종', '족', '▁간의', '▁화', '목,', '종', '중', '재', '산의', '▁보존', '관리', '▁등을', '목', '적으로', '▁공동', '선', '조의', '▁후손', '들로', '▁이루어진', '▁종족', '단체', '로서의', '▁비', '법', '인', '▁사단', '인', '▁종', '중', '은,', '구', '▁택', '지', '소', '유', '상한', '에', '관', '한', '법', '률', '시', '행', '령', '(199', '3.', '5.', '10', '.', '▁대통령', '령', '▁제13', '88', '2', '호로', '▁개정', '되기', '▁전의', '▁것', ')', '▁제12', '조', '▁제2', '호에서', '▁규정', '하는', '▁제사', '<unk>', '종교', '▁기타', '▁공', '익', '사', '업을', '▁영위', '하는', '▁법', '인이라고', '▁할', '▁수', '▁없', '으므로,', '택', '지', '소', '유', '상한', '에', '관', '한', '법', '률', '▁시행', '▁당시', '▁종', '중이', '소', '유', '하여', '▁선조의', '▁분', '묘', '수', '호', '▁등에', '▁직접', '▁사용하는', '▁토지', '<s>', '▁등', '기', '부나', '▁관리', '대장', '▁상', '▁용', '도가', '▁모두', '▁주택', '으로', '▁되어', '▁있음에도', '▁건축', '법', '▁소', '정의', '▁적', '법', '한', '▁용', '도', '변', '경', '허', '가나', '▁신', '고도', '▁없이', '▁다른', '▁용', '도에', '▁사용되고', '▁있는', '▁경우,', '▁그', '▁토', '지는', '▁주택', '의', '▁부지', '이므로', '▁택', '지', '소', '유', '상한', '에', '관', '한', '▁법률', '의', '▁적용', '대', '산이', '▁되는', '▁택', '지에', '▁해당', '하고,', '▁공동', '선', '조의', '▁후손']\n",
      "\n",
      "학습을 위한 데이터에서 label의 형태\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1220, 162, 37267, 4255, 6204, 1087, 1565, 2330, 1315, 10092, 1016, 1503, 25170, 7513, 492, 1056, 2932, 1231, 492, 941, 1565, 286, 499, 123, 952, 32525, 1096, 5169, 1339, 1249, 1565, 3397, 25689, 1045, 8086, 1006, 1614, 1208, 10092, 723, 6239, 10837, 2927, 784, 572, 712, 5023, 663, 137, 941, 7832, 723, 7309, 272, 4033, 1627, 2927, 2329, 5208, 2060, 5158, 557, 3848, 15002]\n",
      "\n",
      "test tokenizing...\n",
      "inference을 위한 데이터에서 tokenizing 된 input 형태\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8806, 45399, 10, 135, 564, 820, 513, 3687, 14856, 820, 4220, 1075, 7618, 11040, 1041, 442, 484, 1187, 6768, 453, 979, 724, 5416, 155, 14856, 820, 4649, 4242, 14202, 2564, 8806, 572, 578, 1016, 1026, 3, 1019, 1045, 18029, 153, 1692, 1045, 5901, 1577, 29460, 950, 754, 950, 2831, 14856, 820, 4220, 1691, 3603, 8564, 1500, 39814, 28332, 8806, 950, 1832, 16271, 3733, 1019, 5095, 2219, 162, 3325, 250, 1238, 8806, 950, 7928, 8708, 215, 8626, 8806, 950, 1832, 1191, 37613, 2271, 1581, 0]\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '▁행정', '행위', '(', '과', '세', '처', '분', ')의', '▁취소', '처', '분의', '▁위', '법이', '▁중대', '하고', '명', '백', '하여', '▁당연', '무', '효', '이', '거나,', '그', '▁취소', '처', '분에', '▁대하여', '▁소원', '▁또는', '▁행정', '소', '송', '으로', '▁다', '<unk>', '▁수', '▁있는', '▁명문', '규', '정이', '▁있는', '▁경우는', '▁별', '론,', '행', '정', '행', '위의', '▁취소', '처', '분의', '▁취', '소에', '▁의하여', '▁이미', '▁효력을', '▁상실한', '▁행정', '행', '위를', '▁소생', '시킬', '▁수', '▁없고,', '그러', '기', '▁위하여', '는', '▁원', '▁행정', '행', '위와', '▁동일', '내', '용의', '▁행정', '행', '위를', '▁다시', '▁행할', '▁수밖에', '▁없다.', '<s>']\n",
      "\n",
      "--- dataset class Done ---\n",
      "--- Set training arguments Done ---\n",
      "--- Set Trainer Done ---\n",
      "--- Start train ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/.local/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/2000 01:45 < 00:45, 13.24 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.525000</td>\n",
       "      <td>2.182662</td>\n",
       "      <td>13.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.458100</td>\n",
       "      <td>2.210392</td>\n",
       "      <td>13.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.844200</td>\n",
       "      <td>2.292986</td>\n",
       "      <td>14.362700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>2.376865</td>\n",
       "      <td>15.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>2.460108</td>\n",
       "      <td>12.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>2.499961</td>\n",
       "      <td>13.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>2.561069</td>\n",
       "      <td>15.711100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_235648/2187569660.py:20: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"rouge\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd22d9d06764fa99d98af989494815d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finish train ---\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "class args():\n",
    "    \"\"\"학습(train)과 추론(infer)에 사용되는 arguments 관리하는 class\"\"\"\n",
    "    dataset_dir = \"./\"\n",
    "    model_type = \"gpt2\"\n",
    "    model_name = 'MrBananaHuman/kogpt2_small'\n",
    "    save_path = \"./\"\n",
    "    save_step = 400\n",
    "    logging_steps = 200\n",
    "    eval_steps = 200\n",
    "    save_limit = 5\n",
    "    seed = 42\n",
    "    epochs = 20 # 10\n",
    "    batch_size = 4  # 메모리 상황에 맞게 조절 e.g) 16 or 32\n",
    "    doc_max_len = 196\n",
    "    sum_max_len = 64\n",
    "    lr = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 5\n",
    "    scheduler = \"linear\"\n",
    "    model_dir = \"./best_model\" #추론 시, 저장된 모델 불러오는 경로 설정\n",
    "    \n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
