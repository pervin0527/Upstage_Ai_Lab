{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## á„†á…®á†«á„‰á…¥á„‹á…­á„‹á…£á†¨ fine-tuningí•˜ê¸°\n",
    "- ì „ì²´ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì…ë ¥ìœ¼ë¡œ ì¤¬ì„ë•Œ, `ìƒì„± ìš”ì•½ë¬¸ ì‘ì„±`\n",
    "- ë²•ë¥  ë¬¸ì„œ : ë²•ì› íŒê²°ë¬¸ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ë° ë²•ì› ì£¼ìš” íŒê²°ë¬¸ í…ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ì‹¤ìŠµ ê°œìš”\n",
    "\n",
    "1) ì‹¤ìŠµ ëª©ì \n",
    "- ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” GPT-2 ëª¨ë¸ì„ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•˜ê³  fine-tuning í•´ë´…ë‹ˆë‹¤.\n",
    "- GPT ëª¨ë¸ì€ decoder-only êµ¬ì¡°ë¥¼ í†µí•´, ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ëª¨ë¸ìœ¼ë¡œ GPT ëª¨ë¸ì˜ êµ¬ì¡°ì™€ downstream task ì—ì„œì˜ fine-tuning ë°©ì‹ì„ ì´í•´í•˜ê³ , í•™ìŠµì´ ì´ë¤„ì§€ëŠ” ê³¼ì •ì„ í•™ìŠµí•´ë´…ë‹ˆë‹¤\n",
    "\n",
    "2) ìˆ˜ê°• ëª©í‘œ\n",
    "  * GPT ëª¨ë¸ì˜ êµ¬ì¡°ì™€ fine-tuning ë°©ë²•ì„ ì´í•´í•œë‹¤\n",
    "  * GPT ëª¨ë¸ì„ ë¬¸ì„œ ìš”ì•½ì„ ìœ„í•œ ëª¨ë¸ë¡œ fine-tuningí•˜ê³ , í•™ìŠµ ê³¼ì •ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì—°ì‚°ê³¼ input/output í˜•íƒœì— ëŒ€í•´ ì´í•´í•œë‹¤\n",
    "  * GPT ëª¨ë¸ì„ ì›í•˜ëŠ” downstream taskë¡œ tuning í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì‹¤ìŠµ ëª©ì°¨\n",
    "\n",
    "1. Dataset & Tokenizing\n",
    "  * 1-1. summ_dataset class ì •ì˜\n",
    "  * 1-2. load_Data, tokenized_dataset í•¨ìˆ˜ ì •ì˜\n",
    "  * 1-3. prepare_dataset í•¨ìˆ˜ ì •ì˜\n",
    "2. Model & Trainer\n",
    "  * 2-1. compute_metrics í•¨ìˆ˜ ì •ì˜ (rouge)\n",
    "  * 2-2. load_tokenizer_and_model_for_train í•¨ìˆ˜ ì •ì˜\n",
    "  * 2-3. load_trainer_for_train í•¨ìˆ˜ ì •ì˜\n",
    "  * 2-4. train í•¨ìˆ˜ ì •ì˜\n",
    "  * 2-5. arguments ì§€ì • ë° í•™ìŠµ ì§„í–‰\n",
    "3. Inference & Evaluation\n",
    "  * 3-1. load_model_for_inference í•¨ìˆ˜ ì •ì˜\n",
    "  * 3-2. inference í•¨ìˆ˜ ì •ì˜\n",
    "  * 3-3. infer_and_eval í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /home/pervinco/miniconda3/envs/upstage/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/pervinco/.local/lib/python3.8/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /home/pervinco/.local/lib/python3.8/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/pervinco/.local/lib/python3.8/site-packages (from rouge_score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/pervinco/.local/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/pervinco/.local/lib/python3.8/site-packages (from nltk->rouge_score) (8.1.6)\n",
      "Requirement already satisfied: joblib in /home/pervinco/.local/lib/python3.8/site-packages (from nltk->rouge_score) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/pervinco/.local/lib/python3.8/site-packages (from nltk->rouge_score) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/pervinco/miniconda3/envs/upstage/lib/python3.8/site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "--2024-08-29 14:25:07--  http://xn--no-check-certificate-2t2l/\n",
      "Resolving xn--no-check-certificate-2t2l (xn--no-check-certificate-2t2l)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address â€˜xn--no-check-certificate-2t2lâ€™\n",
      "--2024-08-29 14:25:07--  https://docs.google.com/uc?export=download&id=14s5orP5j6nNOqdmGdy6DkdyhcIsn-6Zv\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.161.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=14s5orP5j6nNOqdmGdy6DkdyhcIsn-6Zv&export=download [following]\n",
      "--2024-08-29 14:25:08--  https://drive.usercontent.google.com/download?id=14s5orP5j6nNOqdmGdy6DkdyhcIsn-6Zv&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.206.193, 2404:6800:400a:813::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.206.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1074341 (1.0M) [application/octet-stream]\n",
      "Saving to: â€˜./dataset/summarization//train.csvâ€™\n",
      "\n",
      "./dataset/summariza 100%[===================>]   1.02M  3.87MB/s    in 0.3s    \n",
      "\n",
      "2024-08-29 14:25:11 (3.87 MB/s) - â€˜./dataset/summarization//train.csvâ€™ saved [1074341/1074341]\n",
      "\n",
      "FINISHED --2024-08-29 14:25:11--\n",
      "Total wall clock time: 3.6s\n",
      "Downloaded: 1 files, 1.0M in 0.3s (3.87 MB/s)\n",
      "--2024-08-29 14:25:11--  http://xn--no-check-certificate-2t2l/\n",
      "Resolving xn--no-check-certificate-2t2l (xn--no-check-certificate-2t2l)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address â€˜xn--no-check-certificate-2t2lâ€™\n",
      "--2024-08-29 14:25:11--  https://docs.google.com/uc?export=download&id=1wMhyqoJ0D7xQepW4y7Ireq2U2X61ndYz\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.161.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1wMhyqoJ0D7xQepW4y7Ireq2U2X61ndYz&export=download [following]\n",
      "--2024-08-29 14:25:11--  https://drive.usercontent.google.com/download?id=1wMhyqoJ0D7xQepW4y7Ireq2U2X61ndYz&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.207.97, 2404:6800:400a:813::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.207.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 858298 (838K) [application/octet-stream]\n",
      "Saving to: â€˜./dataset/summarization/test.csvâ€™\n",
      "\n",
      "./dataset/summariza 100%[===================>] 838.18K  3.37MB/s    in 0.2s    \n",
      "\n",
      "2024-08-29 14:25:14 (3.37 MB/s) - â€˜./dataset/summarization/test.csvâ€™ saved [858298/858298]\n",
      "\n",
      "FINISHED --2024-08-29 14:25:14--\n",
      "Total wall clock time: 2.9s\n",
      "Downloaded: 1 files, 838K in 0.2s (3.37 MB/s)\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score\n",
    "!wget â€“no-check-certificate 'https://docs.google.com/uc?export=download&id=14s5orP5j6nNOqdmGdy6DkdyhcIsn-6Zv' -O ./dataset/summarization//train.csv\n",
    "!wget â€“no-check-certificate 'https://docs.google.com/uc?export=download&id=1wMhyqoJ0D7xQepW4y7Ireq2U2X61ndYz' -O ./dataset/summarization/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:25:15.709127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-29 14:25:15.784892: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-29 14:25:16.170858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-08-29 14:25:16.170911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-08-29 14:25:16.170915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summ_dataset(Dataset):\n",
    "    \"\"\"dataframeì„ torch dataset classë¡œ ë³€í™˜\"\"\"\n",
    "    def __init__(self, document, tokenizer):\n",
    "      self.dataset = document\n",
    "      self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        input_ids=torch.LongTensor(self.dataset[\"input_ids\"][idx])\n",
    "        labels=torch.LongTensor(self.dataset[\"labels\"][idx])\n",
    "\n",
    "        attention_mask=input_ids.ne(self.tokenizer.pad_token_id) ## padding tokenì€ attention ê³„ì‚°ì— ë°˜ì˜ë˜ë©´ ì•ˆë˜ë‹ˆê¹Œ maskë¥¼ ì •ì˜í•œë‹¤..\n",
    "\n",
    "        return dict(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2.load_data , construct_tokenized_dataset í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    \"\"\"csv fileì„ dataframeìœ¼ë¡œ load\"\"\"\n",
    "    dataset = pd.read_csv(dataset_dir)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, doc_tokenizer, sum_tokenizer, doc_max_length, sum_max_len, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    í† í¬ë‚˜ì´ì§•ì„ ìœ„í•œ í•¨ìˆ˜. trainingê³¼ inference ë‹¨ê³„ì—ì„œì˜ í† í¬ë‚˜ì´ì§•ì´ ë³„ë„ë¡œ êµ¬ì¶•ë˜ì–´ ìˆë‹¤.\n",
    "    - í•™ìŠµì¼ ë•ŒëŠ” ë³¸ë¬¸ê³¼ ìš”ì•½ì´ í•¨ê»˜ ì…ë ¥ëœë‹¤. --> ë³¸ë¬¸ [SEP] ìš”ì•½\n",
    "    - ë°˜ë©´ ì¶”ë¡  ë‹¨ê³„ì—ì„œëŠ” ë³¸ë¬¸ë§Œ ì…ë ¥ë˜ì–´ ìš”ì•½ì„ ìƒì„±í•´ì•¼í•¨.\n",
    "    \"\"\"\n",
    "    ## ì¶”ë¡  ë‹¨ê³„\n",
    "    if mode == \"infer\":\n",
    "      ## inference ì‹œì—ëŠ” document ë§Œ ì£¼ì–´ì§€ê³ , ë§ˆì§€ë§‰ì— bos_tokenì„ ë¶™ì—¬ ìƒì„± ì‹œì‘í•˜ê²Œ í•œë‹¤.\n",
    "      document_text = dataset['document']\n",
    "      summ_text = dataset['summary']\n",
    "\n",
    "      ## document + bos\n",
    "      ## <pad> <pad> d_1 d_2 d_3 ... d_n <bos>\n",
    "      document = [doc_tokenizer(documents, padding = 'max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for documents in document_text.values]\n",
    "      \n",
    "      # labelsì—ëŠ” ìš”ì•½ë¬¸ë§Œí¼ì˜ ë¹ˆì¹¸ìœ¼ë¡œ ì±„ì›Œì¤€ í›„ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ë„ë¡ í•¨\n",
    "      labels = [[-100] * sum_max_len for _ in document]\n",
    "\n",
    "      out = {\"input_ids\": document, \"labels\": labels}\n",
    "      print(\"inferenceì„ ìœ„í•œ ë°ì´í„°ì—ì„œ tokenizing ëœ input í˜•íƒœ\")\n",
    "      print(document[-1])\n",
    "      print(doc_tokenizer.convert_ids_to_tokens(document[-1]))\n",
    "      print()\n",
    "\n",
    "    elif mode == \"train\":\n",
    "      document_text = dataset['document']\n",
    "      summary_text = dataset['summary']\n",
    "      ## document ì™€ summaryë¥¼ ì´ì–´ ë¶™ì—¬ì„œ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©. \n",
    "      ## document ë’¤ì—ëŠ” bos_token ì„ ë¶™ì—¬ ìƒì„± ì‹œì‘ì„ ëª…ì‹œí•˜ê³ , summary ë¥¼ ë¶™ì¸ í›„ ë§¨ ë’¤ì—ëŠ” eos_token ìœ¼ë¡œ ìƒì„±ì˜ ëì„ ëª…ì‹œ.\n",
    "      ## â­ï¸ documentë¥¼ padding í•  ë•ŒëŠ” sideë¥¼ leftë¡œ ì£¼ê³ , summaryë¥¼ padding í•  ë•ŒëŠ” sideë¥¼ rightë¡œ ì¤˜ì„œ ì—°ì†ëœ ë¬¸ì¥ì´ ìƒì„±ë  ìˆ˜ ìˆë„ë¡ í•œë‹¤.\n",
    "      ## â­ï¸ <pad> <pad> d_1 d_2 d_3 ... d_n <bos> s_1 s_2 ... s_m <eos> <pad> <pad>\n",
    "      document = [doc_tokenizer(documents, padding='max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for documents in document_text.values]\n",
    "      summary = [sum_tokenizer(summaries + sum_tokenizer.eos_token, padding = 'max_length',truncation=True, max_length=sum_max_len, add_special_tokens=True)['input_ids'] for summaries in summary_text.values]\n",
    "\n",
    "      ## êµ¬ì„±í•´ë‘” document ì™€ summaryë¥¼ ê²°í•©í•˜ì—¬ input ì¤€ë¹„\n",
    "      tokenized_senetences = [document + summary for (document, summary) in zip(document, summary)]\n",
    "      ## documentëŠ” ìƒì„±í•  ë‚´ìš©ì´ ì•„ë‹ˆë¯€ë¡œ -100ìœ¼ë¡œ labelì„ ë¶€ì—¬í•œë‹¤.\n",
    "      # Input : <pad> <pad> d_1  d_2  d_3  ... d_n  <bos> s_1 s_2 ... s_m <eos> <pad> <pad>\n",
    "      # Label : -100  -100    -100 -100 -100  ... -100  -100  s_1 s_2 ... s_m <eos> -100 -100\n",
    "\n",
    "      labels = [[-100] * len(document) + summary for (document, summary) in zip(document, summary)]\n",
    "      ## â­ï¸ Q. ë‹¤ìŒì— ì˜¬ Tokenì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•´ì•¼ ë˜ë‹ˆê¹Œ s_1ì˜ labelì€ í•œ ì¹¸ì”© ë°€ë¦° s_2ê°€ ë“¤ì–´ê°€ì•¼ ë˜ì§€ ì•Šë‚˜ìš”?\n",
    "      # A. Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ GPT êµ¬í˜„(https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1103-L1104)ì„ ë³´ë©´, \n",
    "      # ëª¨ë¸ì˜ Logitì„ [: -1]ë§Œ ê°€ì ¸ì˜¤ê³ , Labelì€ [1: ]ì„ ê°€ì ¸ì™€ì„œ Lossë¥¼ ê³„ì‚°í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "      # ì¦‰, Inputê³¼ Labelì´ í•œ ì¹¸ì”© ë°€ë¦°ì±„ë¡œ ì…ë ¥ì„ ë„£ì§€ ì•Šì•„ë„, ë‚´ë¶€ êµ¬í˜„ì— ì˜í•´ ìë™ìœ¼ë¡œ ë°€ë¦° ì±„ë¡œ ê³„ì‚°ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "      # padding ëœ ë¶€ë¶„ì´ í•™ìŠµë˜ì§€ ì•Šë„ë¡ -100 ìœ¼ë¡œ ì¹˜í™˜\n",
    "      labels = [[-100 if token == sum_tokenizer.pad_token_id else token for token in l] for l in labels]\n",
    "      out = {\"input_ids\": tokenized_senetences, \"labels\": labels}\n",
    "\n",
    "      print(\"í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì—ì„œ tokenizing ëœ input í˜•íƒœ\")\n",
    "      print(tokenized_senetences[-1])\n",
    "      print(doc_tokenizer.convert_ids_to_tokens(tokenized_senetences[-1]))\n",
    "      print()\n",
    "\n",
    "      print(\"í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì—ì„œ labelì˜ í˜•íƒœ\")\n",
    "      print(labels[-1])\n",
    "      print()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3. prepare_dataset í•¨ìˆ˜ ì •ì˜\n",
    "- ì•ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜\n",
    "\n",
    "** ì „ì²˜ë¦¬ê³¼ì • ** \n",
    "1. train.csv / test.csv íŒŒì¼ì„ pd.dataframe ë¡œ ë‹¤ìš´ë¡œë“œ í•´ì¤€ë‹¤. <br>\n",
    "2. train/validation setì„ ë‚˜ëˆ ì¤€ë‹¤. (7.5:2.5) <br>\n",
    "3. ì „ì²´ ë¬¸ì„œì™€ ìš”ì•½ ë°ì´í„°ë¥¼ ëª¨ë‘ tokenizing í•´ì¤€ë‹¤.  <br>\n",
    "4. ìš”ì•½(label) ë°ì´í„°ëŠ” Padding ëœ ë¶€ë¶„ì€ loss ê°€ íë¥´ì§€ ì•Šë„ë¡ -100 ìœ¼ë¡œ ì¹˜í™˜í•´ì¤€ë‹¤. <br>\n",
    "5. tokenizing ëœ ë°ì´í„°ë¥¼ summ_dataset classë¡œ ë°˜í™˜í•´ì¤€ë‹¤. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_dir, doc_tokenizer,sum_tokenizer,doc_max_len, sum_max_len):\n",
    "    \"\"\"í•™ìŠµ(train)ê³¼ í‰ê°€(test)ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ì„ ì¤€ë¹„\"\"\"\n",
    "    # load_data\n",
    "    train_dataset = load_data(os.path.join(dataset_dir, \"./dataset/summarization/train.csv\"))\n",
    "    test_dataset = load_data(os.path.join(dataset_dir, \"./dataset/summarization/test.csv\"))\n",
    "\n",
    "    # split train / validation = 7.5 : 2.5\n",
    "    train_dataset, val_dataset = train_test_split(train_dataset,test_size=0.2,random_state=42)\n",
    "\n",
    "    ### tokenizer ì— ë“¤ì–´ê°€ê¸° ì „ ë°ì´í„° í˜•íƒœ\n",
    "    print(\"tokenizer ì— ë“¤ì–´ê°€ëŠ” ë°ì´í„° í˜•íƒœ\")\n",
    "    print(train_dataset.iloc[0])\n",
    "\n",
    "    # tokenizing\n",
    "    print(\"train tokenizing...\")\n",
    "    tokenized_train = tokenized_dataset(train_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len)\n",
    "    \n",
    "    print(\"valid tokenizing...\")\n",
    "    tokenized_val = tokenized_dataset(val_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len)\n",
    "    \n",
    "    print(\"test tokenizing...\")\n",
    "    tokenized_test = tokenized_dataset(test_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len, mode=\"infer\")\n",
    "\n",
    "    # make dataset for pytorch.\n",
    "    summ_train_dataset = summ_dataset(tokenized_train, doc_tokenizer)\n",
    "    summ_val_dataset = summ_dataset(tokenized_val, doc_tokenizer)\n",
    "    summ_test_dataset = summ_dataset(tokenized_test, doc_tokenizer)\n",
    "\n",
    "    print(\"--- dataset class Done ---\")\n",
    "\n",
    "    return summ_train_dataset , summ_val_dataset, summ_test_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ï¸âƒ£ Model & Trainer\n",
    "- huggingface ì—ì„œ ì‚¬ì „í•™ìŠµëœ(pre-trained) ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "- huggingface ì˜ Trainer ëª¨ë“ˆì„ ì •ì˜í•˜ê³  í•™ìŠµì— ì‚¬ìš©ë  Arguments ë“¤ì„ ì§€ì •í•´ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. compute_metrics í•¨ìˆ˜ ì •ì˜\n",
    "- í•™ìŠµ ì¤‘ validation í•  ë•Œ ì‚¬ìš©ë  í‰ê°€ì§€í‘œ ì •ì˜í•˜ëŠ” í•¨ìˆ˜\n",
    "- í•´ë‹¹ ì‹¤ìŠµì—ì„œëŠ” Rouge Scoreë¥¼ Metricìœ¼ë¡œ ì‚¬ìš©\n",
    "\n",
    "Q. Rouge Score?\n",
    "\n",
    "ë¬¸ì„œ ìš”ì•½, ê¸°ê³„ ë²ˆì—­ ë“±ì˜ task ì—ì„œ ëª¨ë¸ì´ ìƒì„±í•œ ë¬¸ì¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì§€í‘œ.\n",
    "\n",
    "ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ score ê°€ ìˆì§€ë§Œ, ì´ ì¤‘ ROUGE-Nì˜ F1-score(recall ê³¼ precisiondì„ ë°˜ì˜)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ROUGE-N : N gramì— ê¸°ë°˜í•˜ì—¬, ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸ê³¼ ì •ë‹µ ìš”ì•½ë¬¸ ì¤‘ ê²¹ì¹˜ëŠ” ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•˜ì—¬ ìƒì„±í•œ ìš”ì•½ë¬¸ì„ í‰ê°€í•˜ëŠ” í‰ê°€ì§€í‘œ\n",
    "- ROUGE-N recall : ë‘ ìš”ì•½ë¬¸ ì¤‘ ê²¹ì¹˜ëŠ” N-gramì˜ ìˆ˜ / ì •ë‹µ ìš”ì•½ë¬¸ì˜ N-gramì˜ ìˆ˜\n",
    "- ROUGE-N precision : ë‘ ìš”ì•½ë¬¸ ì¤‘ ê²¹ì¹˜ëŠ” N-gramì˜ ìˆ˜ / ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸ì˜ N-gramì˜ ìˆ˜  \n",
    "\n",
    "ì˜ˆ) ROUGE-1\n",
    "- ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸(R) : \"the hello a cat dog fox jumps\"\n",
    "- ì •ë‹µ ìš”ì•½ë¬¸ (T) : \"the fox jumps\" \n",
    "- ë‘ ìš”ì•½ë¬¸ ì¤‘ ê²¹ì¹˜ëŠ” 1-gram ['the', 'fox', 'jumps'] ì´ë¯€ë¡œ ROUGE-1 recall ì€ 3/3, ROUGE-1 precision ì€ 3/7 = 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(args, pred):\n",
    "    # tokenizer load\n",
    "    MODEL_NAME = args.model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    if isinstance(preds, tuple):\n",
    "      preds = preds[0]\n",
    "\n",
    "    # predsì—ì„œ document ì´í›„ë¶€í„° ìƒì„±ëœ summaryë¥¼ decode\n",
    "    ## GPT-2ëŠ” encoder-decoder êµ¬ì¡°ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì…ë ¥ëœ documentì— ì´ì–´ì„œ summaryë¥¼ ìƒì„±í•˜ë¯€ë¡œ\n",
    "    ## document ë¶€ë¶„ì„ ë³„ë„ë¡œ ì˜ë¼ë‚´ì•¼ í•œë‹¤.\n",
    "    decoded_preds = tokenizer.batch_decode(preds[:, args.doc_max_len:], skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels[:, args.doc_max_len:], skip_special_tokens=True)\n",
    "\n",
    "    # rouge score ê³„ì‚°\n",
    "    metric = datasets.load_metric(\"rouge\")\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # ROUGE ê²°ê³¼ë¥¼ ì¶”ì¶œ\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return {\n",
    "        'Rouge-2' : result['rouge2']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2.load_tokenizer_and_model_for_train í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model_for_train(args):\n",
    "    \"\"\"í•™ìŠµ(train)ì„ ìœ„í•œ ì‚¬ì „í•™ìŠµ(pretrained) í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ huggingfaceì—ì„œ load\"\"\"\n",
    "    # modelê³¼ tokenizerë¥¼ load\n",
    "    MODEL_NAME = args.model_name\n",
    "    doc_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    sum_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n",
    "\n",
    "    # modelì˜ hyperparameterë¥¼ setting\n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    print(model_config)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, config=model_config)\n",
    "    print(\"--- Modeling Done ---\")\n",
    "    \n",
    "    return doc_tokenizer, sum_tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3.load_trainer_for_train í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer_for_train(args, model, summ_train_dataset, summ_val_dataset):\n",
    "    \"\"\"í•™ìŠµ(train)ì„ ìœ„í•œ huggingface trainer ì„¤ì •\"\"\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.save_path + \"results\",  # output directory\n",
    "        save_total_limit=args.save_limit,  # number of total save model.\n",
    "        save_steps=args.save_step,  # model saving step.\n",
    "        num_train_epochs=args.epochs,  # total number of training epochs\n",
    "        learning_rate=args.lr,  # learning_rate\n",
    "        per_device_train_batch_size=args.batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "        warmup_steps=args.warmup_steps,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=args.weight_decay,  # strength of weight decay\n",
    "        logging_dir=args.save_path + \"logs\",  # directory for storing logs\n",
    "        logging_steps=args.logging_steps,  # log saving step.\n",
    "        evaluation_strategy=\"steps\",  # evaluation strategy to adopt during training\n",
    "            # `no`: No evaluation during training.\n",
    "            # `steps`: Evaluate every `eval_steps`.\n",
    "            # `epoch`: Evaluate every end of epoch.\n",
    "        eval_steps=args.eval_steps,  # evaluation step.\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    ## Add callback & optimizer & scheduler\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5, early_stopping_threshold=0.001\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=args.weight_decay,\n",
    "        amsgrad=False,\n",
    "    )\n",
    "\n",
    "    print(\"--- Set training arguments Done ---\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated ğŸ¤— Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=summ_train_dataset,  # training dataset\n",
    "        eval_dataset=summ_val_dataset,  # evaluation dataset\n",
    "        compute_metrics=lambda p: compute_metrics(args, p),\n",
    "        callbacks=[MyCallback],\n",
    "        optimizers=(\n",
    "            optimizer,\n",
    "            get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=args.warmup_steps,\n",
    "                    num_training_steps=len(summ_train_dataset) * args.epochs,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    print(\"--- Set Trainer Done ---\")\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-4.train í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "** í•™ìŠµë™ì‘ê³¼ì • ** \n",
    "1. ì‹¤í—˜ì— ì˜í–¥ì„ ì£¼ëŠ” ëª¨ë“  seedë¥¼ ê³ ì •í•´ì¤€ë‹¤. <br>\n",
    "2. ì‚¬ìš©í•  gpuë¥¼ deviceì— í• ë‹¹í•´ì¤€ë‹¤. <br>\n",
    "3. tokenizerì™€ modelì„ ë¶ˆëŸ¬ì˜¨í›„, modelì„ deviceì— í• ë‹¹í•´ì¤€ë‹¤. <br>\n",
    "4. í•™ìŠµì— ì‚¬ìš©ë  summ_dataset ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.<br>\n",
    "5. í•™ìŠµì— ì‚¬ìš©ë  Trainer ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.<br>\n",
    "6. í•™ìŠµì„ ì§„í–‰í•œí›„ì— best_modelì„ ì €ì¥í•´ì¤€ë‹¤. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \"\"\"ëª¨ë¸ì„ í•™ìŠµ(train)í•˜ê³  best modelì„ ì €ì¥\"\"\"\n",
    "    # fix a seed\n",
    "    pl.seed_everything(seed=42, workers=False)\n",
    "\n",
    "    # set device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # set model and tokenizer\n",
    "    doc_tokenizer, sum_tokenizer , model = load_tokenizer_and_model_for_train(args)\n",
    "    model.to(device)\n",
    "\n",
    "    # set data\n",
    "    summ_train_dataset, summ_val_dataset, summ_test_dataset, test_dataset = prepare_dataset(args.dataset_dir,doc_tokenizer, sum_tokenizer,args.doc_max_len,args.sum_max_len)\n",
    "    # set trainer\n",
    "    trainer = load_trainer_for_train(args, model, summ_train_dataset, summ_val_dataset)\n",
    "\n",
    "    # train model\n",
    "    print(\"--- Start train ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Finish train ---\")\n",
    "    model.save_pretrained(\"./best_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-5.arguments ì§€ì • ë° í•™ìŠµ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"MrBananaHuman/kogpt2_small\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/upstage/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Modeling Done ---\n",
      "tokenizer ì— ë“¤ì–´ê°€ëŠ” ë°ì´í„° í˜•íƒœ\n",
      "document    [1] ç”²ì´ í† ì§€ì†Œìœ ì ä¹™ì—ê²Œì„œ í† ì§€ë¥¼ ì„ì°¨í•œ í›„ ì£¼ìœ ì†Œ ì˜ì—…ì„ ìœ„í•˜ì—¬ ì§€í•˜ì— ìœ ë¥˜...\n",
      "summary     í† ì§€ì— ë§¤ì„¤ëœ ìœ ë¥˜ì €ì¥ì¡°ëŠ” í† ì§€ì™€ ì¼ì²´ë¥¼ ì´ë£¨ëŠ” êµ¬ì„± ë¶€ë¶„ì´ ì•„ë‹ˆë¯€ë¡œ í† ì§€ ì„ì°¨ì¸ì´...\n",
      "Name: 249, dtype: object\n",
      "train tokenizing...\n",
      "í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì—ì„œ tokenizing ëœ input í˜•íƒœ\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12712, 600, 979, 723, 1042, 545, 3772, 1314, 13479, 12712, 1450, 1171, 7832, 979, 135, 3500, 723, 1254, 23745, 11328, 12712, 600, 979, 7303, 1172, 4658, 1036, 476, 752, 2653, 12712, 600, 979, 1667, 16747, 7832, 694, 5242, 27950, 10509, 7491, 5208, 10703, 2867, 1377, 386, 8459, 7309, 8065, 1104, 319, 5295, 14660, 12712, 600, 979, 1042, 545, 16798, 11242, 1487, 10093, 1042, 545, 2336, 1681, 1249, 5490, 1377, 386, 8459, 6650, 549, 1673, 2792, 11242, 1487, 10093, 1042, 545, 2823, 18961, 12712, 600, 29958, 5626, 13821, 7540, 1264, 11242, 1487, 14660, 1042, 545, 727, 1325, 9295, 20622, 1042, 545, 7636, 1957, 10093, 5901, 12763, 901, 25168, 9087, 1219, 1027, 1006, 23171, 6694, 14421, 21415, 0, 12712, 600, 979, 723, 1042, 545, 3772, 1377, 386, 8459, 7309, 8065, 1104, 14660, 12712, 600, 979, 1042, 545, 16798, 47506, 10093, 1042, 545, 2336, 1681, 1249, 6694, 11242, 1487, 14660, 1042, 545, 727, 1325, 9295, 20622, 1042, 545, 7636, 1957, 10093, 5901, 12763, 1006, 23171, 12925, 11242, 1487, 10093, 1042, 545, 2823, 18961, 12712, 600, 29958, 5626, 1580, 1424, 2, 1, 1, 1]\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'â–ì†Œë©¸', 'ì‹œ', 'íš¨', 'ì˜', 'â–ê¸°', 'ì‚°', 'ì¼ì€', 'â–ì±„', 'ë¬´ì˜', 'â–ì†Œë©¸', 'ì´ë¼ê³ ', 'â–í•˜ëŠ”', 'â–ë²•ë¥ ', 'íš¨', 'ê³¼', 'â–ë°œìƒ', 'ì˜', 'â–ìš”', 'ê±´ì—', 'â–í•´ë‹¹í•˜ëŠ”', 'â–ì†Œë©¸', 'ì‹œ', 'íš¨', 'â–ê¸°ê°„', 'â–ê³„', 'ì‚°ì˜', 'â–ì‹œ', 'ë°œ', 'ì ', 'ìœ¼ë¡œì„œ', 'â–ì†Œë©¸', 'ì‹œ', 'íš¨', 'â–í•­', 'ë³€ì˜', 'â–ë²•ë¥ ', 'ìš”', 'ê±´ì„', 'â–êµ¬ì„±í•˜ëŠ”', 'â–êµ¬ì²´ì ì¸', 'â–ì‚¬ì‹¤ì—', 'â–í•´ë‹¹', 'í•˜ë¯€ë¡œ', 'â–ì´ëŠ”', 'â–ë³€', 'ë¡ ', 'ì£¼ì˜ì˜', 'â–ì ìš©', 'â–ëŒ€ìƒì´', 'ê³ ,', 'ë”°', 'ë¼ì„œ', 'â–ë³¸ë˜ì˜', 'â–ì†Œë©¸', 'ì‹œ', 'íš¨', 'â–ê¸°', 'ì‚°', 'ì¼ê³¼', 'â–ë‹¹ì‚¬', 'ìê°€', 'â–ì£¼ì¥í•˜ëŠ”', 'â–ê¸°', 'ì‚°', 'ì¼ì´', 'â–ì„œë¡œ', 'â–ë‹¤ë¥¸', 'â–ê²½ìš°ì—ëŠ”', 'â–ë³€', 'ë¡ ', 'ì£¼ì˜ì˜', 'â–ì›ì¹™', 'ìƒ', 'â–ë²•', 'ì›ì€', 'â–ë‹¹ì‚¬', 'ìê°€', 'â–ì£¼ì¥í•˜ëŠ”', 'â–ê¸°', 'ì‚°', 'ì¼ì„', 'â–ê¸°ì¤€ìœ¼ë¡œ', 'â–ì†Œë©¸', 'ì‹œ', 'íš¨ë¥¼', 'â–ê³„ì‚°', 'í•˜ì—¬ì•¼', 'â–í•˜ëŠ”ë°,', 'ì´ëŠ”', 'â–ë‹¹ì‚¬', 'ìê°€', 'â–ë³¸ë˜ì˜', 'â–ê¸°', 'ì‚°', 'ì¼', 'ë³´ë‹¤', 'â–ë’¤ì˜', 'â–ë‚ ì§œë¥¼', 'â–ê¸°', 'ì‚°', 'ì¼ë¡œ', 'â–í•˜ì—¬', 'â–ì£¼ì¥í•˜ëŠ”', 'â–ê²½ìš°ëŠ”', 'â–ë¬¼ë¡ ì´ê³ ', 'íŠ¹', 'ë³„í•œ', 'â–ì‚¬ì •ì´', 'â–ì—†ëŠ”', 'â–í•œ', 'â–ê·¸', 'â–ë°˜ëŒ€ì˜', 'â–ê²½ìš°ì—', 'â–ìˆì–´ì„œë„', 'â–ë§ˆì°¬ê°€ì§€ì´ë‹¤.', '<s>', 'â–ì†Œë©¸', 'ì‹œ', 'íš¨', 'ì˜', 'â–ê¸°', 'ì‚°', 'ì¼ì€', 'â–ë³€', 'ë¡ ', 'ì£¼ì˜ì˜', 'â–ì ìš©', 'â–ëŒ€ìƒì´', 'ê³ ,', 'â–ë³¸ë˜ì˜', 'â–ì†Œë©¸', 'ì‹œ', 'íš¨', 'â–ê¸°', 'ì‚°', 'ì¼ê³¼', 'â–ë‹¹ì‚¬ì', 'â–ì£¼ì¥í•˜ëŠ”', 'â–ê¸°', 'ì‚°', 'ì¼ì´', 'â–ì„œë¡œ', 'â–ë‹¤ë¥¸', 'â–ê²½ìš°ì—', 'â–ë‹¹ì‚¬', 'ìê°€', 'â–ë³¸ë˜ì˜', 'â–ê¸°', 'ì‚°', 'ì¼', 'ë³´ë‹¤', 'â–ë’¤ì˜', 'â–ë‚ ì§œë¥¼', 'â–ê¸°', 'ì‚°', 'ì¼ë¡œ', 'â–í•˜ì—¬', 'â–ì£¼ì¥í•˜ëŠ”', 'â–ê²½ìš°ëŠ”', 'â–ë¬¼ë¡ ì´ê³ ', 'â–ê·¸', 'â–ë°˜ëŒ€ì˜', 'â–ê²½ìš°ì—ë„', 'â–ë‹¹ì‚¬', 'ìê°€', 'â–ì£¼ì¥í•˜ëŠ”', 'â–ê¸°', 'ì‚°', 'ì¼ì„', 'â–ê¸°ì¤€ìœ¼ë¡œ', 'â–ì†Œë©¸', 'ì‹œ', 'íš¨ë¥¼', 'â–ê³„ì‚°', 'í•´ì•¼', 'â–í•œë‹¤.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì—ì„œ labelì˜ í˜•íƒœ\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 12712, 600, 979, 723, 1042, 545, 3772, 1377, 386, 8459, 7309, 8065, 1104, 14660, 12712, 600, 979, 1042, 545, 16798, 47506, 10093, 1042, 545, 2336, 1681, 1249, 6694, 11242, 1487, 14660, 1042, 545, 727, 1325, 9295, 20622, 1042, 545, 7636, 1957, 10093, 5901, 12763, 1006, 23171, 12925, 11242, 1487, 10093, 1042, 545, 2823, 18961, 12712, 600, 29958, 5626, 1580, 1424, 2, -100, -100, -100]\n",
      "\n",
      "valid tokenizing...\n",
      "í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì—ì„œ tokenizing ëœ input í˜•íƒœ\n",
      "[1018, 15, 1029, 5269, 21716, 14045, 2456, 14525, 1220, 162, 37267, 4255, 6204, 25400, 1565, 2330, 1315, 10092, 1016, 1503, 25170, 7513, 492, 1056, 2932, 1231, 492, 941, 1565, 286, 499, 123, 952, 32525, 1096, 5169, 1339, 1249, 1565, 3397, 25689, 1045, 8086, 155, 1614, 1208, 10092, 723, 6239, 10837, 1006, 8435, 13691, 696, 286, 499, 123, 2539, 15419, 2927, 784, 572, 712, 5023, 663, 137, 941, 492, 397, 723, 7309, 272, 2153, 1627, 2927, 2329, 29840, 3763, 15991, 513, 452, 723, 3423, 687, 8990, 13, 766, 761, 17608, 1290, 39055, 766, 776, 745, 4658, 8990, 18781, 2422, 445, 1283, 5158, 557, 3848, 15002, 3685, 6445, 15591, 40912, 5778, 1080, 492, 726, 10185, 726, 1356, 776, 4198, 143, 2927, 784, 572, 712, 5023, 663, 137, 941, 492, 397, 600, 950, 382, 41776, 10805, 13336, 7198, 15, 3027, 382, 47988, 23075, 19, 14517, 29396, 5258, 6567, 1010, 11, 42516, 760, 5250, 44972, 9368, 1055, 13983, 3, 34111, 7203, 1120, 725, 543, 2989, 24880, 1055, 1673, 24534, 1136, 1019, 1024, 12029, 885, 784, 572, 712, 5023, 663, 137, 941, 492, 397, 10834, 2351, 1356, 8150, 572, 712, 1187, 39316, 1198, 452, 582, 967, 4350, 2416, 6962, 14045, 0, 1220, 162, 37267, 4255, 6204, 1087, 1565, 2330, 1315, 10092, 1016, 1503, 25170, 7513, 492, 1056, 2932, 1231, 492, 941, 1565, 286, 499, 123, 952, 32525, 1096, 5169, 1339, 1249, 1565, 3397, 25689, 1045, 8086, 1006, 1614, 1208, 10092, 723, 6239, 10837, 2927, 784, 572, 712, 5023, 663, 137, 941, 7832, 723, 7309, 272, 4033, 1627, 2927, 2329, 5208, 2060, 5158, 557, 3848, 15002]\n",
      "['â–ê°€', '.', 'â–ì§€', 'ëª©ì´', 'â–ëŒ€ì¸', 'â–í† ì§€', 'ìƒì˜', 'â–ê±´ë¬¼ì´', 'â–ë“±', 'ê¸°', 'ë¶€ë‚˜', 'â–ê´€ë¦¬', 'ëŒ€ì¥', 'ìƒìœ¼ë¡œëŠ”', 'â–ìš©', 'ë„ê°€', 'â–ëª¨ë‘', 'â–ì£¼íƒ', 'ìœ¼ë¡œ', 'â–ë˜ì–´', 'â–ìˆìŒì—ë„', 'â–ê±´ì¶•', 'ë²•', 'â–ì†Œ', 'ì •ì˜', 'â–ì ', 'ë²•', 'í•œ', 'â–ìš©', 'ë„', 'ë³€', 'ê²½', 'í—ˆ', 'ê°€ë‚˜', 'â–ì‹ ', 'ê³ ë„', 'â–ì—†ì´', 'â–ë‹¤ë¥¸', 'â–ìš©', 'ë„ì—', 'â–ì‚¬ìš©ë˜ê³ ', 'â–ìˆëŠ”', 'â–ê²½ìš°,', 'ê·¸', 'â–í† ', 'ì§€ëŠ”', 'â–ì£¼íƒ', 'ì˜', 'â–ë¶€ì§€', 'ì´ë¯€ë¡œ', 'â–ê·¸', 'â–í›„ì˜', 'â–ë¶ˆë²•', 'ìš©', 'ë„', 'ë³€', 'ê²½', 'ê³¼ëŠ”', 'â–ìƒê´€ì—†ì´', 'â–íƒ', 'ì§€', 'ì†Œ', 'ìœ ', 'ìƒí•œ', 'ì—', 'ê´€', 'í•œ', 'ë²•', 'ë¥ ', 'ì˜', 'â–ì ìš©', 'ëŒ€', 'ìƒì´', 'â–ë˜ëŠ”', 'â–íƒ', 'ì§€ì—', 'â–í•´ë‹¹í•œë‹¤.', 'ë‚˜.', 'â–ì„ ì¡°', 'ë¶„', 'ë¬˜', 'ì˜', 'â–ìœ ì§€', 'ì™€', 'â–ë³´ì¡´', ',', 'ì¢…', 'ì¡±', 'â–ê°„ì˜', 'â–í™”', 'ëª©,', 'ì¢…', 'ì¤‘', 'ì¬', 'ì‚°ì˜', 'â–ë³´ì¡´', 'ê´€ë¦¬', 'â–ë“±ì„', 'ëª©', 'ì ìœ¼ë¡œ', 'â–ê³µë™', 'ì„ ', 'ì¡°ì˜', 'â–í›„ì†', 'ë“¤ë¡œ', 'â–ì´ë£¨ì–´ì§„', 'â–ì¢…ì¡±', 'ë‹¨ì²´', 'ë¡œì„œì˜', 'â–ë¹„', 'ë²•', 'ì¸', 'â–ì‚¬ë‹¨', 'ì¸', 'â–ì¢…', 'ì¤‘', 'ì€,', 'êµ¬', 'â–íƒ', 'ì§€', 'ì†Œ', 'ìœ ', 'ìƒí•œ', 'ì—', 'ê´€', 'í•œ', 'ë²•', 'ë¥ ', 'ì‹œ', 'í–‰', 'ë ¹', '(199', '3.', '5.', '10', '.', 'â–ëŒ€í†µë ¹', 'ë ¹', 'â–ì œ13', '88', '2', 'í˜¸ë¡œ', 'â–ê°œì •', 'ë˜ê¸°', 'â–ì „ì˜', 'â–ê²ƒ', ')', 'â–ì œ12', 'ì¡°', 'â–ì œ2', 'í˜¸ì—ì„œ', 'â–ê·œì •', 'í•˜ëŠ”', 'â–ì œì‚¬', '<unk>', 'ì¢…êµ', 'â–ê¸°íƒ€', 'â–ê³µ', 'ìµ', 'ì‚¬', 'ì—…ì„', 'â–ì˜ìœ„', 'í•˜ëŠ”', 'â–ë²•', 'ì¸ì´ë¼ê³ ', 'â–í• ', 'â–ìˆ˜', 'â–ì—†', 'ìœ¼ë¯€ë¡œ,', 'íƒ', 'ì§€', 'ì†Œ', 'ìœ ', 'ìƒí•œ', 'ì—', 'ê´€', 'í•œ', 'ë²•', 'ë¥ ', 'â–ì‹œí–‰', 'â–ë‹¹ì‹œ', 'â–ì¢…', 'ì¤‘ì´', 'ì†Œ', 'ìœ ', 'í•˜ì—¬', 'â–ì„ ì¡°ì˜', 'â–ë¶„', 'ë¬˜', 'ìˆ˜', 'í˜¸', 'â–ë“±ì—', 'â–ì§ì ‘', 'â–ì‚¬ìš©í•˜ëŠ”', 'â–í† ì§€', '<s>', 'â–ë“±', 'ê¸°', 'ë¶€ë‚˜', 'â–ê´€ë¦¬', 'ëŒ€ì¥', 'â–ìƒ', 'â–ìš©', 'ë„ê°€', 'â–ëª¨ë‘', 'â–ì£¼íƒ', 'ìœ¼ë¡œ', 'â–ë˜ì–´', 'â–ìˆìŒì—ë„', 'â–ê±´ì¶•', 'ë²•', 'â–ì†Œ', 'ì •ì˜', 'â–ì ', 'ë²•', 'í•œ', 'â–ìš©', 'ë„', 'ë³€', 'ê²½', 'í—ˆ', 'ê°€ë‚˜', 'â–ì‹ ', 'ê³ ë„', 'â–ì—†ì´', 'â–ë‹¤ë¥¸', 'â–ìš©', 'ë„ì—', 'â–ì‚¬ìš©ë˜ê³ ', 'â–ìˆëŠ”', 'â–ê²½ìš°,', 'â–ê·¸', 'â–í† ', 'ì§€ëŠ”', 'â–ì£¼íƒ', 'ì˜', 'â–ë¶€ì§€', 'ì´ë¯€ë¡œ', 'â–íƒ', 'ì§€', 'ì†Œ', 'ìœ ', 'ìƒí•œ', 'ì—', 'ê´€', 'í•œ', 'â–ë²•ë¥ ', 'ì˜', 'â–ì ìš©', 'ëŒ€', 'ì‚°ì´', 'â–ë˜ëŠ”', 'â–íƒ', 'ì§€ì—', 'â–í•´ë‹¹', 'í•˜ê³ ,', 'â–ê³µë™', 'ì„ ', 'ì¡°ì˜', 'â–í›„ì†']\n",
      "\n",
      "í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì—ì„œ labelì˜ í˜•íƒœ\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1220, 162, 37267, 4255, 6204, 1087, 1565, 2330, 1315, 10092, 1016, 1503, 25170, 7513, 492, 1056, 2932, 1231, 492, 941, 1565, 286, 499, 123, 952, 32525, 1096, 5169, 1339, 1249, 1565, 3397, 25689, 1045, 8086, 1006, 1614, 1208, 10092, 723, 6239, 10837, 2927, 784, 572, 712, 5023, 663, 137, 941, 7832, 723, 7309, 272, 4033, 1627, 2927, 2329, 5208, 2060, 5158, 557, 3848, 15002]\n",
      "\n",
      "test tokenizing...\n",
      "inferenceì„ ìœ„í•œ ë°ì´í„°ì—ì„œ tokenizing ëœ input í˜•íƒœ\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8806, 45399, 10, 135, 564, 820, 513, 3687, 14856, 820, 4220, 1075, 7618, 11040, 1041, 442, 484, 1187, 6768, 453, 979, 724, 5416, 155, 14856, 820, 4649, 4242, 14202, 2564, 8806, 572, 578, 1016, 1026, 3, 1019, 1045, 18029, 153, 1692, 1045, 5901, 1577, 29460, 950, 754, 950, 2831, 14856, 820, 4220, 1691, 3603, 8564, 1500, 39814, 28332, 8806, 950, 1832, 16271, 3733, 1019, 5095, 2219, 162, 3325, 250, 1238, 8806, 950, 7928, 8708, 215, 8626, 8806, 950, 1832, 1191, 37613, 2271, 1581, 0]\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'â–í–‰ì •', 'í–‰ìœ„', '(', 'ê³¼', 'ì„¸', 'ì²˜', 'ë¶„', ')ì˜', 'â–ì·¨ì†Œ', 'ì²˜', 'ë¶„ì˜', 'â–ìœ„', 'ë²•ì´', 'â–ì¤‘ëŒ€', 'í•˜ê³ ', 'ëª…', 'ë°±', 'í•˜ì—¬', 'â–ë‹¹ì—°', 'ë¬´', 'íš¨', 'ì´', 'ê±°ë‚˜,', 'ê·¸', 'â–ì·¨ì†Œ', 'ì²˜', 'ë¶„ì—', 'â–ëŒ€í•˜ì—¬', 'â–ì†Œì›', 'â–ë˜ëŠ”', 'â–í–‰ì •', 'ì†Œ', 'ì†¡', 'ìœ¼ë¡œ', 'â–ë‹¤', '<unk>', 'â–ìˆ˜', 'â–ìˆëŠ”', 'â–ëª…ë¬¸', 'ê·œ', 'ì •ì´', 'â–ìˆëŠ”', 'â–ê²½ìš°ëŠ”', 'â–ë³„', 'ë¡ ,', 'í–‰', 'ì •', 'í–‰', 'ìœ„ì˜', 'â–ì·¨ì†Œ', 'ì²˜', 'ë¶„ì˜', 'â–ì·¨', 'ì†Œì—', 'â–ì˜í•˜ì—¬', 'â–ì´ë¯¸', 'â–íš¨ë ¥ì„', 'â–ìƒì‹¤í•œ', 'â–í–‰ì •', 'í–‰', 'ìœ„ë¥¼', 'â–ì†Œìƒ', 'ì‹œí‚¬', 'â–ìˆ˜', 'â–ì—†ê³ ,', 'ê·¸ëŸ¬', 'ê¸°', 'â–ìœ„í•˜ì—¬', 'ëŠ”', 'â–ì›', 'â–í–‰ì •', 'í–‰', 'ìœ„ì™€', 'â–ë™ì¼', 'ë‚´', 'ìš©ì˜', 'â–í–‰ì •', 'í–‰', 'ìœ„ë¥¼', 'â–ë‹¤ì‹œ', 'â–í–‰í• ', 'â–ìˆ˜ë°–ì—', 'â–ì—†ë‹¤.', '<s>']\n",
      "\n",
      "--- dataset class Done ---\n",
      "--- Set training arguments Done ---\n",
      "--- Set Trainer Done ---\n",
      "--- Start train ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/.local/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/2000 01:45 < 00:45, 13.24 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.525000</td>\n",
       "      <td>2.182662</td>\n",
       "      <td>13.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.458100</td>\n",
       "      <td>2.210392</td>\n",
       "      <td>13.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.844200</td>\n",
       "      <td>2.292986</td>\n",
       "      <td>14.362700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>2.376865</td>\n",
       "      <td>15.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>2.460108</td>\n",
       "      <td>12.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>2.499961</td>\n",
       "      <td>13.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>2.561069</td>\n",
       "      <td>15.711100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_235648/2187569660.py:20: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"rouge\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd22d9d06764fa99d98af989494815d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finish train ---\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "class args():\n",
    "    \"\"\"í•™ìŠµ(train)ê³¼ ì¶”ë¡ (infer)ì— ì‚¬ìš©ë˜ëŠ” arguments ê´€ë¦¬í•˜ëŠ” class\"\"\"\n",
    "    dataset_dir = \"./\"\n",
    "    model_type = \"gpt2\"\n",
    "    model_name = 'MrBananaHuman/kogpt2_small'\n",
    "    save_path = \"./\"\n",
    "    save_step = 400\n",
    "    logging_steps = 200\n",
    "    eval_steps = 200\n",
    "    save_limit = 5\n",
    "    seed = 42\n",
    "    epochs = 20 # 10\n",
    "    batch_size = 4  # ë©”ëª¨ë¦¬ ìƒí™©ì— ë§ê²Œ ì¡°ì ˆ e.g) 16 or 32\n",
    "    doc_max_len = 196\n",
    "    sum_max_len = 64\n",
    "    lr = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 5\n",
    "    scheduler = \"linear\"\n",
    "    model_dir = \"./best_model\" #ì¶”ë¡  ì‹œ, ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ë¡œ ì„¤ì •\n",
    "    \n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
