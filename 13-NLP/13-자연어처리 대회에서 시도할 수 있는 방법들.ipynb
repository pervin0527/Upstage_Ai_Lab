{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대회 전략 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✨실습 개요<br>\n",
    "\n",
    "1) 실습 목적 <br>\n",
    "  이번 실습에서는 대회에서 사용되는 전략들을 구현해봅니다. <br>\n",
    "  자연어 데이터를 증강하는 3가지 방법과, 후처리 기법인 앙상블 그리고 모델을 MLM으로 추가학습하는 코드를 구현합니다.   <br>\n",
    "\n",
    "\n",
    " 2) 수강 목표\n",
    "  - 자연어 데이터를 증강할 수 있다.\n",
    "  - 학습 시 k-fold로 데이터를 나눌 수 있다.\n",
    "  - Ensemble을 통해서 점수를 올려볼 수 있다.\n",
    "  - MLM Task를 구현하고 DAPT & TAPT를 실행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습 목차\n",
    "1. 데이터 증강기법 살펴보기\n",
    "    * 1-1. EDA 사용해보기\n",
    "    * 1-2. AEDA 사용해보기\n",
    "    * 1-3. Back Translation(feat. Google Translate)\n",
    "2. K-fold & Ensemble\n",
    "  * 2-1. K-fold 로 데이터 나누기\n",
    "  * 2-2. Ensemble로 성능 올리기\n",
    "3. DAPT & TAPT 구현하기\n",
    "  * 3-1. 학습에 쓰일 데이터셋 준비하기\n",
    "  * 3-2. 데이터셋 구성 및 마스킹하기\n",
    "  * 3-3. MLM 학습하기\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install koeda\n",
    "%pip install googletrans==4.0.0-rc1\n",
    "%pip install git+https://github.com/kakaobrain/pororo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./dataset/fake_news\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1gucFY-P9a1TzdV8Xb-OwVD4TpPy4iyJX' -O ./dataset/fake_news/train.csv\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1J05RaqfknDzTObofL7OyiSmT0B8rX0gZ' -O ./dataset/fake_news/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./dataset/fake_news/train.csv\",index_col=0)\n",
    "test_df = pd.read_csv(\"./dataset/fake_news/test.csv\",index_col=0)\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "news_dump = load_dataset(\"krenerd/korean-newstext-dump\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dump[\"text\"][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoEDA\n",
    "\n",
    "영어용으로 구현된 Easy data augmentation 과 An Easier Data Augmentation 프로젝트를 한글용으로 재구성한 프로젝트입니다.\n",
    "\n",
    "EasyDataAugmentation 파라미터\n",
    " - `p = (alpha_sr, alpha_ri, alpha_rs, prob_rd)` 은 <br> SR, RI , RS , RD 에 대한 각각의 변환을 어느정도 비율로 할 것인지 결정\n",
    " - Synonym Replacement __(SR)__ : 유의어 교체\n",
    " - Random Insertion __(RI)__ : 임의 단어 삽입\n",
    " - Random Swap __(RS)__ : 두 단어 위치 변경\n",
    " - Random Deletion __(RD)__ : 임의 단어 삭제\n",
    "\n",
    " morpheme_analyzer 는 사용될 형태소 분석기를 지정하는 파라미터로, <br> [\"Okt\", \"Kkma\", \"Komoran\", \"Mecab\", \"Hannanum\"] 중 하나를 선택할 수 있다.  <br>__(단, 일부는 설치 필요하며, 각각 형태소를 나누는 기준이 다르다. )__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터 고정\n",
    "ex_data = news_dump[1]['text']\n",
    "ex_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import EasyDataAugmentation\n",
    "\n",
    "def augment_text_data_with_EDA(text,repetition):\n",
    "    \"\"\"입력된 문장에 대해서 EDA를 통해 데이터 증강\"\"\"\n",
    "    eda = EasyDataAugmentation(morpheme_analyzer=\"Okt\")\n",
    "\n",
    "    result = eda(text,p=(0.5, 0.5, 0.5, 0.5), repetition=repetition)\n",
    "\n",
    "    # 증강 결과 출력\n",
    "    print(\"원문: \" , text)\n",
    "    print(\"--\"*100)\n",
    "    for i in range(repetition):\n",
    "        print(f\"증강문{i+1}: \", result[i])\n",
    "    # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_text_data_with_EDA(ex_data,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnEasierDataAugmentation\n",
    "\n",
    "AnEasierDataAugmentation 파라미터\n",
    " - `p = punc_ratio` 는 punctuations 로의 변환을 어느정도 비율로 할 것인지 결정\n",
    " - punctuations 는 ['.', ',', '!', '?', ';', ':'] 로 입력\n",
    " - morpheme_analyzer 는 사용될 형태소분석기를 지정하는 파라미터로, <br> [\"Okt\", \"Kkma\", \"Komoran\", \"Mecab\", \"Hannanum\"] 중 하나를 선택할 수 있다.  <br>__(단, 일부는 설치 필요하며, 각각 형태소를 나누는 기준이 다르다. )__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import AEasierDataAugmentation\n",
    "def augment_text_data_with_AEDA(text, repetition):\n",
    "    \"\"\"입력된 문장에 대해서 AEDA를 통해 데이터 증강\"\"\"\n",
    "    aeda = AEasierDataAugmentation(morpheme_analyzer=\"Okt\", punctuations=[\".\", \",\", \"!\", \"?\", \";\", \":\"])\n",
    "\n",
    "    result = aeda(text, p=0.3, repetition=repetition)\n",
    "\n",
    "    # 증강 결과 출력\n",
    "    print(\"원문: \" , text)\n",
    "    print(\"--\"*100)\n",
    "    for i in range(repetition):\n",
    "        print(f\"증강문{i+1}: \", result[i])\n",
    "    # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_text_data_with_AEDA(ex_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "def augment_text_data_with_BT(text, repetition):\n",
    "    \"\"\"입력된 문장에 대해서 BT를 통해 데이터 증강\"\"\"\n",
    "    # Translator 객체 생성\n",
    "    translator = Translator()\n",
    "    result = []\n",
    "\n",
    "    # 번역 실행 (한국어 > 영어 > 한국어)\n",
    "    for i in range(repetition):\n",
    "        translated = translator.translate(text, src='ko', dest='en')\n",
    "        re_translated = translator.translate(translated.text, src='en', dest='ko')\n",
    "        result.append(re_translated.text)\n",
    "\n",
    "    # 번역 결과 출력\n",
    "    print(\"원문: \" , text)\n",
    "    print(\"--\"*100)\n",
    "    for i in range(repetition):\n",
    "        print(f\"증강문{i+1}: \", result[i])\n",
    "    # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_text_data_with_BT(ex_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pororo import Pororo\n",
    "\n",
    "# def augment_text_data_with_pororo_BT(text, repetition):\n",
    "#     \"\"\"입력된 문장에 대해 Pororo 모델을 이용하여 BT로 데이터 증강\"\"\"\n",
    "#     # Pororo 번역 모델 초기화 (ko -> en, en -> ko)\n",
    "#     translator_ko_en = Pororo(task=\"translation\", lang=\"ko\", tgt=\"en\")\n",
    "#     translator_en_ko = Pororo(task=\"translation\", lang=\"en\", tgt=\"ko\")\n",
    "    \n",
    "#     result = []\n",
    "\n",
    "#     # 번역 실행 (한국어 > 영어 > 한국어)\n",
    "#     for i in range(repetition):\n",
    "#         translated = translator_ko_en(text)\n",
    "#         re_translated = translator_en_ko(translated)\n",
    "#         result.append(re_translated)\n",
    "\n",
    "#     # 번역 결과 출력\n",
    "#     print(\"원문: \" , text)\n",
    "#     print(\"--\" * 100)\n",
    "#     for i in range(repetition):\n",
    "#         print(f\"증강문 {i+1}: \", result[i])\n",
    "    \n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAPT, TAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer , AutoModelForMaskedLM\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_news_dump = news_dump['text'][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, block_size):\n",
    "        concated_ls = []\n",
    "        # 제목 + 본문으로 합치기\n",
    "        for i in range(1,len(data)):\n",
    "            concated_ls.append(data[i-1] + data[i])\n",
    "\n",
    "        batch_encoding = tokenizer(concated_ls, truncation=True, max_length=block_size)\n",
    "        self.examples = batch_encoding[\"input_ids\"]\n",
    "        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_pretraining(tokenizer,train_input):\n",
    "    train_dataset = LineByLineTextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        data=train_input,\n",
    "        block_size=512,\n",
    "    )\n",
    "    # set mlm task\n",
    "    # DataCollatorForSOP로 변경시 SOP 사용 가능 (DataCollatorForLanguageModeling)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15 # 0.3\n",
    "    )\n",
    "    eval_dataset = LineByLineTextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        data=train_input[:20],\n",
    "        block_size=512,\n",
    "    )\n",
    "\n",
    "    return train_dataset, data_collator, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainer_for_pretraining(\n",
    "        model,\n",
    "        data_collator,\n",
    "        dataset,\n",
    "        eval_dataset,\n",
    "        epoch = 10,\n",
    "        batch_size = 16,\n",
    "        accumalation_step = 1,):\n",
    "     # set training args\n",
    "    training_args = TrainingArguments(\n",
    "        report_to = 'tensorboard',\n",
    "        output_dir='./',\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=epoch,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=accumalation_step,\n",
    "        evaluation_strategy = 'steps',\n",
    "        eval_steps=150,\n",
    "        save_steps=150,\n",
    "        save_total_limit=1,\n",
    "        fp16=True,\n",
    "        load_best_model_at_end=True,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # set Trainer class for pre-training\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain():\n",
    "    \"\"\"MLM task 기반 사전학습 진행\"\"\"\n",
    "    # fix a seed\n",
    "    pl.seed_everything(seed=42, workers=False)\n",
    "\n",
    "    # set device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # set model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"klue/bert-base\")\n",
    "    model.to(device)\n",
    "\n",
    "    # set data\n",
    "    train_dataset, data_collator, eval_dataset = prepare_dataset_for_pretraining(tokenizer, ex_news_dump)\n",
    "\n",
    "    # set trainer\n",
    "    trainer = set_trainer_for_pretraining(model,data_collator,train_dataset,eval_dataset)\n",
    "\n",
    "    # train model\n",
    "    print(\"--- Start train ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Finish train ---\")\n",
    "    model.save_pretrained(\"./pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
