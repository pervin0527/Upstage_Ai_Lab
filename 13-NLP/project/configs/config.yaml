general:
  data_path: "./dataset" # 학습에 사용할 데이터셋 경로
  train_file: "cleaned_train.csv"
  valid_file: "cleaned_dev.csv"
  new_file: "new_data.csv"
  model_name: "EbanLee/kobart-summary-v3" ## "suriya7/bart-finetuned-text-summarization", "digit82/kobart-summarization", "eenzeenee/t5-base-korean-summarization"
  output_dir: "./runs" # 모델의 최종 출력 값을 저장할 경로를 설정합니다.
  model_cfg: ""

tokenizer:
  path: "./tokenizer"
  encoder_max_len: 1000 ## 512
  decoder_max_len: 200 ## 100
  bos_token: "<s>" # "[BOS]"
  eos_token: "</s>" # "[EOS]"
  sep_token: "<sep>"
  mask_token: "<mask>s"
  
  # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정해줍니다.
  special_tokens:
    # - '<sep>'
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#Person4#'
    - '#Person5#'
    - '#Person6#'
    - '#Person7#'
    - '#SSN#'
    - '#Email#'
    - '#Address#'
    - '#Reaction#'
    - '#CarNumber#'
    - '#Movietitle#'
    - '#DateOfBirth#'
    - '#CardNumber#'
    - '#PhoneNumber#'
    - '#PassportNumber#'

training:
  seed: 42
  overwrite_output_dir: true

  num_train_epochs: 26
  learning_rate: 2.1169083140275866e-05 ## 0.00001
  per_device_train_batch_size: 32 ## 8
  per_device_eval_batch_size: 4
  optim: 'adamw_torch'
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: 'cosine'
  gradient_accumulation_steps: 4 ## 3
  evaluation_strategy: 'epoch'
  save_strategy: 'epoch'
  save_total_limit: 1000
  fp16: true 
  load_best_model_at_end: true
  logging_dir: "./logs"
  logging_strategy: "epoch"
  predict_with_generate: true
  generation_max_length: 100
  do_train: true
  do_eval: true
  early_stopping_patience: 10
  early_stopping_threshold: 0.001
  report_to: tensorboard
  lambda_r3f: 1.0

inference:
  ckt_path: "/home/pervinco/Upstage_Ai_Lab/project/runs/2024-09-05-09-45-46/checkpoint-1657"
  result_path: "./prediction"
  no_repeat_ngram_size: 2
  early_stopping: true
  generate_max_length: 200
  num_beams: 4
  batch_size: 32

  remove_tokens:
    - <unk>
    - <s>
    - </s>
    - <pad>
    - <sep>
    - <mask>