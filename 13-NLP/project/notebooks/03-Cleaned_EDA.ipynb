{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../dataset\"\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_path}/cleaned_train.csv\")\n",
    "valid_df = pd.read_csv(f\"{data_path}/cleaned_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame\n",
      "(12457, 4)\n",
      "Index(['fname', 'dialogue', 'summary', 'topic'], dtype='object')\n",
      "\n",
      "Valid DataFrame\n",
      "(499, 4)\n",
      "Index(['fname', 'dialogue', 'summary', 'topic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Train DataFrame\")\n",
    "print(train_df.shape)\n",
    "print(train_df.columns)\n",
    "\n",
    "print(\"\\nValid DataFrame\")\n",
    "print(valid_df.shape)\n",
    "print(valid_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname                                              train_5426\n",
      "dialogue    #Person1# 택시를 예약해 주실 수 있나요? #Person2# 개인 차량이 더...\n",
      "summary     #Person1#은 개인 차량 대신 택시를 예약하도록 #Person2#에게 요청하여...\n",
      "topic                                                   차량 예약\n",
      "Name: 5426, dtype: object \n",
      "\n",
      "전체 대화 :\n",
      "#Person1# 택시를 예약해 주실 수 있나요? #Person2# 개인 차량이 더 마음에 드실까요? #Person1# 개인 차량이요? 아니요, 괜찮습니다. #Person2# 개인 차량보다는 리무진이 더 좋습니다. 어떠세요? #Person1# 택시를 부탁드립니다. #Person2# 택시로 하겠습니다. 어디로 가시나요? #Person1# 록펠러 센터입니다. 택시를 바로 여기로 오게 할 수 있나요? #Person2# 택시는 곧 도착할 것입니다, 선생님. #Person1# 좋습니다. 제코트를 입고 내려가겠습니다. #Person2# 선생님이 준비되면 택시가 준비되어 있을 것입니다. \n",
      "\n",
      "요약 내용 :\n",
      "#Person1#은 개인 차량 대신 택시를 예약하도록 #Person2#에게 요청하여 록펠러 센터로 향한다., \n",
      "\n",
      "대화 주제 :\n",
      " 차량 예약, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, train_df.shape[0]-1)\n",
    "sample = train_df.iloc[idx]\n",
    "print(sample, \"\\n\")\n",
    "\n",
    "print(f\"전체 대화 :\\n{sample['dialogue']} \\n\")\n",
    "print(f\"요약 내용 :\\n{sample['summary']}, \\n\")\n",
    "print(f\"대화 주제 :\\n {sample['topic']}, \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_df.sample(10)\n",
    "sample.to_csv(\"../dataset/cleaned_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_train'] = 1\n",
    "valid_df['is_train'] = 0\n",
    "total_df = pd.concat([train_df, valid_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_words(text, n=1000):\n",
    "    mecab = Mecab()\n",
    "    words = []\n",
    "    for sentence in text:\n",
    "        words.extend(mecab.nouns(sentence))\n",
    "    return [word for word, _ in Counter(words).most_common(n)]\n",
    "\n",
    "frequent_words = get_frequent_words(total_df['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece tokens: ['<unk>', '<s>', '</s>', '▁', 's', '요', '있', '를', '것', '이', '리', '게', '지', '신', '스', '기', '나', '않', '가', '서', '어', '다', '일', '상', '러', '시', '라', '드', '로', '자', '금', '도', '야', '인', '장', '구', '사', '분', '싶', '까', '니', '면', '생', '안', '우', '람', '문', '고', '운', '원', '히', '소', '데', '터', '두', '미', '직', '트', '려', '든', '하', '학', '전', '음', '진', '디', '아', '님', '대', '무', '해', '실', '용', '부', '녀', '공', '래', '치', '으', '연', '크', '교', '5', '명', '식', '유', '후', '제', '세', '성', '영', '위', '주', '행', '관', '엇', '화', '들', '레', '경']\n",
      "Total SentencePiece tokens: 1077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=temp_mecab.txt --model_prefix=sp_model --vocab_size=1077 --character_coverage=0.9995 --split_by_whitespace=true\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: temp_mecab.txt\n",
      "  input_format: \n",
      "  model_prefix: sp_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1077\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: temp_mecab.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4591 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 12932 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 24 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=6965785\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=1074\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 12932 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3136450\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 11511 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 12932\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 22106\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 22106 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9801 obj=8.00901 num_tokens=44276 num_tokens/piece=4.5175\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8053 obj=7.43848 num_tokens=44561 num_tokens/piece=5.53347\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6038 obj=7.37646 num_tokens=45682 num_tokens/piece=7.56575\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6035 obj=7.35998 num_tokens=45695 num_tokens/piece=7.57167\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4526 obj=7.62243 num_tokens=48907 num_tokens/piece=10.8058\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4526 obj=7.57024 num_tokens=48908 num_tokens/piece=10.806\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3394 obj=7.85673 num_tokens=52758 num_tokens/piece=15.5445\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3394 obj=7.79335 num_tokens=52815 num_tokens/piece=15.5613\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2545 obj=8.05152 num_tokens=57480 num_tokens/piece=22.5855\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2545 obj=8.02592 num_tokens=57480 num_tokens/piece=22.5855\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1908 obj=8.22827 num_tokens=62335 num_tokens/piece=32.6703\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1908 obj=8.18556 num_tokens=62335 num_tokens/piece=32.6703\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1431 obj=8.48772 num_tokens=67851 num_tokens/piece=47.4151\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1431 obj=8.39067 num_tokens=67851 num_tokens/piece=47.4151\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1184 obj=8.76809 num_tokens=72095 num_tokens/piece=60.891\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1184 obj=8.63489 num_tokens=72095 num_tokens/piece=60.891\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: sp_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sp_model.vocab\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab()\n",
    "def mecab_tokenize(text):\n",
    "    return ' '.join(mecab.morphs(text))\n",
    "\n",
    "def train_sentencepiece_with_mecab(text, vocab_size=1077, model_prefix='sp_model'):\n",
    "    preprocessed_text = [mecab_tokenize(sentence) for sentence in text]\n",
    "    \n",
    "    with open('temp_mecab.txt', 'w', encoding='utf-8') as f:\n",
    "        for line in preprocessed_text:\n",
    "            f.write(line + '\\n')\n",
    "    \n",
    "    SentencePieceTrainer.Train(\n",
    "        f'--input=temp_mecab.txt --model_prefix={model_prefix} '\n",
    "        f'--vocab_size={vocab_size} --character_coverage=0.9995 '\n",
    "        f'--split_by_whitespace=true')\n",
    "    \n",
    "    sp = SentencePieceProcessor()\n",
    "    sp.Load(f'{model_prefix}.model')\n",
    "    \n",
    "    return [sp.IdToPiece(id) for id in range(sp.GetPieceSize())]\n",
    "\n",
    "sentencepiece_tokens = train_sentencepiece_with_mecab(total_df['dialogue'])\n",
    "print(f\"SentencePiece tokens: {sentencepiece_tokens[:100]}\")\n",
    "print(f\"Total SentencePiece tokens: {len(sentencepiece_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날', '위험', '컵', '화요일', '피아노', '청구', '주인', '사이', '길', '걱정', '화장실', '운동', '불', '대신', '파운드', '일부', '감기', '텐데', '결혼', '페이지', '경찰', '끝', '왕', '고양이', '자동차', '병', '그곳', '경기', '구입', '코트', '프로젝트', '상자', '기업', '시스템', '숙제', '마일', '금액', '복용', '빵', '활동', '입학', '교통', '규칙', '언어', '최소', '성과', '티켓', '블록', '색상', '베이징', '여행', '비행', '보통', '회의', '이쪽', '에너지', '내용', '사이트', '포기', '뭔가', '처음', '언니', '전문', '구매', '휴가', '숨', '기사', '운', '상사', '매니저', '주차', '시기', '정류장', '결정', '타이핑', '연락', '유용', '보고서', '치즈', '클래식', '예술', '세트', '이상', '외국', '층', '모습', '대학교', '내일', '퍼센트', '꽃', '안전', '삶', '과목', '뭘', '명함', '생선', '이해', '의사', '목', '추가', '그걸로', '동안', '제출', '인생', '미안', '흡연', '아들', '애', '종류', '백', '근무', '쪽', '통과', '코', '이틀', '가이드', '며칠', '면접', '나이', '가격', '축구', '산책', '화', '기차', '방문', '유지', '초대', '통', '주문', '아내', '기타', '색', '일요일', '아래', '졸업', '나무', '편리', '교육', '더블', '능력', '문서', '최근', '아이', '가게', '제니', '채소', '해결', '삼촌', '시험', '사무실', '다이어트', '신발', '인터넷', '등록', '연주', '신문', '현재', '명', '일본', '마찬가지', '수표', '개', '옆', '가입', '기록', '대만', '제안', '정말', '물건', '미래', '학위', '흥미', '스타일', '지원', '영수증', '잡지', '마지막', '오염', '정상', '차량', '인사', '고등학교', '월', '정원', '전시회', '출장', '최대', '동생', '입금', '그게', '실례', '외', '원', '여름', '얼마', '의미', '참석', '정부', '피터', '정장', '비디오', '특별', '빨간색', '헬스', '사고', '웹', '정리', '보관', '종일', '편안', '학생', '주방', '스트레스', '공연', '예금', '설명', '향상', '잠', '주제', '시간', '왕복', '이걸로', '일정', '브라운', '그것', '너', '조금', '둘', '동의', '거스름돈', '가치', '제', '영향', '비행기', '현금', '가족', '실수', '이번', '주의', '행운', '최선', '기본', '콘서트', '자리', '교수', '장', '허용', '주말', '투표', '주변', '마을', '사항', '아이스크림', '발표', '공간', '잔', '중국', '건강', '연결', '건물', '때', '소프트웨어', '전공', '나머지', '임대료', '속도', '만족', '책', '제품', '대략', '너희', '행사', '고급', '노트북', '저기', '면허증', '박사', '세계', '화이트', '거기', '이거', '도착', '반대', '영국', '변화', '행복', '평소', '존', '객실', '공항', '거래', '그거', '주일', '뭔가요', '채널', '춤', '영화관', '게', '근처', '최신', '팁', '관계', '장비', '수입', '문', '사진', '계약서', '관광', '권', '청소', '필요', '겁니다', '번호', '아이디어', '투어', '실망', '경제', '사이즈', '크리스마스', '마이크', '피부', '자료', '담배', '비밀', '자신', '나중', '관리', '제공', '다섯', '그걸', '그분', '샌드위치', '여사', '냉장고', '침실', '처리', '룸', '이유', '주요', '앨리스', '파티', '가구', '주', '최고', '영업', '스미스', '옷', '누구', '남편', '톰', '환영', '건데', '연기', '오', '외출', '고려', '사', '테이블', '전', '해', '상황', '귀하', '마크', '대로', '바람', '얼굴', '조사', '어려움', '목표', '병원', '과학', '위', '초', '가을', '판매', '샐러드', '손님', '널', '하나', '질문', '식사', '작성', '신경', '비서', '스프', '고객', '배', '유일', '음악', '식당', '주소', '발견', '소설', '우편', '노래', '나', '중국어', '성격', '도서관', '노력', '온라인', '고기', '부족', '자연', '과일', '여동생', '알', '인상', '공원', '수프', '미국', '밥', '그', '마케팅', '비', '넌', '건', '종이', '음료', '이름', '서로', '여섯', '바보', '스키', '현지', '자기', '상', '벽', '카드', '기분', '유럽', '효과', '년', '글', '난', '선택', '도시', '사실', '기대', '모델', '훈련', '밴드', '차이', '것', '수잔', '공유', '사회', '음식', '지', '여권', '프랑스', '금요일', '데이비드', '수영', '관련', '포장', '속', '할인', '술', '절약', '파일', '수업', '수강', '신청', '전문가', '청구서', '월요일', '책상', '빕', '형', '삼', '성장', '어디', '가지', '기계', '요구', '취소', '환전', '평가', '천만', '이력서', '중요', '팀', '브랜드', '위안', '동네', '달', '부분', '위치', '모자', '일', '침대', '관심', '이게', '누가', '다양', '정책', '안', '슈퍼마켓', '시설', '긴장', '오랜만', '감당', '버스', '열', '가요', '어머니', '뒤', '다행', '확신', '경험', '요청', '소식', '품질', '거', '집중', '시', '선호', '테', '단어', '회사', '이동', '협상', '내년', '의견', '세부', '이모', '직원', '부서', '케이크', '완벽', '사업', '갈색', '씨', '말씀', '그건', '안내', '표', '재미', '강', '포함', '얘기', '팬', '시도', '강의', '선수', '플레이어', '기술', '요리', '승진', '신용카드', '시장', '통화', '할머니', '신용', '영화', '오후', '프로그램', '남성', '프랑스어', '모두', '비자', '체크인', '귀사', '넥타이', '파란색', '돈', '대출', '이웃', '물론', '직장', '퇴근', '그룹', '지갑', '상태', '계산', '빌', '축하', '점', '주식', '불편', '발', '번', '캘리포니아', '일반', '바다', '파리', '확인', '패션', '예약', '정도', '지난달', '선생', '계약', '제외', '어제', '시내', '아파트', '하루', '자전거', '비즈니스', '검사', '쇼핑', '천', '쇼', '메리', '교환', '완료', '스타', '아기', '사촌', '문화', '영어', '논의', '디자인', '발전', '진행', '이사', '비용', '캐나다', '낚시', '수학', '제인', '몸', '선물', '이걸', '낭비', '방법', '미터', '가방', '걸', '이전', '항공', '열쇠', '문제', '목록', '시작', '개월', '작업', '계속', '보호', '세일', '반', '동물', '누군가', '우리', '이야기', '만약', '엄마', '출발', '책임', '아무것', '사장', '산업', '일곱', '독일', '맥주', '보험', '약간', '뭐', '개인', '작동', '친구', '콜라', '데', '예', '가능', '공기', '택시', '장소', '지하철', '걸로', '생각', '약', '세상', '곳', '참여', '점심', '제게', '나라', '소리', '센트', '동료', '차', '정보', '부인', '초콜릿', '운전', '신분증', '이탈리아', '건가요', '대부분', '입', '은행', '뉴욕', '수수료', '패키지', '예상', '와인', '계좌', '내', '물', '이', '그녀', '시골', '조심', '리', '야채', '저희', '계획', '기간', '다리', '이메일', '호', '오전', '해외', '공장', '연습', '그날', '역사', '제한', '유감', '이용', '꿈', '치킨', '대회', '설정', '중', '레스토랑', '언제', '연구', '결국', '논문', '자격', '월급', '개설', '양', '친절', '왼쪽', '식물', '딸', '아침', '잭', '약속', '겨울', '드레스', '분야', '담당', '흰색', '후', '휴식', '공부', '날씨', '셔츠', '네', '출근', '기억', '잠시', '박물관', '미국인', '극장', '우체국', '센터', '여러분', '특정', '전통', '부탁', '편지', '스트리트', '상점', '이건', '관리자', '도움', '상품', '박', '뉴스', '성함', '회계', '맛', '날짜', '수리', '사랑', '소개', '서류', '호주', '말', '라디오', '휴대폰', '해변', '전화', '팔', '수요일', '보장', '표현', '주차장', '만큼', '소고기', '자원', '전기', '우유', '아빠', '사라', '결제', '농담', '바닥', '그림', '산', '신청서', '방향', '할아버지', '장미', '피곤', '출신', '결과', '오랫동안', '등', '적', '성적', '직무', '안녕', '광고', '감사', '결혼식', '뜻', '때문', '정확', '저축', '스포츠', '캠퍼스', '양식', '국제', '기회', '호텔', '식', '테니스', '수집', '체중', '요즘', '다음', '느낌', '변경', '지불', '살', '사과', '전체', '커피', '학기', '데이트', '등산', '사용', '도', '여기', '그때', '텔레비전', '조건', '피자', '대화', '점수', '방', '체증', '달러', '게임', '주스', '카메라', '합리', '스테이크', '홍콩', '도전', '손', '요금', '마감', '조언', '창문', '지도', '화학', '오늘', '동물원', '수', '머리', '전부', '줄', '인터뷰', '분', '대', '짐', '당신', '무료', '클럽', '작년', '밖', '대학', '블랙', '무언가', '디저트', '중간', '지금', '상상', '제임스', '예전', '저녁', '투자', '급여', '거리', '외국인', '환경', '저', '준비', '인기', '이후', '오른쪽', '서명', '역', '작품', '뿐', '직업', '십', '봄', '생산', '메뉴', '인출', '도로', '만', '면', '여가', '추천', '경우', '국가', '지구', '과정', '올해', '취미', '눈', '앤', '행동', '업무', '이곳', '여성', '목요일', '학교', '키', '아버지', '스웨터', '생활', '좌석', '편', '방식', '기능', '흥분', '모퉁이', '역할', '활용', '시계', '컴퓨터', '룸메이트', '이것', '스티븐', '밤', '피트', '예정', '무엇', '자유', '사람', '여자', '런던', '성공', '혼자', '부엌', '공', '집', '남자', '생일', '일자리', '현대', '공식', '계란', '법', '서비스', '수영장', '메시지', '마음', '간', '바', '표시', '지역', '배송', '세', '부모', '크기', '웨이터', '매력', '앞', '고용', '농구', '운영', '아무', '거실', '토요일', '발생', '올림픽', '청소년']\n"
     ]
    }
   ],
   "source": [
    "def refine_tokens(frequent_words, sentencepiece_tokens):\n",
    "    refined_tokens = set()\n",
    "    for token in sentencepiece_tokens:\n",
    "        token_without_underscore = token.replace('▁', '')\n",
    "        if token_without_underscore in frequent_words or token in frequent_words:\n",
    "            refined_tokens.add(token)\n",
    "    \n",
    "    # EDA 결과(frequent words)에서 SentencePiece 토큰과 겹치지 않는 단어들 추가\n",
    "    for word in frequent_words:\n",
    "        if word not in refined_tokens and f'▁{word}' not in refined_tokens:\n",
    "            refined_tokens.add(word)\n",
    "    \n",
    "    return list(refined_tokens)\n",
    "\n",
    "refined_tokens = refine_tokens(frequent_words, sentencepiece_tokens)\n",
    "print(refined_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날', '위험', '컵', '화요일', '피아노', '청구', '주인', '사이', '길', '걱정', '화장실', '운동', '불', '대신', '파운드', '일부', '감기', '텐데', '결혼', '페이지', '경찰', '끝', '왕', '고양이', '자동차', '병', '그곳', '경기', '구입', '코트', '프로젝트', '상자', '기업', '시스템', '숙제', '마일', '금액', '복용', '빵', '활동', '입학', '교통', '규칙', '언어', '최소', '성과', '티켓', '블록', '색상', '베이징', '여행', '비행', '보통', '회의', '이쪽', '에너지', '내용', '사이트', '포기', '뭔가', '처음', '언니', '전문', '구매', '휴가', '숨', '기사', '운', '상사', '매니저', '주차', '시기', '정류장', '결정', '타이핑', '연락', '유용', '보고서', '치즈', '클래식', '예술', '세트', '이상', '외국', '층', '모습', '대학교', '내일', '퍼센트', '꽃', '안전', '삶', '과목', '뭘', '명함', '생선', '이해', '의사', '목', '추가', '그걸로', '동안', '제출', '인생', '미안', '흡연', '아들', '애', '종류', '백', '근무', '쪽', '통과', '코', '이틀', '가이드', '며칠', '면접', '나이', '가격', '축구', '산책', '화', '기차', '방문', '유지', '초대', '통', '주문', '아내', '기타', '색', '일요일', '아래', '졸업', '나무', '편리', '교육', '더블', '능력', '문서', '최근', '아이', '가게', '제니', '채소', '해결', '삼촌', '시험', '사무실', '다이어트', '신발', '인터넷', '등록', '연주', '신문', '현재', '명', '일본', '마찬가지', '수표', '개', '옆', '가입', '기록', '대만', '제안', '정말', '물건', '미래', '학위', '흥미', '스타일', '지원', '영수증', '잡지', '마지막', '오염', '정상', '차량', '인사', '고등학교', '월', '정원', '전시회', '출장', '최대', '동생', '입금', '그게', '실례', '외', '원', '여름', '얼마', '의미', '참석', '정부', '피터', '정장', '비디오', '특별', '빨간색', '헬스', '사고', '웹', '정리', '보관', '종일', '편안', '학생', '주방', '스트레스', '공연', '예금', '설명', '향상', '잠', '주제', '시간', '왕복', '이걸로', '일정', '브라운', '그것', '너', '조금', '둘', '동의', '거스름돈', '가치', '제', '영향', '비행기', '현금', '가족', '실수', '이번', '주의', '행운', '최선', '기본', '콘서트', '자리', '교수', '장', '허용', '주말', '투표', '주변', '마을', '사항', '아이스크림', '발표', '공간', '잔', '중국', '건강', '연결', '때', '소프트웨어', '전공', '나머지', '임대료', '속도', '만족', '책', '제품', '대략', '너희', '행사', '고급', '노트북', '저기', '면허증', '박사', '세계', '화이트', '거기', '이거', '도착', '반대', '영국', '변화', '행복', '평소', '존', '객실', '공항', '거래', '그거', '주일', '뭔가요', '채널', '춤', '영화관', '게', '근처', '최신', '팁', '관계', '장비', '수입', '문', '사진', '계약서', '관광', '권', '청소', '필요', '겁니다', '번호', '아이디어', '투어', '실망', '경제', '사이즈', '크리스마스', '마이크', '피부', '자료', '담배', '비밀', '자신', '나중', '관리', '제공', '다섯', '그걸', '그분', '샌드위치', '여사', '냉장고', '침실', '처리', '룸', '이유', '주요', '앨리스', '파티', '가구', '주', '최고', '영업', '스미스', '옷', '누구', '남편', '톰', '환영', '건데', '연기', '오', '외출', '고려', '사', '테이블', '전', '해', '상황', '귀하', '마크', '대로', '바람', '얼굴', '조사', '어려움', '목표', '병원', '과학', '위', '초', '가을', '판매', '샐러드', '손님', '널', '하나', '질문', '식사', '작성', '신경', '비서', '스프', '고객', '배', '유일', '음악', '식당', '주소', '발견', '소설', '우편', '노래', '나', '중국어', '성격', '도서관', '노력', '온라인', '고기', '부족', '자연', '과일', '여동생', '알', '인상', '공원', '수프', '미국', '밥', '그', '마케팅', '비', '넌', '건', '종이', '음료', '이름', '서로', '여섯', '바보', '스키', '현지', '자기', '상', '벽', '카드', '기분', '유럽', '효과', '년', '글', '난', '선택', '도시', '사실', '기대', '모델', '훈련', '밴드', '차이', '것', '수잔', '공유', '사회', '음식', '지', '여권', '프랑스', '금요일', '데이비드', '수영', '관련', '포장', '속', '할인', '술', '절약', '파일', '수업', '수강', '신청', '전문가', '청구서', '월요일', '책상', '빕', '형', '삼', '성장', '어디', '가지', '기계', '요구', '취소', '환전', '평가', '천만', '이력서', '중요', '팀', '브랜드', '위안', '동네', '달', '부분', '위치', '모자', '일', '침대', '관심', '이게', '누가', '다양', '정책', '안', '슈퍼마켓', '시설', '긴장', '오랜만', '감당', '버스', '열', '가요', '어머니', '뒤', '다행', '확신', '경험', '요청', '소식', '품질', '거', '집중', '시', '선호', '테', '단어', '이동', '협상', '내년', '의견', '세부', '이모', '직원', '부서', '케이크', '완벽', '사업', '갈색', '씨', '말씀', '그건', '안내', '표', '재미', '강', '포함', '얘기', '팬', '강의', '선수', '플레이어', '기술', '요리', '승진', '신용카드', '시장', '통화', '할머니', '신용', '영화', '오후', '프로그램', '남성', '프랑스어', '모두', '비자', '체크인', '귀사', '넥타이', '파란색', '돈', '대출', '이웃', '물론', '직장', '퇴근', '그룹', '지갑', '상태', '계산', '빌', '축하', '점', '주식', '불편', '발', '번', '캘리포니아', '일반', '바다', '파리', '확인', '패션', '예약', '정도', '지난달', '계약', '제외', '어제', '시내', '아파트', '하루', '자전거', '비즈니스', '검사', '쇼핑', '천', '쇼', '메리', '교환', '완료', '스타', '아기', '사촌', '문화', '영어', '논의', '디자인', '발전', '진행', '비용', '캐나다', '낚시', '수학', '제인', '몸', '선물', '이걸', '낭비', '방법', '미터', '가방', '걸', '이전', '열쇠', '문제', '목록', '시작', '개월', '작업', '계속', '보호', '세일', '반', '동물', '누군가', '우리', '이야기', '만약', '엄마', '출발', '책임', '아무것', '사장', '산업', '일곱', '독일', '맥주', '보험', '약간', '뭐', '개인', '작동', '친구', '콜라', '데', '예', '가능', '공기', '택시', '장소', '지하철', '걸로', '생각', '약', '세상', '곳', '참여', '점심', '제게', '나라', '소리', '센트', '동료', '차', '정보', '부인', '초콜릿', '운전', '신분증', '이탈리아', '건가요', '대부분', '입', '은행', '뉴욕', '수수료', '패키지', '예상', '와인', '계좌', '내', '물', '이', '그녀', '시골', '조심', '리', '야채', '저희', '계획', '기간', '다리', '이메일', '호', '오전', '해외', '공장', '연습', '그날', '역사', '제한', '유감', '이용', '꿈', '치킨', '대회', '설정', '중', '레스토랑', '언제', '연구', '결국', '논문', '자격', '월급', '개설', '양', '친절', '왼쪽', '식물', '딸', '아침', '잭', '약속', '겨울', '드레스', '분야', '담당', '흰색', '후', '휴식', '공부', '날씨', '셔츠', '네', '출근', '기억', '잠시', '박물관', '미국인', '극장', '우체국', '센터', '여러분', '특정', '전통', '부탁', '편지', '스트리트', '상점', '이건', '관리자', '도움', '상품', '박', '뉴스', '성함', '회계', '맛', '날짜', '수리', '사랑', '소개', '서류', '호주', '말', '라디오', '휴대폰', '해변', '전화', '팔', '수요일', '보장', '표현', '주차장', '만큼', '소고기', '자원', '전기', '우유', '아빠', '사라', '결제', '농담', '바닥', '그림', '산', '신청서', '방향', '할아버지', '장미', '피곤', '출신', '결과', '오랫동안', '등', '적', '성적', '직무', '안녕', '광고', '감사', '결혼식', '뜻', '때문', '정확', '저축', '스포츠', '캠퍼스', '양식', '국제', '기회', '호텔', '식', '테니스', '수집', '체중', '요즘', '다음', '느낌', '변경', '지불', '살', '사과', '전체', '커피', '학기', '데이트', '등산', '사용', '도', '여기', '그때', '텔레비전', '조건', '피자', '대화', '점수', '방', '체증', '달러', '게임', '주스', '카메라', '합리', '스테이크', '홍콩', '도전', '손', '요금', '마감', '조언', '창문', '지도', '화학', '오늘', '동물원', '수', '머리', '전부', '줄', '인터뷰', '분', '대', '짐', '당신', '무료', '클럽', '작년', '밖', '대학', '블랙', '무언가', '디저트', '중간', '지금', '제임스', '예전', '저녁', '투자', '급여', '거리', '외국인', '환경', '저', '준비', '인기', '이후', '오른쪽', '서명', '역', '작품', '뿐', '직업', '십', '봄', '생산', '메뉴', '인출', '도로', '만', '면', '여가', '추천', '경우', '국가', '지구', '과정', '올해', '취미', '눈', '앤', '행동', '업무', '이곳', '여성', '목요일', '학교', '키', '아버지', '스웨터', '생활', '좌석', '편', '방식', '기능', '흥분', '모퉁이', '역할', '활용', '시계', '컴퓨터', '룸메이트', '이것', '스티븐', '밤', '피트', '예정', '무엇', '자유', '사람', '여자', '런던', '성공', '혼자', '부엌', '공', '집', '남자', '생일', '일자리', '현대', '공식', '계란', '법', '서비스', '수영장', '메시지', '마음', '간', '바', '표시', '지역', '배송', '세', '부모', '크기', '웨이터', '매력', '앞', '고용', '농구', '운영', '아무', '거실', '토요일', '발생', '올림픽', '청소년']\n"
     ]
    }
   ],
   "source": [
    "def remove_similar_tokens(tokens, similarity_threshold=0.8):\n",
    "    def similarity(a, b):\n",
    "        return len(set(a) & set(b)) / float(len(set(a) | set(b)))\n",
    "    \n",
    "    unique_tokens = []\n",
    "    for token in tokens:\n",
    "        if not any(similarity(token, t) > similarity_threshold for t in unique_tokens):\n",
    "            unique_tokens.append(token)\n",
    "    \n",
    "    return unique_tokens\n",
    "\n",
    "final_tokens = remove_similar_tokens(refined_tokens)\n",
    "print(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary size: 30507\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EbanLee/kobart-summary-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_tokens(final_tokens)\n",
    "\n",
    "print(f\"Updated vocabulary size: {len(tokenizer)}\")\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../tokenizer/tokenizer_config.json',\n",
       " '../tokenizer/special_tokens_map.json',\n",
       " '../tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
