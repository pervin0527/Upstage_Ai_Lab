{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, LlamaForCausalLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import wandb\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"beomi/Llama-3-Open-Ko-8B\"\n",
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"../data/\",\n",
    "        \"model_name\": model_name,\n",
    "        \"output_dir\": \"./\"\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 256,\n",
    "        \"decoder_max_len\": 50,\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 20,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": False,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 100,\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"wandb\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"legendki\",\n",
    "        \"project\": \"NLP-Summarization\",\n",
    "        \"name\": \"Llama-LoRA-Optuna\",\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"model ckt path\",\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 100,\n",
    "        \"num_beams\": 4,\n",
    "        \"batch_size\": 32,\n",
    "        \"remove_tokens\": ['<s>', '</s>', '<pad>']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(config, tokenizer, pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token, \" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token, \" \") for sentence in replaced_labels]\n",
    "\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels, avg=True)\n",
    "\n",
    "    result = {\n",
    "        'rouge1': results['rouge-1']['f'],\n",
    "        'rouge2': results['rouge-2']['f'],\n",
    "        'rougeL': results['rouge-l']['f'],\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):\n",
    "    print('-'*10, 'Make training arguments', '-'*10,)\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config['general']['output_dir'],  \n",
    "        overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
    "        num_train_epochs=config['training']['num_train_epochs'],\n",
    "        learning_rate=config['training']['learning_rate'],\n",
    "        per_device_train_batch_size=config['training']['per_device_train_batch_size'], \n",
    "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'], \n",
    "        warmup_ratio=config['training']['warmup_ratio'], \n",
    "        weight_decay=config['training']['weight_decay'], \n",
    "        lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "        optim=config['training']['optim'],\n",
    "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "        evaluation_strategy=config['training']['evaluation_strategy'], \n",
    "        save_strategy=config['training']['save_strategy'],\n",
    "        save_total_limit=config['training']['save_total_limit'], \n",
    "        fp16=config['training']['fp16'],\n",
    "        load_best_model_at_end=config['training']['load_best_model_at_end'], \n",
    "        seed=config['training']['seed'],\n",
    "        logging_dir=config['training']['logging_dir'], \n",
    "        logging_strategy=config['training']['logging_strategy'],\n",
    "        predict_with_generate=config['training']['predict_with_generate'],\n",
    "        generation_max_length=config['training']['generation_max_length'],\n",
    "        do_train=config['training']['do_train'],\n",
    "        do_eval=config['training']['do_eval'],\n",
    "        report_to=config['training']['report_to']\n",
    "    )\n",
    "\n",
    "    wandb.init(\n",
    "        entity=config['wandb']['entity'],\n",
    "        project=config['wandb']['project'],\n",
    "        name=config['wandb']['name'],\n",
    "    )\n",
    "\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
    "    os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
    "    )\n",
    "    print('-'*10, 'Make training arguments complete', '-'*10,)\n",
    "    print('-'*10, 'Make trainer', '-'*10,)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=generate_model,  \n",
    "        args=training_args,\n",
    "        train_dataset=train_inputs_dataset,\n",
    "        eval_dataset=val_inputs_dataset,\n",
    "        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),\n",
    "        callbacks=[MyCallback]\n",
    "    )\n",
    "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['general']['model_name'])\n",
    "    tokenizer.padding_side = 'left'\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        config['general']['model_name'], \n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(device)\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    data_path = config['general']['data_path']\n",
    "    train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join(data_path, 'dev.csv'))\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, bos_token, eos_token):\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "    @staticmethod\n",
    "    def make_set_as_df(file_path, is_train=True):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if is_train:\n",
    "            return df[['fname', 'dialogue', 'summary']]\n",
    "        else:\n",
    "            return df[['fname', 'dialogue']]\n",
    "\n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        if is_test:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n",
    "            return encoder_input.tolist(), list(decoder_input)\n",
    "        else:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n",
    "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    train_file_path = os.path.join(data_path, 'train.csv')\n",
    "    val_file_path = os.path.join(data_path, 'dev.csv')\n",
    "\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n",
    "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n",
    "    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n",
    "\n",
    "    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
    "    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n",
    "    print('-'*10, 'Load data complete', '-'*10, )\n",
    "\n",
    "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer'][\n",
    "            'encoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer'][\n",
    "            'decoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_outputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "                                          add_special_tokens=True, truncation=True, max_length=config['tokenizer'][\n",
    "            'decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_outputs,\n",
    "                                           len(encoder_input_train))\n",
    "\n",
    "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer'][\n",
    "            'encoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer'][\n",
    "            'decoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_outputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "                                              add_special_tokens=True, truncation=True, max_length=config['tokenizer'][\n",
    "            'decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs,\n",
    "                                       val_tokenized_decoder_outputs, len(encoder_input_val))\n",
    "\n",
    "    print('-'*10, 'Make dataset complete', '-'*10, )\n",
    "    return train_inputs_dataset, val_inputs_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, length):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2)\n",
    "\n",
    "        label = self.labels['input_ids'][idx]\n",
    "        item['labels'] = torch.cat([label, torch.full((item['input_ids'].shape[-1] - label.shape[-1],), -100)], dim=0)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, length):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2)\n",
    "\n",
    "        label = self.labels['input_ids'][idx]\n",
    "\n",
    "        if item['input_ids'].shape[-1] > label.shape[-1]:\n",
    "            label = torch.cat([label, torch.full((item['input_ids'].shape[-1] - label.shape[-1],), -100)], dim=0)\n",
    "        elif item['input_ids'].shape[-1] < label.shape[-1]:\n",
    "            item['input_ids'] = torch.cat([item['input_ids'], torch.full((label.shape[-1] - item['input_ids'].shape[-1],), tokenizer.pad_token_id)], dim=0)\n",
    "\n",
    "        item['labels'] = label\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    train_file_path = os.path.join(data_path, 'train.csv')\n",
    "    val_file_path = os.path.join(data_path, 'dev.csv')\n",
    "\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
    "    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n",
    "\n",
    "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_outputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "                                          add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_outputs, len(encoder_input_train))\n",
    "\n",
    "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_outputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "                                              add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_outputs, len(encoder_input_val))\n",
    "\n",
    "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
    "    return train_inputs_dataset, val_inputs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    config_data['training']['learning_rate'] = 1e-5\n",
    "    config_data['training']['per_device_train_batch_size'] = 4\n",
    "    config_data['training']['num_train_epochs'] = 26\n",
    "    config_data['training']['warmup_ratio'] = 0.09577831393575928\n",
    "    config_data['training']['optim'] = 'adamw_hf'\n",
    "    config_data['training']['gradient_accumulation_steps'] = 3\n",
    "    config_data['training']['lr_scheduler_type'] = 'cosine'\n",
    "    config_data['training']['fp16'] = True\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on: {device}\")\n",
    "\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_train(config_data, device)\n",
    "    print(\"Model and Tokenizer Loaded.\")\n",
    "\n",
    "    preprocessor = Preprocess(config_data['tokenizer']['bos_token'], config_data['tokenizer']['eos_token'])\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config_data, preprocessor, config_data['general']['data_path'], tokenizer)\n",
    "\n",
    "    trainer = load_trainer_for_train(config_data, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_metrics = trainer.evaluate(eval_dataset=val_inputs_dataset)\n",
    "    rougeL = eval_metrics.get('rougeL', 0.0)\n",
    "\n",
    "    return rougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "generate_model, tokenizer = load_tokenizer_and_model_for_train(config_data, device)\n",
    "print(\"Model and Tokenizer Loaded.\")\n",
    "\n",
    "preprocessor = Preprocess(config_data['tokenizer']['bos_token'], config_data['tokenizer']['eos_token'])\n",
    "train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config_data, preprocessor, config_data['general']['data_path'], tokenizer)\n",
    "\n",
    "trainer = load_trainer_for_train(config_data, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "best_model_path = trainer.state.best_model_checkpoint\n",
    "config_data['inference']['ckt_path'] = best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
