{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summ_dataset(Dataset):\n",
    "    \"\"\"dataframe을 torch dataset class로 변환\"\"\"\n",
    "    def __init__(self, document, tokenizer):\n",
    "      self.dataset = document\n",
    "      self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        input_ids=torch.LongTensor(self.dataset[\"input_ids\"][idx])\n",
    "        labels=torch.LongTensor(self.dataset[\"labels\"][idx])\n",
    "\n",
    "        attention_mask=input_ids.ne(self.tokenizer.pad_token_id) ## padding token은 attention 계산에 반영되면 안되니까 mask를 정의한다..\n",
    "\n",
    "        return dict(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    \"\"\"csv file을 dataframe으로 load\"\"\"\n",
    "    dataset = pd.read_csv(dataset_dir)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, doc_tokenizer, sum_tokenizer, doc_max_length, sum_max_len, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    토크나이징을 위한 함수. training과 inference 단계에서의 토크나이징이 별도로 구축되어 있다.\n",
    "    - 학습일 때는 본문과 요약이 함께 입력된다. --> 본문 [SEP] 요약\n",
    "    - 반면 추론 단계에서는 본문만 입력되어 요약을 생성해야함.\n",
    "    \"\"\"\n",
    "    ## 추론 단계\n",
    "    if mode == \"infer\":\n",
    "      ## inference 시에는 document 만 주어지고, 마지막에 bos_token을 붙여 생성 시작하게 한다.\n",
    "      document_text = dataset['dialogue']\n",
    "      summ_text = dataset['summary']\n",
    "\n",
    "      ## document + bos\n",
    "      ## <pad> <pad> d_1 d_2 d_3 ... d_n <bos>\n",
    "      document = [doc_tokenizer(documents, padding = 'max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for documents in document_text.values]\n",
    "      # labels에는 요약문만큼의 빈칸으로 채워준 후 모델이 예측하도록 함\n",
    "      labels = [[-100] * sum_max_len for _ in document]\n",
    "      out = {\"input_ids\": document, \"labels\": labels}\n",
    "\n",
    "    elif mode == \"train\":\n",
    "      document_text = dataset['dialogue']\n",
    "      summary_text = dataset['summary']\n",
    "      ## document 와 summary를 이어 붙여서 모델 학습에 사용. \n",
    "      ## document 뒤에는 bos_token 을 붙여 생성 시작을 명시하고, summary 를 붙인 후 맨 뒤에는 eos_token 으로 생성의 끝을 명시.\n",
    "      ## ⭐️ document를 padding 할 때는 side를 left로 주고, summary를 padding 할 때는 side를 right로 줘서 연속된 문장이 생성될 수 있도록 한다.\n",
    "      ## ⭐️ <pad> <pad> d_1 d_2 d_3 ... d_n <bos> s_1 s_2 ... s_m <eos> <pad> <pad>\n",
    "      document = [doc_tokenizer(documents, padding='max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for documents in document_text.values]\n",
    "      summary = [sum_tokenizer(summaries + sum_tokenizer.eos_token, padding = 'max_length',truncation=True, max_length=sum_max_len, add_special_tokens=True)['input_ids'] for summaries in summary_text.values]\n",
    "\n",
    "      ## 구성해둔 document 와 summary를 결합하여 input 준비\n",
    "      tokenized_senetences = [document + summary for (document, summary) in zip(document, summary)]\n",
    "      ## document는 생성할 내용이 아니므로 -100으로 label을 부여한다.\n",
    "      # Input : <pad> <pad> d_1  d_2  d_3  ... d_n  <bos> s_1 s_2 ... s_m <eos> <pad> <pad>\n",
    "      # Label : -100  -100    -100 -100 -100  ... -100  -100  s_1 s_2 ... s_m <eos> -100 -100\n",
    "\n",
    "      labels = [[-100] * len(document) + summary for (document, summary) in zip(document, summary)]\n",
    "      ## ⭐️ Q. 다음에 올 Token을 생성하도록 학습해야 되니까 s_1의 label은 한 칸씩 밀린 s_2가 들어가야 되지 않나요?\n",
    "      # A. Transformer 라이브러리의 GPT 구현(https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1103-L1104)을 보면, \n",
    "      # 모델의 Logit을 [: -1]만 가져오고, Label은 [1: ]을 가져와서 Loss를 계산하게 됩니다.\n",
    "      # 즉, Input과 Label이 한 칸씩 밀린채로 입력을 넣지 않아도, 내부 구현에 의해 자동으로 밀린 채로 계산이 됩니다.\n",
    "\n",
    "      # padding 된 부분이 학습되지 않도록 -100 으로 치환\n",
    "      labels = [[-100 if token == sum_tokenizer.pad_token_id else token for token in l] for l in labels]\n",
    "      out = {\"input_ids\": tokenized_senetences, \"labels\": labels}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_dir, doc_tokenizer,sum_tokenizer,doc_max_len, sum_max_len):\n",
    "    \"\"\"학습(train)과 평가(test)를 위한 데이터셋을 준비\"\"\"\n",
    "    # load_data\n",
    "    train_dataset = load_data(os.path.join(dataset_dir, \"train_translated.csv\"))\n",
    "    val_dataset = load_data(os.path.join(dataset_dir, \"dev_translated.csv\"))\n",
    "\n",
    "    tokenized_train = tokenized_dataset(train_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len)\n",
    "    tokenized_val = tokenized_dataset(val_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len)\n",
    "\n",
    "    summ_train_dataset = summ_dataset(tokenized_train, doc_tokenizer)\n",
    "    summ_val_dataset = summ_dataset(tokenized_val, doc_tokenizer)\n",
    "\n",
    "    return summ_train_dataset , summ_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(args, pred):\n",
    "    MODEL_NAME = args.model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # 예측값과 정답\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    if isinstance(preds, tuple):\n",
    "      preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds[:, args.doc_max_len:], skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels[:, args.doc_max_len:], skip_special_tokens=True)\n",
    "\n",
    "    metric = datasets.load_metric(\"rouge\")\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return {\n",
    "        'Rouge-2' : result['rouge2']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model_for_train(args):\n",
    "    MODEL_NAME = args.model_name\n",
    "    doc_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    sum_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, config=model_config)\n",
    "    \n",
    "    return doc_tokenizer, sum_tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer_for_train(args, model, summ_train_dataset, summ_val_dataset):\n",
    "    \"\"\"학습(train)을 위한 huggingface trainer 설정\"\"\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        output_dir=args.save_path + \"results\",  # output directory\n",
    "        save_total_limit=args.save_limit,  # number of total save model.\n",
    "        save_steps=args.save_step,  # model saving step.\n",
    "        num_train_epochs=args.epochs,  # total number of training epochs\n",
    "        learning_rate=args.lr,  # learning_rate\n",
    "        per_device_train_batch_size=args.batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "        warmup_steps=args.warmup_steps,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=args.weight_decay,  # strength of weight decay\n",
    "        logging_dir=args.save_path + \"logs\",  # directory for storing logs\n",
    "        logging_steps=args.logging_steps,  # log saving step.\n",
    "        evaluation_strategy=\"steps\",  # evaluation strategy to adopt during training\n",
    "            # `no`: No evaluation during training.\n",
    "            # `steps`: Evaluate every `eval_steps`.\n",
    "            # `epoch`: Evaluate every end of epoch.\n",
    "        eval_steps=args.eval_steps,  # evaluation step.\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    ## Add callback & optimizer & scheduler\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5, early_stopping_threshold=0.001\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=args.weight_decay,\n",
    "        amsgrad=False,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=summ_train_dataset,  # training dataset\n",
    "        eval_dataset=summ_val_dataset,  # evaluation dataset\n",
    "        compute_metrics=lambda p: compute_metrics(args, p),\n",
    "        callbacks=[MyCallback],\n",
    "        optimizers=(\n",
    "            optimizer,\n",
    "            get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=args.warmup_steps,\n",
    "                    num_training_steps=len(summ_train_dataset) * args.epochs,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    pl.seed_everything(seed=42, workers=False)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    doc_tokenizer, sum_tokenizer , model = load_tokenizer_and_model_for_train(args)\n",
    "    model.to(device)\n",
    "\n",
    "    summ_train_dataset, summ_val_dataset = prepare_dataset(args.dataset_dir,doc_tokenizer, sum_tokenizer,args.doc_max_len,args.sum_max_len)\n",
    "    trainer = load_trainer_for_train(args, model, summ_train_dataset, summ_val_dataset)\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"./best_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    \"\"\"학습(train)과 추론(infer)에 사용되는 arguments 관리하는 class\"\"\"\n",
    "    dataset_dir = \"../dataset\"\n",
    "    model_type = \"gpt2\"\n",
    "    model_name = 'MrBananaHuman/kogpt2_small'\n",
    "    save_path = \"./GPT2\"\n",
    "    save_step = 400\n",
    "    logging_steps = 200\n",
    "    eval_steps = 200\n",
    "    save_limit = 5\n",
    "    seed = 42\n",
    "    epochs = 20 # 10\n",
    "    batch_size = 2\n",
    "    doc_max_len = 512\n",
    "    sum_max_len = 128\n",
    "    lr = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 5\n",
    "    scheduler = \"linear\"\n",
    "    model_dir = \"./best_model\"\n",
    "    \n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
