{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/ir-project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/pervinco/.cache/huggingface/token\n",
      "Login successful\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/pervinco/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/pervinco/Upstage_Ai_Lab/Final/IR/src\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import anthropic\n",
    "import threading\n",
    "import huggingface_hub\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "os.environ['UPSTAGE_API_KEY'] = upstage_api_key\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = anthropic_api_key\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(hf_token)\n",
    "\n",
    "from config import Args\n",
    "from data.data import load_document\n",
    "from dense_retriever.model import load_dense_model\n",
    "from sparse_retriever.model import load_sparse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "\n",
    "# total_documents = load_document(path=\"../dataset/processed_documents.jsonl\")\n",
    "total_documents = load_document(path=\"../dataset/en_4.0_document.jsonl\")\n",
    "print(len(total_documents))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = args.chunk_size,\n",
    "    chunk_overlap  = args.chunk_overlap,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<document>\n",
    "{DOCUMENT}\n",
    "</document> \n",
    "전체 문서에서 발췌한 청크는 다음과 같습니다.\n",
    "<chunk> \n",
    "{CHUNK}\n",
    "</chunk>\n",
    "\n",
    "이 청크가 전체 문서의 어떤 맥락에 속하는지 한국어로 간결하게 설명하세요. 청크가 문서의 어떤 부분에서 발췌되었는지에 대한 정보를 제공하고, 청크의 배경 설명을 명확하게 해주세요.\n",
    "\n",
    "입력 예시:\n",
    "    건강한 사람이 에너지 균형을 평형 상태로 유지하는 것은 중요합니다.\n",
    "예시에 대한 설명:\n",
    "    이 청크는 건강한 생활습관과 관련된 영양학 문서에서 발췌되었으며, 에너지 섭취와 소비의 균형을 유지하는 방법에 대한 설명입니다. 이 설명은 특히 식단과 운동을 통한 에너지 조절의 중요성에 초점을 맞추고 있습니다.\n",
    "출력 예시:\n",
    "    이 청크는 영양학과 관련된 2024년 연구 보고서에서 발췌되었습니다. 이 문서에서는 에너지 균형을 유지하는 것이 건강한 생활에 얼마나 중요한지 설명하고 있으며, 특히 1-2주 동안의 에너지 섭취와 소비 조절을 강조하고 있습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_contextual_retrieval(document, chunk, model: str, client: OpenAI):\n",
    "    prompt = \"\"\"\n",
    "    <document>\n",
    "    {DOCUMENT}\n",
    "    </document> \n",
    "    The following chunk is extracted from the entire document:\n",
    "    <chunk> \n",
    "    {CHUNK}\n",
    "    </chunk>\n",
    "\n",
    "    Please provide a concise explanation in English of how this chunk fits within the overall context of the document. Include information about where in the document the chunk was extracted from and clarify the background of the chunk.\n",
    "\n",
    "    Input Example:\n",
    "        It is important for a healthy person to maintain energy balance in equilibrium.\n",
    "    Explanation for the example:\n",
    "        This chunk is extracted from a nutrition-related document about healthy lifestyle, explaining how to maintain a balance between energy intake and expenditure. The explanation focuses on the importance of regulating energy through diet and exercise.\n",
    "\n",
    "    Output Example:\n",
    "        This chunk is extracted from a 2024 nutrition research report. The document explains how maintaining energy balance is crucial for a healthy lifestyle, with particular emphasis on regulating energy intake and expenditure over a 1-2 week period.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt.format(DOCUMENT=document, CHUNK=chunk)\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "            time.sleep(2 ** attempt + random.random())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(args):\n",
    "    document, chunk, model, client = args\n",
    "    result = gpt_contextual_retrieval(document.page_content, chunk, model, client)\n",
    "    if result is not None:\n",
    "        return {\n",
    "            \"docid\": document.metadata['docid'],\n",
    "            \"content\": f\"{chunk}\\n\\n{result}\"\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_documents(documents, text_splitter, output_file, max_workers=5):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for document in tqdm(documents, desc=\"Processing documents\"):\n",
    "                chunks = text_splitter.split_text(document.page_content)\n",
    "                futures = [executor.submit(process_chunk, (document, chunk, model, client)) for chunk in chunks]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    if result is not None:\n",
    "                        f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "                \n",
    "                time.sleep(random.uniform(1, 2))  # 문서 간 1~2초 랜덤 대기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 4272/4272 [8:11:10<00:00,  6.90s/it]    \n"
     ]
    }
   ],
   "source": [
    "output_file = '../dataset/gpt_contextual_retrieval_documents_en_v3.jsonl'\n",
    "process_documents(total_documents, text_splitter, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
