{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /data/ephemeral/home/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/data/ephemeral/home/Upstage_Ai_Lab/Final/IR/src\")\n",
    "\n",
    "import json\n",
    "import random\n",
    "import huggingface_hub\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "os.environ['UPSTAGE_API_KEY'] = upstage_api_key\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "voyage_api_key = os.getenv('VOYAGE_API_KEY')\n",
    "os.environ['VOYAGE_API_KEY'] = voyage_api_key\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(hf_token)\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from config import Args\n",
    "from data.data import load_document, chunking, load_query\n",
    "from sparse_retriever.model import load_sparse_model\n",
    "from dense_retriever.model import load_dense_model, load_upstage_encoder, load_hf_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../dataset/test\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272\n",
      "12816\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "args.encoder_method = \"hugginface\"\n",
    "args.hf_model_name = \"dragonkue/bge-m3-ko\"\n",
    "args.doc_file_path = \"../dataset/processed_documents_queries.jsonl\"\n",
    "args.faiss_index_file = None\n",
    "\n",
    "args.chunk_size = 100\n",
    "args.chunk_overlap = 50\n",
    "\n",
    "documents = load_document(args.doc_file_path)\n",
    "print(len(documents))\n",
    "\n",
    "questions = load_query(args.doc_file_path)\n",
    "print(len(questions))\n",
    "\n",
    "random.shuffle(questions)\n",
    "questions = questions[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = load_hf_encoder(args.hf_model_name, args.model_kwargs, args.encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24799\n"
     ]
    }
   ],
   "source": [
    "rec_chunks = chunking(args, documents)\n",
    "print(len(rec_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_map(gt, pred):    \n",
    "    sum_average_precision = 0    \n",
    "    for j in pred:        \n",
    "        if gt[j[\"eval_id\"]]:            \n",
    "            hit_count = 0            \n",
    "            sum_precision = 0            \n",
    "            for i,docid in enumerate(j[\"topk\"][:3]):                \n",
    "                if docid in gt[j[\"eval_id\"]]:                    \n",
    "                    hit_count += 1                    \n",
    "                    sum_precision += hit_count/(i+1)            \n",
    "            average_precision = sum_precision / hit_count if hit_count > 0 else 0        \n",
    "        else:            \n",
    "            average_precision = 0 if j[\"topk\"] else 1        \n",
    "        sum_average_precision += average_precision    \n",
    "    return sum_average_precision/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(rec_chunks, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 29.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.9207500000000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = vector_store.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540\n"
     ]
    }
   ],
   "source": [
    "splitter = SemanticChunker(embedder, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "perc_chunks = splitter.split_documents(documents)\n",
    "print(len(perc_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/test/percentile_chunk_documents.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for doc in perc_chunks:\n",
    "        id = doc.metadata['docid']\n",
    "        content = doc.page_content\n",
    "        \n",
    "        json.dump({\"docid\":id, \"content\":content}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(perc_chunks, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:21<00:00, 46.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.9175000000000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = vector_store.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272\n"
     ]
    }
   ],
   "source": [
    "splitter = SemanticChunker(embedder, breakpoint_threshold_type=\"standard_deviation\")\n",
    "\n",
    "std_chunks = splitter.split_documents(documents)\n",
    "print(len(std_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/test/std_chunk_documents.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for doc in std_chunks:\n",
    "        id = doc.metadata['docid']\n",
    "        content = doc.page_content\n",
    "        \n",
    "        json.dump({\"docid\":id, \"content\":content}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(std_chunks, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:17<00:00, 55.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.9230000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = vector_store.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5741\n"
     ]
    }
   ],
   "source": [
    "splitter = SemanticChunker(embedder, breakpoint_threshold_type=\"interquartile\")\n",
    "\n",
    "inter_chunks = splitter.split_documents(documents)\n",
    "print(len(inter_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/test/interquartile_chunk_documents.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for doc in inter_chunks:\n",
    "        id = doc.metadata['docid']\n",
    "        content = doc.page_content\n",
    "        \n",
    "        json.dump({\"docid\":id, \"content\":content}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(inter_chunks, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 51.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.9199166666666674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = vector_store.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8505\n"
     ]
    }
   ],
   "source": [
    "splitter = SemanticChunker(embedder, breakpoint_threshold_type=\"gradient\")\n",
    "\n",
    "grad_chunks = splitter.split_documents(documents)\n",
    "print(len(grad_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/test/gradient_chunk_documents.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for doc in grad_chunks:\n",
    "        id = doc.metadata['docid']\n",
    "        content = doc.page_content\n",
    "        \n",
    "        json.dump({\"docid\":id, \"content\":content}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(grad_chunks, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:21<00:00, 46.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.9045000000000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = vector_store.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
