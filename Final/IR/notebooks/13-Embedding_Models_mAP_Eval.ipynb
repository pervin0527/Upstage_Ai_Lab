{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /data/ephemeral/home/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data/ephemeral/home/Upstage_Ai_Lab/Final/IR\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import faiss\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import huggingface_hub\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "from src.sparse_retriever.kiwi_bm25 import KiwiBM25Retriever\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "os.environ['UPSTAGE_API_KEY'] = upstage_api_key\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "\n",
    "en_document_path = \"../dataset/en_4.0_processed_documents_queries.jsonl\"\n",
    "ko_document_path = \"../dataset/processed_documents_queries.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_normalizer(val: float) -> float:\n",
    "    return 1 / (1 + val)\n",
    "\n",
    "def load_upstage_encoder(model_name):\n",
    "    encoder = UpstageEmbeddings(model=model_name)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def load_openai_encoder(model_name):\n",
    "    encoder = OpenAIEmbeddings(model=model_name)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def load_hf_encoder(model_name, model_kwargs, encode_kwargs):\n",
    "    encoder = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                    model_kwargs=model_kwargs,\n",
    "                                    encode_kwargs=encode_kwargs)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def load_hf_reranker(model_name, retriever):\n",
    "    reranker = HuggingFaceCrossEncoder(model_name=model_name)\n",
    "    compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever.as_retriever(search_kwargs={\"k\": 10}))\n",
    "\n",
    "    return compression_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "def load_document(file_path):\n",
    "    raw_documents = load_jsonl(file_path)\n",
    "\n",
    "    documents = []\n",
    "    for doc in raw_documents:\n",
    "        doc_id = doc['docid']\n",
    "        content = doc['content']\n",
    "        documents.append(Document(page_content=content, metadata={\"docid\": doc_id}))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_query(file_path):\n",
    "    raw_queries = load_jsonl(file_path)\n",
    "\n",
    "    queries = []\n",
    "    for query in raw_queries:\n",
    "        doc_id = query['docid']\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            queries.append({\"query\": query[f'question{i}'], \"metadata\": {\"docid\": doc_id}})\n",
    "    \n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_documents = load_document(en_document_path)\n",
    "en_questions = load_query(en_document_path)\n",
    "\n",
    "random.shuffle(en_questions)\n",
    "en_questions = en_questions[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_documents = load_document(ko_document_path)\n",
    "ko_questions = load_query(ko_document_path)\n",
    "\n",
    "random.shuffle(ko_questions)\n",
    "ko_questions = ko_questions[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_map(gt, pred):    \n",
    "    sum_average_precision = 0    \n",
    "    for j in pred:        \n",
    "        if gt[j[\"eval_id\"]]:            \n",
    "            hit_count = 0            \n",
    "            sum_precision = 0            \n",
    "            for i,docid in enumerate(j[\"topk\"][:3]):                \n",
    "                if docid in gt[j[\"eval_id\"]]:                    \n",
    "                    hit_count += 1                    \n",
    "                    sum_precision += hit_count/(i+1)            \n",
    "            average_precision = sum_precision / hit_count if hit_count > 0 else 0        \n",
    "        else:            \n",
    "            average_precision = 0 if j[\"topk\"] else 1        \n",
    "        sum_average_precision += average_precision    \n",
    "    return sum_average_precision/len(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KiwiBM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = KiwiBM25Retriever.from_documents(ko_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:41<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.8799999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.search_with_score(query)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc in search_result:\n",
    "        score = doc.metadata.get('score', 'N/A')\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace - intfloat/multilingual-e5-large-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_hf_encoder(\"intfloat/multilingual-e5-large-instruct\", \n",
    "                          {\"device\": \"cuda:0\"}, \n",
    "                          {\"normalize_embeddings\": False, \"clean_up_tokenization_spaces\": True})\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4272개 documents 임베딩 시간 22.5초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:09<00:00, 54.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.8493333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3, score_threshold=0.6)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500개 쿼리 처리시간 4초\n",
    "\n",
    "mAP : 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace - bge-m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "encoder = load_hf_encoder('BAAI/bge-m3', \n",
    "                          {\"device\": \"cuda:0\"}, \n",
    "                          {\"normalize_embeddings\": False, \"clean_up_tokenization_spaces\": True})\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4272개 documents 임베딩 시간 22.5초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 55.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.8933333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_hf_encoder(\"dragonkue/bge-m3-ko\", \n",
    "                          {\"device\": \"cuda:0\"}, \n",
    "                          {\"normalize_embeddings\": False, \"clean_up_tokenization_spaces\": True})\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 55.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.9193333333333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_hf_encoder(\"nlpai-lab/KoE5\", \n",
    "                          {\"device\": \"cuda:0\"}, \n",
    "                          {\"normalize_embeddings\": False, \"clean_up_tokenization_spaces\": True})\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:09<00:00, 55.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.8946666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI - text-embedding-3-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_openai_encoder(\"text-embedding-3-large\")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4272개 documents 임베딩 시간 37초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500개 쿼리 처리시간 5분\n",
    "\n",
    "mAP : 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upstage - solar-embedding-1-large-passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_upstage_encoder(\"solar-embedding-1-large-passage\")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss_index.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스를 npy로부터 불러옴\n",
    "index = faiss.read_index(\"faiss_index.npy\")\n",
    "\n",
    "# 벡터 스토어 다시 생성\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "# 검색에 활용\n",
    "retrieval = vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4272개 documents 임베딩 시간 12분 17초..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500개 쿼리 처리시간 4분 16초\n",
    "\n",
    "mAP : 0.9073"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
