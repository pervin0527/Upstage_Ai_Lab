{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/pervinco/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/pervinco/Upstage_Ai_Lab/Final/IR\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import faiss\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import huggingface_hub\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "from src.sparse_retriever.kiwi_bm25 import KiwiBM25Retriever\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "os.environ['UPSTAGE_API_KEY'] = upstage_api_key\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "\n",
    "en_document_path = \"../dataset/en_4.0_processed_documents_queries.jsonl\"\n",
    "ko_document_path = \"../dataset/processed_documents_queries.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_normalizer(val: float) -> float:\n",
    "    return 1 / (1 + val)\n",
    "\n",
    "def load_upstage_encoder(model_name):\n",
    "    encoder = UpstageEmbeddings(model=model_name)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def load_openai_encoder(model_name):\n",
    "    encoder = OpenAIEmbeddings(model=model_name)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def load_hf_encoder(model_name, model_kwargs, encode_kwargs):\n",
    "    encoder = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                    model_kwargs=model_kwargs,\n",
    "                                    encode_kwargs=encode_kwargs)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def load_hf_reranker(model_name, retriever):\n",
    "    reranker = HuggingFaceCrossEncoder(model_name=model_name)\n",
    "    compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever.as_retriever(search_kwargs={\"k\": 10}))\n",
    "\n",
    "    return compression_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "def load_document(file_path):\n",
    "    raw_documents = load_jsonl(file_path)\n",
    "\n",
    "    documents = []\n",
    "    for doc in raw_documents:\n",
    "        doc_id = doc['docid']\n",
    "        content = doc['content']\n",
    "        documents.append(Document(page_content=content, metadata={\"docid\": doc_id}))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_query(file_path):\n",
    "    raw_queries = load_jsonl(file_path)\n",
    "\n",
    "    queries = []\n",
    "    for query in raw_queries:\n",
    "        doc_id = query['docid']\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            queries.append({\"query\": query[f'question{i}'], \"metadata\": {\"docid\": doc_id}})\n",
    "    \n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_documents = load_document(en_document_path)\n",
    "en_questions = load_query(en_document_path)\n",
    "\n",
    "random.shuffle(en_questions)\n",
    "en_questions = en_questions[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_documents = load_document(ko_document_path)\n",
    "ko_questions = load_query(ko_document_path)\n",
    "\n",
    "random.shuffle(ko_questions)\n",
    "ko_questions = ko_questions[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_map(gt, pred):    \n",
    "    sum_average_precision = 0    \n",
    "    for j in pred:        \n",
    "        if gt[j[\"eval_id\"]]:            \n",
    "            hit_count = 0            \n",
    "            sum_precision = 0            \n",
    "            for i,docid in enumerate(j[\"topk\"][:3]):                \n",
    "                if docid in gt[j[\"eval_id\"]]:                    \n",
    "                    hit_count += 1                    \n",
    "                    sum_precision += hit_count/(i+1)            \n",
    "            average_precision = sum_precision / hit_count if hit_count > 0 else 0        \n",
    "        else:            \n",
    "            average_precision = 0 if j[\"topk\"] else 1        \n",
    "        sum_average_precision += average_precision    \n",
    "    return sum_average_precision/len(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KiwiBM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = KiwiBM25Retriever.from_documents(ko_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.search_with_score(query)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc in search_result:\n",
    "        score = doc.metadata.get('score', 'N/A')\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace - intfloat/multilingual-e5-large-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_hf_encoder(\"intfloat/multilingual-e5-large-instruct\", \n",
    "                          {\"device\": \"cuda:0\"}, \n",
    "                          {\"normalize_embeddings\": False, \"clean_up_tokenization_spaces\": True})\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4272개 documents 임베딩 시간 22.5초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3, score_threshold=0.6)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_hf_encoder(\"intfloat/multilingual-e5-large-instruct\", \n",
    "                          {\"device\": \"cuda:0\"}, \n",
    "                          {\"normalize_embeddings\": False, \"clean_up_tokenization_spaces\": True})\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=en_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in en_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(en_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3, score_threshold=0.6)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500개 쿼리 처리시간 4초\n",
    "\n",
    "mAP : 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace - intfloat/multilingual-e5-large-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_hf_encoder('BAAI/bge-m3', \n",
    "                          {\"device\": \"cuda:0\"}, \n",
    "                          {\"normalize_embeddings\": False, \"clean_up_tokenization_spaces\": True})\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4272개 documents 임베딩 시간 22.5초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a695f422b298417fb515eeacc4ce4b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caea840aa149472a98e91bcafce2057d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43581fefcfa440c795f647744bb16836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12899b37bbc344ce835962f8b24f4918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f97ba9b89a747afb0c3e73280b6d984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/796 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f2ecee1cf44a4ab5fee61aa5b62ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb4d556b96e4254ab1a3d7fcdb7e0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ae9eaf31674ee7b9810284e0c25a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e728ef2e43944fd397f44c733e8f4983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/752k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb40af70630476b9e4af60873c966c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f5488e271e4f08959d27d77f303431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = load_hf_encoder('bespin-global/klue-sroberta-base-continue-learning-by-mnr', \n",
    "                          {\"device\": \"cuda:0\"}, \n",
    "                          {\"normalize_embeddings\": False, \"clean_up_tokenization_spaces\": True})\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 183.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.8266666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI - text-embedding-3-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_openai_encoder(\"text-embedding-3-large\")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4272개 documents 임베딩 시간 37초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500개 쿼리 처리시간 5분\n",
    "\n",
    "mAP : 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_openai_encoder(\"text-embedding-3-large\")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=en_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in en_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(en_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upstage - solar-embedding-1-large-passage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_upstage_encoder(\"solar-embedding-1-large-passage\")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss_index.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스를 npy로부터 불러옴\n",
    "index = faiss.read_index(\"faiss_index.npy\")\n",
    "\n",
    "# 벡터 스토어 다시 생성\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "# 검색에 활용\n",
    "retrieval = vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4272개 documents 임베딩 시간 12분 17초..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500개 쿼리 처리시간 4분 16초\n",
    "\n",
    "mAP : 0.9073"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_upstage_encoder(\"solar-embedding-1-large-passage\")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=en_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "for question in en_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(en_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "encoder = OllamaEmbeddings(model=\"llama3-instruct-8b\")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(encoder.embed_query(\"파이썬\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=encoder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    "    relevance_score_fn=score_normalizer\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=ko_documents)\n",
    "retrieval = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:25<00:00, 19.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.10533333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt = {}\n",
    "for question in ko_questions:\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    gt[question_id] = [question_id]\n",
    "\n",
    "pred = []\n",
    "for question in tqdm(ko_questions):\n",
    "    query, question_id = question['query'], question['metadata']['docid']\n",
    "    \n",
    "    search_result = retrieval.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    topk_result = []\n",
    "    for doc, score in search_result:\n",
    "        topk_result.append(doc.metadata.get('docid'))\n",
    "\n",
    "    pred.append({\n",
    "        \"eval_id\": question_id,\n",
    "        \"topk\": topk_result\n",
    "    })\n",
    "\n",
    "mean_average_precision = calc_map(gt, pred)\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
