{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/pervinco/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/pervinco/Upstage_Ai_Lab/Final/IR/src\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import anthropic\n",
    "import threading\n",
    "import huggingface_hub\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "os.environ['UPSTAGE_API_KEY'] = upstage_api_key\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = anthropic_api_key\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(hf_token)\n",
    "\n",
    "from config import Args\n",
    "from data.data import load_document\n",
    "from dense_retriever.model import load_dense_model\n",
    "from sparse_retriever.model import load_sparse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "\n",
    "total_documents = load_document(path=\"../dataset/processed_documents.jsonl\")\n",
    "print(len(total_documents))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = args.chunk_size,\n",
    "    chunk_overlap  = args.chunk_overlap,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "전체 문서 내에 배치하려는 청크는 다음과 같습니다.\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "이 청크가 전체 문서에서 어떤 맥락에 속하는지 설명하는 간결한 문맥을 한국어로 작성하세요. 답변은 이 청크에 관한 짧고 구체적인 배경 설명을 포함해야 하며, 청크가 문서의 어느 부분에서 나온 것인지에 대한 정보를 제공해야 합니다.\n",
    "\n",
    "    입력 예시:\n",
    "        회사의 매출이 전 분기 대비 3% 증가했습니다.\n",
    "\n",
    "    주어진 청크 예시에서는 '회사'가 어떤 회사를 말하는 것인지, '전 분기'가 정확하게 몇년도 몇분기에 대한 것인지 정보가 포함되어 있지 않습니다. 따라서 당신은 아래 출력 예시처럼 입력되는 청크를 읽고 정보검색에 유용하도록 더 명확한 청크로 재구성해야합니다.\n",
    "\n",
    "    출력 예시: \n",
    "        이 청크는 2023년 2분기에 ACME 회사의 실적을 다룬 SEC 보고서에서 발췌되었습니다. 이전 분기의 수익은 3억 1천 4백만 달러였으며, 회사의 수익은 이전 분기 대비 3% 증가했습니다.\n",
    "\"\"\"\n",
    "\n",
    "token_counts = {\n",
    "    'input': 0,\n",
    "    'output': 0,\n",
    "    'cache_read': 0,\n",
    "    'cache_creation': 0\n",
    "}\n",
    "token_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def situate_context(doc: str, chunk: str, max_retries=5) -> str:\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.beta.prompt_caching.messages.create(\n",
    "                model=\"claude-3-haiku-20240307\",\n",
    "                max_tokens=1024,\n",
    "                temperature=0.0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                                \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "            )\n",
    "            with token_lock:\n",
    "                token_counts['input'] += response.usage.input_tokens\n",
    "                token_counts['output'] += response.usage.output_tokens\n",
    "                token_counts['cache_read'] += response.usage.cache_read_input_tokens\n",
    "                token_counts['cache_creation'] += response.usage.cache_creation_input_tokens\n",
    "            return response\n",
    "        except anthropic.RateLimitError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait_time = (2 ** attempt) + (random.random() * 0.1)\n",
    "            print(f\"Rate limit hit. Waiting for {wait_time:.2f} seconds before retry.\")\n",
    "            time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(document, chunk):\n",
    "    result = situate_context(document.page_content, chunk)\n",
    "    return {\n",
    "        \"docid\": document.metadata['docid'],\n",
    "        \"content\": f\"{chunk}\\n\\n{result.content[0].text}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(documents, text_splitter, output_file, parallel_threads=5):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        with ThreadPoolExecutor(max_workers=parallel_threads) as executor:\n",
    "            for document in tqdm(documents, desc=\"Processing documents\"):\n",
    "                chunks = text_splitter.split_text(document.page_content)\n",
    "                futures = [executor.submit(process_chunk, document, chunk) for chunk in chunks]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing chunk: {e}\")\n",
    "                \n",
    "                time.sleep(random.uniform(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../dataset/antropic_contextual_retrieval_documents.jsonl'\n",
    "\n",
    "# 중단 지점 저장 및 불러오기\n",
    "try:\n",
    "    with open('progress.json', 'r') as f:\n",
    "        progress = json.load(f)\n",
    "        start_index = progress['last_processed_index'] + 1\n",
    "except FileNotFoundError:\n",
    "    start_index = 0\n",
    "\n",
    "try:\n",
    "    process_documents(total_documents[start_index:], text_splitter, output_file)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"작업이 중단되었습니다. 진행 상황을 저장합니다.\")\n",
    "finally:\n",
    "    with open('progress.json', 'w') as f:\n",
    "        json.dump({'last_processed_index': start_index + len(total_documents) - 1}, f)\n",
    "\n",
    "# 토큰 사용량 출력\n",
    "print(f\"Total input tokens: {token_counts['input']}\")\n",
    "print(f\"Total output tokens: {token_counts['output']}\")\n",
    "print(f\"Total tokens read from cache: {token_counts['cache_read']}\")\n",
    "print(f\"Total tokens written to cache: {token_counts['cache_creation']}\")\n",
    "\n",
    "total_tokens = token_counts['input'] + token_counts['cache_read'] + token_counts['cache_creation']\n",
    "savings_percentage = (token_counts['cache_read'] / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "print(f\"Total input token savings from prompt caching: {savings_percentage:.2f}% of all input tokens used were read from cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=upstage_api_key,\n",
    "#     base_url=\"https://api.upstage.ai/v1/solar\"\n",
    "# )\n",
    "# model = \"solar-pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<document>\n",
    "{DOCUMENT}\n",
    "</document> \n",
    "전체 문서에서 발췌한 청크는 다음과 같습니다.\n",
    "<chunk> \n",
    "{CHUNK}\n",
    "</chunk>\n",
    "\n",
    "이 청크가 전체 문서의 어떤 맥락에 속하는지 한국어로 간결하게 설명하세요. 청크가 문서의 어떤 부분에서 발췌되었는지에 대한 정보를 제공하고, 청크의 배경 설명을 명확하게 해주세요.\n",
    "\n",
    "입력 예시:\n",
    "    건강한 사람이 에너지 균형을 평형 상태로 유지하는 것은 중요합니다.\n",
    "예시에 대한 설명:\n",
    "    이 청크는 건강한 생활습관과 관련된 영양학 문서에서 발췌되었으며, 에너지 섭취와 소비의 균형을 유지하는 방법에 대한 설명입니다. 이 설명은 특히 식단과 운동을 통한 에너지 조절의 중요성에 초점을 맞추고 있습니다.\n",
    "출력 예시:\n",
    "    이 청크는 영양학과 관련된 2024년 연구 보고서에서 발췌되었습니다. 이 문서에서는 에너지 균형을 유지하는 것이 건강한 생활에 얼마나 중요한지 설명하고 있으며, 특히 1-2주 동안의 에너지 섭취와 소비 조절을 강조하고 있습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_contextual_retrieval(document, chunk, model: str, client: OpenAI):\n",
    "    prompt = \"\"\"\n",
    "    <document>\n",
    "    {DOCUMENT}\n",
    "    </document> \n",
    "    전체 문서에서 발췌한 청크는 다음과 같습니다.\n",
    "    <chunk> \n",
    "    {CHUNK}\n",
    "    </chunk>\n",
    "\n",
    "    1.주어진 청크에 대한 제목, 요약, 여러 개의 가설적 질문 등 다양한 정보들을 생성해주세요.\n",
    "    2.이 청크가 전체 문서의 어떤 맥락에 속하는지 한국어로 간결하게 설명하세요. 청크가 문서의 어떤 부분에서 발췌되었는지에 대한 정보를 제공하고, 청크의 배경 설명을 명확하게 해주세요.\n",
    "    \"\"\"\n",
    "    prompt = prompt.format(DOCUMENT=document, CHUNK=chunk)\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "            time.sleep(2 ** attempt + random.random())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(args):\n",
    "    document, chunk, model, client = args\n",
    "    result = gpt_contextual_retrieval(document.page_content, chunk, model, client)\n",
    "    if result is not None:\n",
    "        return {\n",
    "            \"docid\": document.metadata['docid'],\n",
    "            \"content\": f\"{chunk}\\n\\n{result}\"\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_documents(documents, text_splitter, output_file, max_workers=5):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for document in tqdm(documents, desc=\"Processing documents\"):\n",
    "                chunks = text_splitter.split_text(document.page_content)\n",
    "                futures = [executor.submit(process_chunk, (document, chunk, model, client)) for chunk in chunks]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    if result is not None:\n",
    "                        f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "                \n",
    "                time.sleep(random.uniform(1, 2))  # 문서 간 1~2초 랜덤 대기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 4272/4272 [12:12:42<00:00, 10.29s/it]   \n"
     ]
    }
   ],
   "source": [
    "output_file = '../dataset/gpt_contextual_retrieval_documents_v4.jsonl'\n",
    "process_documents(total_documents, text_splitter, output_file)\n",
    "\n",
    "# try:\n",
    "#     with open('progress.json', 'r') as f:\n",
    "#         progress = json.load(f)\n",
    "#         start_index = progress['last_processed_index'] + 1\n",
    "# except FileNotFoundError:\n",
    "#     start_index = 0\n",
    "\n",
    "# try:\n",
    "#     process_documents(total_documents[start_index:], text_splitter, output_file)\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"작업이 중단되었습니다. 진행 상황을 저장합니다.\")\n",
    "# finally:\n",
    "#     with open('progress.json', 'w') as f:\n",
    "#         json.dump({'last_processed_index': start_index + len(total_documents) - 1}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
