{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import huggingface_hub\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_community.chat_models import ChatOllama, ChatOpenAI\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "os.environ['UPSTAGE_API_KEY'] = upstage_api_key\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(hf_token)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    retrieval_debug = False\n",
    "    llm_model = \"ollama\"\n",
    "    \n",
    "    src_lang = \"ko\"\n",
    "    if src_lang == \"en\":\n",
    "        eval_file_path = \"../dataset/eval.jsonl\" ## \"../dataset/en_eval.jsonl\" --> 성능이 별로임.\n",
    "        doc_file_path = \"../dataset/en_4.0_document.jsonl\" ## \"../dataset/processed_documents.jsonl\"\n",
    "    else:\n",
    "        eval_file_path = \"../dataset/eval.jsonl\"\n",
    "        doc_file_path = \"../dataset/processed_documents.jsonl\"\n",
    "\n",
    "    output_path = \"./outputs/output.csv\"\n",
    "\n",
    "    ## sparse or dense or ensemble\n",
    "    doc_method = \"dense\"\n",
    "    encoder_method = \"huggingface\" ## huggingface, upstage, openai\n",
    "    retriever_weights = [0.6, 0.4] ## [sparse, dense]\n",
    "\n",
    "    ## HuggingFace\n",
    "    hf_model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "    model_kwargs = {\"device\": \"cuda:0\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": False,\n",
    "                     \"clean_up_tokenization_spaces\": True}\n",
    "    \n",
    "    ## Upstage\n",
    "    upstage_model_name = \"solar-embedding-1-large-passage\"\n",
    "    faiss_index_file = \"./index_files/upstage-faiss.npy\"\n",
    "    \n",
    "    ## OpenAI\n",
    "    openai_model_name = \"text-embedding-3-large\"\n",
    "\n",
    "    ## chunking\n",
    "    chunking = True\n",
    "    chunk_method = \"recursive\" ## recursive, semantic\n",
    "    semantic_chunk_method = \"upstage\"\n",
    "    chunk_size = 320\n",
    "    chunk_overlap = 80\n",
    "\n",
    "    ## query expension\n",
    "    query_expansion = False\n",
    "\n",
    "    ## query ensemble\n",
    "    query_ensemble = False  # 쿼리 앙상블 수행 여부\n",
    "    # 앙상블에 사용할 모델\n",
    "    ensemble_models = [\n",
    "        # {'type': 'hf', 'name': \"\"},\n",
    "        # {'type': 'hf', 'name': \"nlpai-lab/KoE5\"},\n",
    "        # {'type': 'hf', 'name': \"BAAI/bge-large-en-v1.5\",\n",
    "        # {'type': 'hf', 'name': \"intfloat/multilingual-e5-large\"},\n",
    "        # {'type': 'upstage', 'name': \"solar-embedding-1-large-query\"},\n",
    "        # {'type': 'hf', 'name': \"sentence-transformers/all-MiniLM-L6-v2\"},\n",
    "    ]\n",
    "    ensemble_weights = [1]  # 각각의 모델 가중치 설정\n",
    "\n",
    "    ## reranker\n",
    "    rerank = False\n",
    "    reranker_name = \"BAAI/bge-reranker-v2-m3\"  ## \"BAAI/bge-reranker-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ollama_encoder(model_name):\n",
    "    encoder = OllamaEmbeddings(model_name)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def load_upstage_encoder(model_name):\n",
    "    encoder = UpstageEmbeddings(model=model_name)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def load_openai_encoder(model_name):\n",
    "    encoder = OpenAIEmbeddings(model=model_name)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def load_hf_encoder(model_name, model_kwargs, encode_kwargs):\n",
    "    encoder = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                    model_kwargs=model_kwargs,\n",
    "                                    encode_kwargs=encode_kwargs)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def load_hf_reranker(model_name, retriever):\n",
    "    reranker = HuggingFaceCrossEncoder(model_name=model_name)\n",
    "    compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever.as_retriever(search_kwargs={\"k\": 10}))\n",
    "\n",
    "    return compression_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "def load_document(path='../dataset/documents.jsonl'):\n",
    "    raw_documents = load_jsonl(path)\n",
    "\n",
    "    documents = []\n",
    "    for doc in raw_documents:\n",
    "        doc_id = doc['docid']\n",
    "        content = doc['content']\n",
    "        documents.append(Document(page_content=content, metadata={\"docid\": doc_id}))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def chunking(args, documents):\n",
    "    if args.chunk_method == \"recursive\":\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=args.chunk_size,\n",
    "            chunk_overlap=args.chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "    elif args.chunk_method == \"semantic\":\n",
    "        if args.semantic_chunk_method == \"huggingface\":\n",
    "            encoder = load_hf_encoder(args.hf_model_name, args.model_kwargs, args.encode_kwargs)\n",
    "        elif args.semantic_chunk_method == \"upstage\":\n",
    "            encoder = load_upstage_encoder(args.upstage_model_name)\n",
    "        elif args.semantic_chunk_method == \"openai\":\n",
    "            encoder = load_openai_encoder(args.openai_model_name)\n",
    "\n",
    "        text_splitter = SemanticChunker(encoder)\n",
    "\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "\n",
    "documents = load_document(path=args.doc_file_path)\n",
    "print(len(documents))\n",
    "\n",
    "documents = chunking(args, documents)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_contextual_retrieval(model):\n",
    "    prompt_context3 = (\n",
    "    \"입력된 문서 조각을 읽고 핵심을 파악해서 문서조각으로부터 '제목', '요약', '본문', '인사이트', '관련 질문들'을 만들어내고 위키피디아 형식으로 작성해주세요.\\n\"\n",
    "    \"단, '*'나 '#'등의 특수 기호는 사용하면 안됩니다.\"\n",
    "\n",
    "    \"입력 : {document_chunk}\\n\"\n",
    "    \"출력 :\"\n",
    "\n",
    "    )\n",
    "\n",
    "    prompt3 = ChatPromptTemplate.from_template(prompt_context3)\n",
    "    chain3 = prompt3 | model | StrOutputParser()\n",
    "\n",
    "    return chain3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.2-ko\", temperature=0.1)\n",
    "chain3 = ollama_contextual_retrieval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/contextual_retrieval_documents.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # for document in tqdm(documents):\n",
    "    for document in documents:\n",
    "        print(document.metadata['docid'])\n",
    "        print(document.page_content)\n",
    "\n",
    "        result = chain3.invoke({\"document_chunk\" : document.page_content})\n",
    "        print(f\"{result}\\n\")\n",
    "\n",
    "        result_with_id = {\n",
    "            \"docid\": document.metadata['docid'],\n",
    "            \"content\": result\n",
    "        }\n",
    "        f.write(json.dumps(result_with_id, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
