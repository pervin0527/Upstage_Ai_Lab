{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/pervinco/Upstage_Ai_Lab/Final/IR/src\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import faiss\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import huggingface_hub\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from itertools import product\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "from config import Args\n",
    "from data.data import load_document, load_query, chunking\n",
    "from dense_retriever.model import load_dense_model\n",
    "from sparse_retriever.model import load_sparse_model\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "os.environ['UPSTAGE_API_KEY'] = upstage_api_key\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "num_questions = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_document_path = \"../dataset/en_4.0_processed_documents_queries.jsonl\"\n",
    "ko_document_path = \"../dataset/processed_documents_queries.jsonl\"\n",
    "\n",
    "en_documents = load_document(en_document_path)\n",
    "en_questions = load_query(en_document_path)\n",
    "\n",
    "ko_documents = load_document(ko_document_path)\n",
    "ko_questions = load_query(ko_document_path)\n",
    "\n",
    "random.shuffle(en_questions)\n",
    "en_questions = en_questions[:num_questions]\n",
    "\n",
    "random.shuffle(ko_questions)\n",
    "ko_questions = ko_questions[:num_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_map(gt, pred):    \n",
    "    sum_average_precision = 0    \n",
    "    for j in pred:        \n",
    "        if gt[j[\"eval_id\"]]:            \n",
    "            hit_count = 0            \n",
    "            sum_precision = 0            \n",
    "            for i,docid in enumerate(j[\"topk\"][:3]):                \n",
    "                if docid in gt[j[\"eval_id\"]]:                    \n",
    "                    hit_count += 1                    \n",
    "                    sum_precision += hit_count/(i+1)            \n",
    "            average_precision = sum_precision / hit_count if hit_count > 0 else 0        \n",
    "        else:            \n",
    "            average_precision = 0 if j[\"topk\"] else 1        \n",
    "        sum_average_precision += average_precision    \n",
    "    return sum_average_precision/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 값의 범위를 설정합니다.\n",
    "chunk_sizes = [50, 100, 200, 300, 400]\n",
    "chunk_overlaps = [0, 25, 50, 100]\n",
    "retriever_weights_list = [[0.5, 0.5], [0.6, 0.4], [0.4, 0.6], [0.7, 0.3], [0.3, 0.7]]\n",
    "\n",
    "# 최적의 MAP 값을 추적하기 위한 변수 초기화\n",
    "best_map = 0\n",
    "best_params = {}\n",
    "\n",
    "# 모든 파라미터 조합에 대해 실험을 수행합니다.\n",
    "for chunk_size, chunk_overlap, retriever_weights in product(chunk_sizes, chunk_overlaps, retriever_weights_list):\n",
    "    args.chunk_size = chunk_size\n",
    "    args.chunk_overlap = chunk_overlap\n",
    "    args.retriever_weights = retriever_weights\n",
    "    \n",
    "    # 문서 조각화\n",
    "    chunk_documents = chunking(args, ko_documents)\n",
    "    \n",
    "    # Sparse 및 Dense 리트리버 로드\n",
    "    sparse_retriever = load_sparse_model(ko_documents)\n",
    "    sparse_retriever.k = 5\n",
    "    \n",
    "    dense_retriever = load_dense_model(args, chunk_documents).as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    # 앙상블 리트리버 설정\n",
    "    retriever = EnsembleRetriever(\n",
    "        retrievers=[sparse_retriever, dense_retriever],\n",
    "        weights=args.retriever_weights,\n",
    "        search_type=\"similarity_score_threshold\"  # 또는 \"mmr\"\n",
    "    )\n",
    "    \n",
    "    # 정답 레이블 생성\n",
    "    gt = {}\n",
    "    for question in ko_questions:\n",
    "        query, question_id = question['query'], question['metadata']['docid']\n",
    "        gt[question_id] = [question_id]\n",
    "    \n",
    "    # 예측 결과 수집\n",
    "    pred = []\n",
    "    for question in tqdm(ko_questions):\n",
    "        query, question_id = question['query'], question['metadata']['docid']\n",
    "        \n",
    "        search_result = retriever.invoke(query)\n",
    "        \n",
    "        topk_result = []\n",
    "        for result in search_result:\n",
    "            topk_result.append(result.metadata.get('docid'))\n",
    "        \n",
    "        pred.append({\n",
    "            \"eval_id\": question_id,\n",
    "            \"topk\": topk_result\n",
    "        })\n",
    "    \n",
    "    # MAP 계산\n",
    "    mean_average_precision = calc_map(gt, pred)\n",
    "    print(\"Parameters:\")\n",
    "    print(f\"  -chunk_size={chunk_size}\")\n",
    "    print(f\"  -chunk_overlap={chunk_overlap}\")\n",
    "    print(f\"  -retriever_weights={retriever_weights}\\n\")\n",
    "    print(f\"Mean Average Precision (MAP): {mean_average_precision}\\n\")\n",
    "    \n",
    "    # 최적의 MAP 값과 파라미터 저장\n",
    "    if mean_average_precision > best_map:\n",
    "        best_map = mean_average_precision\n",
    "        best_params = {\n",
    "            'chunk_size': chunk_size,\n",
    "            'chunk_overlap': chunk_overlap,\n",
    "            'retriever_weights': retriever_weights\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Mean Average Precision (MAP): {best_map}\")\n",
    "print(f\"Best Parameters:\")\n",
    "print(f\"  chunk_size: {best_params['chunk_size']}\")\n",
    "print(f\"  chunk_overlap: {best_params['chunk_overlap']}\")\n",
    "print(f\"  retriever_weights: {best_params['retriever_weights']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
