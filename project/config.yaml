general:
  data_path: "./dataset" # 학습에 사용할 데이터셋 경로
  train_file: "cleaned_train.csv"
  valid_file: "cleaned_dev.csv"
  model_name: "digit82/kobart-summarization" # 불러올 모델의 이름을 사용자 환경에 맞게 지정할 수 있습니다.
  output_dir: "./runs" # 모델의 최종 출력 값을 저장할 경로를 설정합니다.

tokenizer:
  path: "./tokenizer"
  encoder_max_len: 1000 ## 512
  decoder_max_len: 200 ## 100
  bos_token: "[BOS]"  # "<s>"
  eos_token: "[EOS]"  # "</s>"
  sep_token: "[SEP]"
  
  # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정해줍니다.
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#Person4#'
    - '#Person5#'
    - '#Person6#'
    - '#Person7#'
    - '#SSN#'
    - '#Email#'
    - '#Address#'
    - '#Reaction#'
    - '#CarNumber#'
    - '#Movietitle#'
    - '#DateOfBirth#'
    - '#CardNumber#'
    - '#PhoneNumber#'
    - '#PassportNumber#'

training:
  seed: 42
  overwrite_output_dir: true

  num_train_epochs: 20
  learning_rate: 0.00001
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 4
  optim: 'adamw_torch'
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: 'cosine'
  gradient_accumulation_steps: 4
  evaluation_strategy: 'epoch'
  save_strategy: 'epoch'
  save_total_limit: 1000
  fp16: false
  load_best_model_at_end: true
  logging_dir: "./logs"
  logging_strategy: "epoch"
  predict_with_generate: true
  generation_max_length: 200
  do_train: true
  do_eval: true
  early_stopping_patience: 10
  early_stopping_threshold: 0.001
  report_to: tensorboard ## "wandb" # (선택) wandb를 사용할 때 설정합니다.

# wandb:
#   entity: "Upstage-Ai-Lab"
#   project: "upstage-nlp-competition"
#   name: "run"

inference:
  ckt_path: "/home/pervinco/Upstage_Ai_Lab/project/runs/2024-08-31-14-08-57/checkpoint-3880"
  result_path: "./prediction"
  no_repeat_ngram_size: 2
  early_stopping: true
  generate_max_length: 200
  num_beams: 4
  batch_size: 32

  # 정확한 모델 평가를 위해 제거할 불필요한 생성 토큰들을 정의합니다.
  remove_tokens:
    - "[PAD]"
    - "[UNK]"
    - "[BOS]"
    - "[EOS]"
    - "[MASK]"
    - "[SEP]"
