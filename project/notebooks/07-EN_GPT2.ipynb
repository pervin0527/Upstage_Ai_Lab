{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summ_dataset(Dataset):\n",
    "    \"\"\"dataframeì„ torch dataset classë¡œ ë³€í™˜\"\"\"\n",
    "    def __init__(self, document, tokenizer):\n",
    "      self.dataset = document\n",
    "      self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        input_ids=torch.LongTensor(self.dataset[\"input_ids\"][idx])\n",
    "        labels=torch.LongTensor(self.dataset[\"labels\"][idx])\n",
    "\n",
    "        attention_mask=input_ids.ne(self.tokenizer.pad_token_id) ## padding tokenì€ attention ê³„ì‚°ì— ë°˜ì˜ë˜ë©´ ì•ˆë˜ë‹ˆê¹Œ maskë¥¼ ì •ì˜í•œë‹¤..\n",
    "\n",
    "        return dict(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    \"\"\"csv fileì„ dataframeìœ¼ë¡œ load\"\"\"\n",
    "    dataset = pd.read_csv(dataset_dir)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, doc_tokenizer, sum_tokenizer, doc_max_length, sum_max_len, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    í† í¬ë‚˜ì´ì§•ì„ ìœ„í•œ í•¨ìˆ˜. trainingê³¼ inference ë‹¨ê³„ì—ì„œì˜ í† í¬ë‚˜ì´ì§•ì´ ë³„ë„ë¡œ êµ¬ì¶•ë˜ì–´ ìˆë‹¤.\n",
    "    - í•™ìŠµì¼ ë•ŒëŠ” ë³¸ë¬¸ê³¼ ìš”ì•½ì´ í•¨ê»˜ ì…ë ¥ëœë‹¤. --> ë³¸ë¬¸ [SEP] ìš”ì•½\n",
    "    - ë°˜ë©´ ì¶”ë¡  ë‹¨ê³„ì—ì„œëŠ” ë³¸ë¬¸ë§Œ ì…ë ¥ë˜ì–´ ìš”ì•½ì„ ìƒì„±í•´ì•¼í•¨.\n",
    "    \"\"\"\n",
    "    ## ì¶”ë¡  ë‹¨ê³„\n",
    "    if mode == \"infer\":\n",
    "      ## inference ì‹œì—ëŠ” document ë§Œ ì£¼ì–´ì§€ê³ , ë§ˆì§€ë§‰ì— bos_tokenì„ ë¶™ì—¬ ìƒì„± ì‹œì‘í•˜ê²Œ í•œë‹¤.\n",
    "      document_text = dataset['dialogue']\n",
    "      summ_text = dataset['summary']\n",
    "\n",
    "      ## document + bos\n",
    "      ## <pad> <pad> d_1 d_2 d_3 ... d_n <bos>\n",
    "      document = [doc_tokenizer(documents, padding = 'max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for documents in document_text.values]\n",
    "      # labelsì—ëŠ” ìš”ì•½ë¬¸ë§Œí¼ì˜ ë¹ˆì¹¸ìœ¼ë¡œ ì±„ì›Œì¤€ í›„ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ë„ë¡ í•¨\n",
    "      labels = [[-100] * sum_max_len for _ in document]\n",
    "      out = {\"input_ids\": document, \"labels\": labels}\n",
    "\n",
    "    elif mode == \"train\":\n",
    "      document_text = dataset['dialogue']\n",
    "      summary_text = dataset['summary']\n",
    "      ## document ì™€ summaryë¥¼ ì´ì–´ ë¶™ì—¬ì„œ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©. \n",
    "      ## document ë’¤ì—ëŠ” bos_token ì„ ë¶™ì—¬ ìƒì„± ì‹œì‘ì„ ëª…ì‹œí•˜ê³ , summary ë¥¼ ë¶™ì¸ í›„ ë§¨ ë’¤ì—ëŠ” eos_token ìœ¼ë¡œ ìƒì„±ì˜ ëì„ ëª…ì‹œ.\n",
    "      ## â­ï¸ documentë¥¼ padding í•  ë•ŒëŠ” sideë¥¼ leftë¡œ ì£¼ê³ , summaryë¥¼ padding í•  ë•ŒëŠ” sideë¥¼ rightë¡œ ì¤˜ì„œ ì—°ì†ëœ ë¬¸ì¥ì´ ìƒì„±ë  ìˆ˜ ìˆë„ë¡ í•œë‹¤.\n",
    "      ## â­ï¸ <pad> <pad> d_1 d_2 d_3 ... d_n <bos> s_1 s_2 ... s_m <eos> <pad> <pad>\n",
    "      document = [doc_tokenizer(documents, padding='max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for documents in document_text.values]\n",
    "      summary = [sum_tokenizer(summaries + sum_tokenizer.eos_token, padding = 'max_length',truncation=True, max_length=sum_max_len, add_special_tokens=True)['input_ids'] for summaries in summary_text.values]\n",
    "\n",
    "      ## êµ¬ì„±í•´ë‘” document ì™€ summaryë¥¼ ê²°í•©í•˜ì—¬ input ì¤€ë¹„\n",
    "      tokenized_senetences = [document + summary for (document, summary) in zip(document, summary)]\n",
    "      ## documentëŠ” ìƒì„±í•  ë‚´ìš©ì´ ì•„ë‹ˆë¯€ë¡œ -100ìœ¼ë¡œ labelì„ ë¶€ì—¬í•œë‹¤.\n",
    "      # Input : <pad> <pad> d_1  d_2  d_3  ... d_n  <bos> s_1 s_2 ... s_m <eos> <pad> <pad>\n",
    "      # Label : -100  -100    -100 -100 -100  ... -100  -100  s_1 s_2 ... s_m <eos> -100 -100\n",
    "\n",
    "      labels = [[-100] * len(document) + summary for (document, summary) in zip(document, summary)]\n",
    "      ## â­ï¸ Q. ë‹¤ìŒì— ì˜¬ Tokenì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•´ì•¼ ë˜ë‹ˆê¹Œ s_1ì˜ labelì€ í•œ ì¹¸ì”© ë°€ë¦° s_2ê°€ ë“¤ì–´ê°€ì•¼ ë˜ì§€ ì•Šë‚˜ìš”?\n",
    "      # A. Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ GPT êµ¬í˜„(https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1103-L1104)ì„ ë³´ë©´, \n",
    "      # ëª¨ë¸ì˜ Logitì„ [: -1]ë§Œ ê°€ì ¸ì˜¤ê³ , Labelì€ [1: ]ì„ ê°€ì ¸ì™€ì„œ Lossë¥¼ ê³„ì‚°í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "      # ì¦‰, Inputê³¼ Labelì´ í•œ ì¹¸ì”© ë°€ë¦°ì±„ë¡œ ì…ë ¥ì„ ë„£ì§€ ì•Šì•„ë„, ë‚´ë¶€ êµ¬í˜„ì— ì˜í•´ ìë™ìœ¼ë¡œ ë°€ë¦° ì±„ë¡œ ê³„ì‚°ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "      # padding ëœ ë¶€ë¶„ì´ í•™ìŠµë˜ì§€ ì•Šë„ë¡ -100 ìœ¼ë¡œ ì¹˜í™˜\n",
    "      labels = [[-100 if token == sum_tokenizer.pad_token_id else token for token in l] for l in labels]\n",
    "      out = {\"input_ids\": tokenized_senetences, \"labels\": labels}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_dir, doc_tokenizer,sum_tokenizer,doc_max_len, sum_max_len):\n",
    "    \"\"\"í•™ìŠµ(train)ê³¼ í‰ê°€(test)ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ì„ ì¤€ë¹„\"\"\"\n",
    "    # load_data\n",
    "    train_dataset = load_data(os.path.join(dataset_dir, \"train_translated.csv\"))\n",
    "    val_dataset = load_data(os.path.join(dataset_dir, \"dev_translated.csv\"))\n",
    "\n",
    "    tokenized_train = tokenized_dataset(train_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len)\n",
    "    tokenized_val = tokenized_dataset(val_dataset, doc_tokenizer,sum_tokenizer, doc_max_len, sum_max_len)\n",
    "\n",
    "    summ_train_dataset = summ_dataset(tokenized_train, doc_tokenizer)\n",
    "    summ_val_dataset = summ_dataset(tokenized_val, doc_tokenizer)\n",
    "\n",
    "    return summ_train_dataset , summ_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(args, pred):\n",
    "    MODEL_NAME = args.model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    if isinstance(preds, tuple):\n",
    "      preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds[:, args.doc_max_len:], skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels[:, args.doc_max_len:], skip_special_tokens=True)\n",
    "\n",
    "    metric = datasets.load_metric(\"rouge\")\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return {\n",
    "        'Rouge-2' : result['rouge2']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model_for_train(args):\n",
    "    MODEL_NAME = args.model_name\n",
    "    doc_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    sum_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, config=model_config)\n",
    "    \n",
    "    return doc_tokenizer, sum_tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer_for_train(args, model, summ_train_dataset, summ_val_dataset):\n",
    "    \"\"\"í•™ìŠµ(train)ì„ ìœ„í•œ huggingface trainer ì„¤ì •\"\"\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        output_dir=args.save_path + \"results\",  # output directory\n",
    "        save_total_limit=args.save_limit,  # number of total save model.\n",
    "        save_steps=args.save_step,  # model saving step.\n",
    "        num_train_epochs=args.epochs,  # total number of training epochs\n",
    "        learning_rate=args.lr,  # learning_rate\n",
    "        per_device_train_batch_size=args.batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "        warmup_steps=args.warmup_steps,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=args.weight_decay,  # strength of weight decay\n",
    "        logging_dir=args.save_path + \"logs\",  # directory for storing logs\n",
    "        logging_steps=args.logging_steps,  # log saving step.\n",
    "        evaluation_strategy=\"steps\",  # evaluation strategy to adopt during training\n",
    "            # `no`: No evaluation during training.\n",
    "            # `steps`: Evaluate every `eval_steps`.\n",
    "            # `epoch`: Evaluate every end of epoch.\n",
    "        eval_steps=args.eval_steps,  # evaluation step.\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    ## Add callback & optimizer & scheduler\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5, early_stopping_threshold=0.001\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=args.weight_decay,\n",
    "        amsgrad=False,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated ğŸ¤— Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=summ_train_dataset,  # training dataset\n",
    "        eval_dataset=summ_val_dataset,  # evaluation dataset\n",
    "        compute_metrics=lambda p: compute_metrics(args, p),\n",
    "        callbacks=[MyCallback],\n",
    "        optimizers=(\n",
    "            optimizer,\n",
    "            get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=args.warmup_steps,\n",
    "                    num_training_steps=len(summ_train_dataset) * args.epochs,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    pl.seed_everything(seed=42, workers=False)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    doc_tokenizer, sum_tokenizer , model = load_tokenizer_and_model_for_train(args)\n",
    "    model.to(device)\n",
    "\n",
    "    summ_train_dataset, summ_val_dataset = prepare_dataset(args.dataset_dir,doc_tokenizer, sum_tokenizer,args.doc_max_len,args.sum_max_len)\n",
    "    trainer = load_trainer_for_train(args, model, summ_train_dataset, summ_val_dataset)\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"./best_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    \"\"\"í•™ìŠµ(train)ê³¼ ì¶”ë¡ (infer)ì— ì‚¬ìš©ë˜ëŠ” arguments ê´€ë¦¬í•˜ëŠ” class\"\"\"\n",
    "    dataset_dir = \"../dataset\"\n",
    "    model_type = \"gpt2\"\n",
    "    model_name = 'MrBananaHuman/kogpt2_small'\n",
    "    save_path = \"./GPT2\"\n",
    "    save_step = 400\n",
    "    logging_steps = 200\n",
    "    eval_steps = 200\n",
    "    save_limit = 5\n",
    "    seed = 42\n",
    "    epochs = 20 # 10\n",
    "    batch_size = 2\n",
    "    doc_max_len = 512\n",
    "    sum_max_len = 128\n",
    "    lr = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 5\n",
    "    scheduler = \"linear\"\n",
    "    model_dir = \"./best_model\"\n",
    "    \n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
