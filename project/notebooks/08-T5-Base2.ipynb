{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "from datetime import datetime\n",
    "\n",
    "from torch import nn\n",
    "from konlpy.tag import Mecab\n",
    "from tensorboardX import SummaryWriter\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "## \"psyche/KoT5-summarization\", \"eenzeenee/t5-base-korean-summarization\"\n",
    "model_id = \"philschmid/bart-large-cnn-samsum\"\n",
    "train_df = pd.read_csv('../dataset/en_train.csv')\n",
    "val_df = pd.read_csv('../dataset/en_dev.csv')\n",
    "test_df = pd.read_csv(\"../dataset/en_test.csv\")\n",
    "\n",
    "# new_df = pd.read_csv(\"../dataset/new_data.csv\").sample(10000)\n",
    "# train_df = pd.concat([train_df, new_df])\n",
    "# print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 4\n",
    "accumulation_steps = 4\n",
    "num_workers = 0\n",
    "patience = 10\n",
    "\n",
    "init_lr = 0.000001\n",
    "max_lr = 0.00001\n",
    "weight_decay = 0\n",
    "warmup_epochs = 10\n",
    "T_0 = 100\n",
    "T_mult = 1\n",
    "T_gamma = 0.5\n",
    "\n",
    "dig_max_len = 142\n",
    "sum_max_len = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"_name_or_path\": \"philschmid/bart-large-cnn-samsum\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "tokenizer = BartTokenizerFast.from_pretrained(\"../tokenizer\")\n",
    "model = BartForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#SSN#', '#Email#', '#Address#', '#Reaction#', '#CarNumber#', '#Movietitle#', '#DateOfBirth#', '#CardNumber#', '#PhoneNumber#', '#PassportNumber#', '<sep>']}\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\n",
    "    '#Person1#',\n",
    "    '#Person2#',\n",
    "    '#Person3#',\n",
    "    '#Person4#',\n",
    "    '#Person5#',\n",
    "    '#Person6#',\n",
    "    '#Person7#',\n",
    "    '#SSN#',\n",
    "    '#Email#',\n",
    "    '#Address#',\n",
    "    '#Reaction#',\n",
    "    '#CarNumber#',\n",
    "    '#Movietitle#',\n",
    "    '#DateOfBirth#',\n",
    "    '#CardNumber#',\n",
    "    '#PhoneNumber#',\n",
    "    '#PassportNumber#',\n",
    "    '<sep>'\n",
    "]\n",
    "\n",
    "remove_tokens = ['<usr>', f\"{tokenizer.unk_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "\n",
    "# extra_tokens = [f\"<extra_id_{i}>\" for i in range(500)]\n",
    "# remove_tokens = ['<usr>', f\"{tokenizer.unk_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"] + extra_tokens\n",
    "# tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens + extra_tokens})\n",
    "\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, input_len, summ_len, is_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df.copy()\n",
    "        self.source_len = input_len\n",
    "        self.summ_len = summ_len\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # 화자가 바뀔 때 SEP 토큰을 추가\n",
    "        self.df.loc[:, 'dialogue'] = self.df['dialogue'].apply(self.add_sep_tokens)\n",
    "\n",
    "        if self.is_train:\n",
    "            # self.df['dialogue'] = self.df['dialogue'].apply(self.apply_augmentations)\n",
    "            \n",
    "            self.input_ids = tokenizer(self.df['dialogue'].tolist(), \n",
    "                                       return_tensors=\"pt\", \n",
    "                                       padding=True,\n",
    "                                       add_special_tokens=True, \n",
    "                                       truncation=True, \n",
    "                                       max_length=dig_max_len, \n",
    "                                       return_token_type_ids=False).input_ids\n",
    "            \n",
    "            self.labels = tokenizer(self.df['summary'].tolist(), \n",
    "                                    return_tensors=\"pt\", \n",
    "                                    padding=True,\n",
    "                                    add_special_tokens=True, \n",
    "                                    truncation=True, \n",
    "                                    max_length=sum_max_len, \n",
    "                                    return_token_type_ids=False).input_ids\n",
    "        else:\n",
    "            self.input_ids = tokenizer(self.df['dialogue'].tolist(), \n",
    "                                       return_tensors=\"pt\", \n",
    "                                       padding=True,\n",
    "                                       add_special_tokens=True, \n",
    "                                       truncation=True, \n",
    "                                       max_length=dig_max_len, \n",
    "                                       return_token_type_ids=False).input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.input_ids[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.input_ids[idx]\n",
    "\n",
    "\n",
    "    def add_sep_tokens(self, dialogue):\n",
    "        # 화자가 바뀔 때 SEP 토큰을 추가\n",
    "        pattern = r'(#Person\\d+#)'  # 화자를 나타내는 패턴\n",
    "        parts = re.split(pattern, dialogue)  # 화자를 기준으로 대화 분리\n",
    "        result = []\n",
    "        prev_speaker = None\n",
    "        for part in parts:\n",
    "            if re.match(pattern, part):  # 화자가 바뀌면\n",
    "                if prev_speaker and prev_speaker != part:\n",
    "                    result.append('<sep>')  # SEP 토큰 추가\n",
    "                prev_speaker = part\n",
    "            result.append(part)\n",
    "        return ''.join(result)\n",
    "\n",
    "    ###\n",
    "    def apply_augmentations(self, text):\n",
    "        # 세 가지 변환 적용: text infilling, sentence permutation, token masking\n",
    "        text = self.text_infilling(text)\n",
    "        # text = self.sentence_permutation(text)\n",
    "        text = self.token_masking(text)\n",
    "        return text\n",
    "\n",
    "    def text_infilling(self, text, fill_prob=0.15):\n",
    "        # 15% 확률로 텍스트 일부를 빈칸 처리 (T5 스타일로 <extra_id_0>, <extra_id_1> 사용)\n",
    "        words = text.split()\n",
    "        num_to_fill = int(len(words) * fill_prob)\n",
    "        fill_indices = random.sample(range(len(words)), num_to_fill)\n",
    "        \n",
    "        # <extra_id_0>, <extra_id_1> 등의 토큰을 사용해 infilling 처리\n",
    "        extra_id = 0\n",
    "        for idx in fill_indices:\n",
    "            words[idx] = f'<extra_id_{extra_id}>'\n",
    "            extra_id += 1  # 각 빈칸에 다른 extra_id 부여\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def sentence_permutation(self, text):\n",
    "        # 문장의 순서를 랜덤으로 섞기\n",
    "        sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "        random.shuffle(sentences)\n",
    "        return ' '.join(sentences)\n",
    "\n",
    "    def token_masking(self, text, mask_prob=0.15):\n",
    "        # 15% 확률로 토큰을 마스킹 (T5 스타일로 <extra_id_*> 사용)\n",
    "        tokens = text.split()\n",
    "        num_to_mask = int(len(tokens) * mask_prob)\n",
    "        mask_indices = random.sample(range(len(tokens)), num_to_mask)\n",
    "        \n",
    "        # <extra_id_*> 형식으로 마스킹 처리\n",
    "        extra_id = 0\n",
    "        for idx in mask_indices:\n",
    "            tokens[idx] = f'<extra_id_{extra_id}>'\n",
    "            extra_id += 1\n",
    "        \n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df[['dialogue', 'summary']], tokenizer, dig_max_len, sum_max_len)\n",
    "val_dataset = CustomDataset(val_df[['dialogue', 'summary']], tokenizer, dig_max_len, sum_max_len)\n",
    "test_dataset = CustomDataset(test_df[['dialogue']], tokenizer, dig_max_len, sum_max_len, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [23, 247, 32, 219, 31, 607, 31, 33, 34, 52, 1823, 1383, 658, 697, 416, 38, 31, 174, 51, 35, 107, 193, 37, 30, 24, 33, 573, 41, 106, 54, 40, 96, 245, 39, 81, 40, 280, 86, 793, 31, 30, 23, 69, 32, 159, 32, 35, 321, 34, 43, 154, 90, 47, 465, 270, 31, 63, 128, 49, 90, 300, 236, 31, 30, 24, 33, 75, 31, 33, 1452, 126, 177, 126, 79, 44, 538, 432, 32, 272, 83, 92, 36, 496, 37, 30, 23, 82, 32, 36, 279, 165, 39, 1839, 725, 5577, 472, 44, 39, 196, 120, 67, 133, 556, 31, 139, 239, 39, 157, 70, 579, 565, 40, 236, 47, 50, 456, 96, 31, 30, 24, 366, 31, 30, 23, 166, 57, 92, 107, 31, 403, 1018, 42, 2787, 150, 314, 31, 792, 40, 1934, 2561, 32, 136, 31, 121, 35, 1382, 32, 219, 31, 607, 37, 30, 24, 69, 31, 30, 23, 5653, 44, 36, 30, 3435, 1213, 45, 6025, 3902, 42, 1200, 4302, 32, 35, 75, 31, 63, 105, 128, 1555, 31, 30, 24, 33, 34, 103, 763, 3272, 45, 724, 32, 71, 33, 88, 53, 34, 43, 572, 39, 2241, 36, 2436, 31, 30, 23, 82, 32, 61, 49, 779, 42, 93, 4253, 38, 48, 311, 135, 31, 33, 34, 68, 188, 35, 131, 463, 205, 35, 338, 31, 30, 24, 366, 32, 437, 496, 31], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [219, 31, 607, 34, 38, 368, 40, 280, 86, 793, 32, 42, 1823, 1383, 658, 697, 416, 38, 2800, 38, 185, 39, 49, 90, 300, 236, 31, 1383, 658, 697, 416, 38, 34, 68, 188, 93, 463, 67, 240, 779, 42, 4253, 38, 39, 135, 219, 31, 607, 1555, 1325, 31], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['#Person1#:', '▁Hi', ',', '▁Mr', '.', '▁Smith', '.', '▁I', \"'\", 'm', '▁Doctor', '▁Ha', 'w', 'k', 'in', 's', '.', '▁Why', '▁are', '▁you', '▁here', '▁today', '?', '▁', '#Person2#:', '▁I', '▁found', '▁it', '▁would', '▁be', '▁a', '▁good', '▁idea', '▁to', '▁get', '▁a', '▁check', '-', 'up', '.', '▁', '#Person1#:', '▁Yes', ',', '▁well', ',', '▁you', '▁haven', \"'\", 't', '▁had', '▁one', '▁for', '▁5', '▁years', '.', '▁You', '▁should', '▁have', '▁one', '▁every', '▁year', '.', '▁', '#Person2#:', '▁I', '▁know', '.', '▁I', '▁figure', '▁as', '▁long', '▁as', '▁there', '▁is', '▁nothing', '▁wrong', ',', '▁why', '▁go', '▁see', '▁the', '▁doctor', '?', '▁', '#Person1#:', '▁Well', ',', '▁the', '▁best', '▁way', '▁to', '▁avoid', '▁serious', '▁illness', 'es', '▁is', '▁to', '▁find', '▁out', '▁about', '▁them', '▁early', '.', '▁So', '▁try', '▁to', '▁come', '▁at', '▁least', '▁once', '▁a', '▁year', '▁for', '▁your', '▁own', '▁good', '.', '▁', '#Person2#:', '▁Ok', '.', '▁', '#Person1#:', '▁Let', '▁me', '▁see', '▁here', '.', '▁Your', '▁eyes', '▁and', '▁ears', '▁look', '▁fine', '.', '▁Take', '▁a', '▁deep', '▁breath', ',', '▁please', '.', '▁Do', '▁you', '▁smoke', ',', '▁Mr', '.', '▁Smith', '?', '▁', '#Person2#:', '▁Yes', '.', '▁', '#Person1#:', '▁Smoking', '▁is', '▁the', '▁', 'leading', '▁cause', '▁of', '▁lung', '▁cancer', '▁and', '▁heart', '▁disease', ',', '▁you', '▁know', '.', '▁You', '▁really', '▁should', '▁quit', '.', '▁', '#Person2#:', '▁I', \"'\", 've', '▁tried', '▁hundreds', '▁of', '▁times', ',', '▁but', '▁I', '▁just', '▁can', \"'\", 't', '▁seem', '▁to', '▁kick', '▁the', '▁habit', '.', '▁', '#Person1#:', '▁Well', ',', '▁we', '▁have', '▁classes', '▁and', '▁some', '▁medication', 's', '▁that', '▁might', '▁help', '.', '▁I', \"'\", 'll', '▁give', '▁you', '▁more', '▁information', '▁before', '▁you', '▁leave', '.', '▁', '#Person2#:', '▁Ok', ',', '▁thanks', '▁doctor', '.']\n",
      "['▁Mr', '.', '▁Smith', \"'\", 's', '▁getting', '▁a', '▁check', '-', 'up', ',', '▁and', '▁Doctor', '▁Ha', 'w', 'k', 'in', 's', '▁advise', 's', '▁him', '▁to', '▁have', '▁one', '▁every', '▁year', '.', '▁Ha', 'w', 'k', 'in', 's', \"'\", 'll', '▁give', '▁some', '▁information', '▁about', '▁their', '▁classes', '▁and', '▁medication', 's', '▁to', '▁help', '▁Mr', '.', '▁Smith', '▁quit', '▁smoking', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_dig = train_df.iloc[0]['dialogue']\n",
    "sample_sum = train_df.iloc[0]['summary']\n",
    "\n",
    "dig_tk = tokenizer(sample_dig)\n",
    "sum_tk = tokenizer(sample_sum)\n",
    "\n",
    "print(dig_tk)\n",
    "print(sum_tk)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(dig_tk.input_ids))\n",
    "print(tokenizer.convert_ids_to_tokens(sum_tk.input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142])\n",
      "['#Person1#:', '▁Hi', ',', '▁Mr', '.', '▁Smith', '.', '▁I', \"'\", 'm', '▁Doctor', '▁Ha', 'w', 'k', 'in', 's', '.', '▁Why', '▁are', '▁you', '▁here', '▁today', '?', '▁', '<sep>', '#Person2#:', '▁I', '▁found', '▁it', '▁would', '▁be', '▁a', '▁good', '▁idea', '▁to', '▁get', '▁a', '▁check', '-', 'up', '.', '▁', '<sep>', '#Person1#:', '▁Yes', ',', '▁well', ',', '▁you', '▁haven', \"'\", 't', '▁had', '▁one', '▁for', '▁5', '▁years', '.', '▁You', '▁should', '▁have', '▁one', '▁every', '▁year', '.', '▁', '<sep>', '#Person2#:', '▁I', '▁know', '.', '▁I', '▁figure', '▁as', '▁long', '▁as', '▁there', '▁is', '▁nothing', '▁wrong', ',', '▁why', '▁go', '▁see', '▁the', '▁doctor', '?', '▁', '<sep>', '#Person1#:', '▁Well', ',', '▁the', '▁best', '▁way', '▁to', '▁avoid', '▁serious', '▁illness', 'es', '▁is', '▁to', '▁find', '▁out', '▁about', '▁them', '▁early', '.', '▁So', '▁try', '▁to', '▁come', '▁at', '▁least', '▁once', '▁a', '▁year', '▁for', '▁your', '▁own', '▁good', '.', '▁', '<sep>', '#Person2#:', '▁Ok', '.', '▁', '<sep>', '#Person1#:', '▁Let', '▁me', '▁see', '▁here', '.', '▁Your', '▁eyes', '▁and', '▁ears', '▁look', '▁fine', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_batch = train_dataset[0]\n",
    "print(sample_batch[0].shape)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(sample_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_words(tokenizer, preds, labels):\n",
    "    decoded_preds = tokenizer.batch_decode(preds, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "        \n",
    "    return replaced_predictions, replaced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(replaced_predictions, replaced_labels):\n",
    "    rouge = Rouge()\n",
    "\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(epoch, model, device, train_loader, optimizer, writer):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch}\", leave=False):\n",
    "#         input_ids = batch[0].to(device, dtype=torch.long)\n",
    "#         labels = batch[1].to(device, dtype=torch.long)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids=input_ids, labels=labels)\n",
    "#         ce_loss = outputs.loss\n",
    "\n",
    "#         ce_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += ce_loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "#     writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "#     return avg_loss\n",
    "\n",
    "def train(epoch, model, device, train_loader, optimizer, writer, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch}\", leave=False):\n",
    "        input_ids = batch[0].to(device, dtype=torch.long)\n",
    "        labels = batch[1].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        ce_loss = outputs.loss\n",
    "        ce_loss = ce_loss / accumulation_steps \n",
    "        ce_loss.backward()\n",
    "\n",
    "        if (idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += ce_loss.item() * accumulation_steps\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(tokenizer, model, device, val_loader, writer, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_results = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validating\", leave=False):\n",
    "            input_ids = batch[0].to(device, dtype=torch.long)\n",
    "            labels = batch[1].to(device, dtype=torch.long)\n",
    "\n",
    "            pred_ids = model.generate(input_ids=input_ids, max_length=sum_max_len, num_beams=4, repetition_penalty=2.0, \n",
    "                                      length_penalty=1.0, early_stopping=True, no_repeat_ngram_size=2)\n",
    "\n",
    "            loss = model(input_ids=input_ids, labels=labels).loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            replaced_predictions, replaced_labels = ids_to_words(tokenizer, pred_ids, labels)\n",
    "            result = compute_metrics(replaced_predictions, replaced_labels)\n",
    "\n",
    "            all_results.append(result)\n",
    "            all_predictions.extend(replaced_predictions)\n",
    "            all_labels.extend(replaced_labels)\n",
    "\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    avg_result = {key: sum(r[key] for r in all_results) / len(all_results) for key in all_results[0]}\n",
    "    \n",
    "    writer.add_scalar('Loss/valid', val_loss, epoch)\n",
    "    writer.add_scalar('ROUGE/rouge-1', avg_result['rouge-1'], epoch)\n",
    "    writer.add_scalar('ROUGE/rouge-2', avg_result['rouge-2'], epoch)\n",
    "    writer.add_scalar('ROUGE/rouge-l', avg_result['rouge-l'], epoch)\n",
    "\n",
    "    return val_loss, avg_result, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Current Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  35%|███▌      | 1101/3115 [00:54<01:39, 20.22it/s]"
     ]
    }
   ],
   "source": [
    "train_step = 0\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_path = os.path.join(\"./T5_runs\", timestamp)\n",
    "\n",
    "weights_path = os.path.join(save_path, 'weights')\n",
    "logs_path = os.path.join(save_path, 'logs')\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "best_rouge = 0\n",
    "early_stopping_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "writer = SummaryWriter(log_dir=logs_path)\n",
    "scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_max=max_lr,  T_up=warmup_epochs, gamma=T_gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch}/{epochs}, Current Learning Rate: {current_lr:.6f}\")\n",
    "    train_loss = train(epoch, model, device, train_loader, optimizer, writer, accumulation_steps)\n",
    "    val_loss, val_result, val_predictions, val_labels = validate(tokenizer, model, device, val_loader, writer, epoch)\n",
    "\n",
    "    # avg_rouge = (val_result['rouge-1'] + val_result['rouge-2'] + val_result['rouge-l']) / 3\n",
    "    print(f\"Train Loss: {train_loss:.6f}, Valid Loss: {val_loss:.6f}\")\n",
    "    print(f\"Rouge-1: {val_result['rouge-1']:.6f}, Rouge-2: {val_result['rouge-2']:.6f}, Rouge-l: {val_result['rouge-l']:.6f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print('-'*150)\n",
    "    for i in range(3):\n",
    "        print(f\"PRED: {val_predictions[i].strip()}\")\n",
    "        print(f\"GOLD: {val_labels[i]}\")\n",
    "        print('-'*150)\n",
    "\n",
    "    # if avg_rouge > best_rouge:\n",
    "    if val_result['rouge-2'] > best_rouge:\n",
    "        # best_rouge = avg_rouge\n",
    "        best_rouge = val_result['rouge-2']\n",
    "        early_stopping_counter = 0 \n",
    "        torch.save(model.state_dict(), os.path.join(weights_path, 'best.pth'))\n",
    "        print(f\"New best model saved with average ROUGE: {best_rouge:.6f}\")\n",
    "\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"Not improve. Early stopping counter: {early_stopping_counter}/{patience}\")\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(weights_path, f'epoch-{epoch}.pth'))\n",
    "    print()\n",
    "\n",
    "writer.close()\n",
    "torch.save(model.state_dict(), os.path.join(weights_path, 'last.pth'))\n",
    "print(\"Training completed. Last model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, device, test_loader, fname):\n",
    "    model.eval()\n",
    "    summary = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids in tqdm(test_loader):\n",
    "            input_ids = input_ids.to(device, dtype=torch.long)\n",
    "\n",
    "            pred_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=sum_max_len, \n",
    "                num_beams=4,\n",
    "                repetition_penalty=2.0, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "            for ids in pred_ids:\n",
    "                result = tokenizer.decode(ids)\n",
    "                summary.append(result)\n",
    "                \n",
    "    # remove_tokens = ['<usr>', f\"{tokenizer.unk_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    preprocessed_summary = summary.copy()\n",
    "    for token in remove_tokens:\n",
    "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": fname,\n",
    "            \"summary\" : preprocessed_summary,\n",
    "        }\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'{save_path}/weights/best.pth'))\n",
    "output = predict(tokenizer, model, device, test_loader, test_df['fname'])\n",
    "output.to_csv(f\"{save_path}/prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
