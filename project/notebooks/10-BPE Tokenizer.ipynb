{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./tokenizer/bpe\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../dataset/cleaned_train.csv\")\n",
    "valid_df = pd.read_csv(\"../dataset/cleaned_dev.csv\")\n",
    "df = pd.concat([train_df, valid_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(df, vocab_size=30000, min_frequency=2):\n",
    "    all_text = df['dialogue'].tolist() + df['summary'].tolist()\n",
    "    \n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = ByteLevel()\n",
    "    \n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\",\n",
    "                        \"#Person1#\", \"#Person2#\", \"#Person3#\", \"#Person4#\", \"#Person5#\", \n",
    "                        \"#Person6#\", \"#Person7#\", \"#PhoneNumber#\", \"#Address#\", \"#PassportNumber#\", \n",
    "                        \"#CardNumber#\", \"#Email#\", \"#DateOfBirth#\"]\n",
    "    )\n",
    "    \n",
    "    tokenizer.train_from_iterator(all_text, trainer=trainer)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pretrained_tokenizer(tokenizer):\n",
    "    return PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(df, tokenizer, percentile=95):\n",
    "    dialogue_lengths = [len(tokenizer.encode(str(text))) for text in df['dialogue']]\n",
    "    summary_lengths = [len(tokenizer.encode(str(text))) for text in df['summary']]\n",
    "    all_lengths = dialogue_lengths + summary_lengths\n",
    "    return int(np.percentile(all_lengths, percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer, max_length):\n",
    "    return tokenizer(\n",
    "        examples[\"dialogue\"],\n",
    "        examples[\"summary\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7649e0db6de475e84eae79166c0394d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 어휘 크기: 30000\n",
      "처리된 데이터셋 크기: 12956\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = train_tokenizer(df)\n",
    "pretrained_tokenizer = convert_to_pretrained_tokenizer(base_tokenizer)\n",
    "max_length = get_max_length(df, pretrained_tokenizer, percentile=95)\n",
    "print(max_length)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_datasets = dataset.map(\n",
    "    lambda examples: tokenize_function(examples, pretrained_tokenizer, max_length),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "pretrained_tokenizer.save_pretrained(\"./tokenizer/bpe/\")\n",
    "\n",
    "print(f\"토크나이저 어휘 크기: {len(pretrained_tokenizer)}\")\n",
    "print(f\"처리된 데이터셋 크기: {len(tokenized_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_tokenize(df, tokenizer, n_samples=5):\n",
    "    # 랜덤 샘플링\n",
    "    sampled_df = df.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    for _, row in sampled_df.iterrows():\n",
    "        dialogue = row['dialogue']\n",
    "        summary = row['summary']\n",
    "        \n",
    "        # Dialogue 토큰화\n",
    "        dialogue_tokens = tokenizer.encode(dialogue)\n",
    "        \n",
    "        # Summary 토큰화\n",
    "        summary_tokens = tokenizer.encode(summary)\n",
    "        \n",
    "        print(f\"Original Dialogue: {dialogue}\")\n",
    "        print(f\"Tokenized Dialogue: {dialogue_tokens}\")\n",
    "        print(f\"Decoded Dialogue: {tokenizer.decode(dialogue_tokens)}\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"Original Summary: {summary}\")\n",
    "        print(f\"Tokenized Summary: {summary_tokens}\")\n",
    "        print(f\"Decoded Summary: {tokenizer.decode(summary_tokens)}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dialogue: #Person1#: 아이고, 지난 3개월 동안 10파운드나 쪘어, 옷들이 하나도 안 맞아.\n",
      "#Person2#: 나라면 불평하지 않을 거야, 너는 훨씬 더 잘 생겨 보여. 사실, 너는 또 5파운드를 더 찌워도 여전히 잘 보일 거야.\n",
      "Tokenized Dialogue: [5, 139, 42, 6703, 28, 1307, 670, 2564, 719, 1045, 28638, 13218, 10116, 28, 13000, 8907, 388, 1676, 30, 138, 6, 139, 42, 19053, 20515, 1160, 572, 28, 974, 1609, 421, 541, 6752, 937, 30, 887, 28, 974, 924, 844, 7929, 421, 6360, 16902, 2425, 541, 3932, 572, 30]\n",
      "Decoded Dialogue: #Person1# Ġ : ĠìķĦìĿ´ê³ł, Ġì§ĢëĤľ Ġ3 ê°ľìĽĶ ĠëıĻìķĪ Ġ10 íĮĮìļ´ëĵľëĤĺ Ġìª ĺìĸ´, Ġìĺ·ëĵ¤ìĿ´ ĠíķĺëĤĺëıĦ ĠìķĪ Ġë§ŀìķĦ. Ċ #Person2# Ġ : ĠëĤĺëĿ¼ë©´ Ġë¶Īíıīíķĺì§Ģ ĠìķĬìĿĦ Ġê±°ìķ¼, ĠëĦĪëĬĶ ĠíĽ¨ìĶ¬ ĠëįĶ Ġìŀĺ ĠìĥĿê²¨ Ġë³´ìĹ¬. ĠìĤ¬ìĭ¤, ĠëĦĪëĬĶ ĠëĺĲ Ġ5 íĮĮìļ´ëĵľë¥¼ ĠëįĶ Ġì°Į ìĽĮëıĦ ĠìĹ¬ìłĦíŀĪ Ġìŀĺ Ġë³´ìĿ¼ Ġê±°ìķ¼.\n",
      "\n",
      "\n",
      "Original Summary: #Person1#은 체중이 증가했지만 #Person2#는 #Person1#이 잘 보인다고 생각한다.\n",
      "Tokenized Summary: [5, 1345, 11986, 6728, 2282, 139, 6, 679, 160, 139, 5, 249, 541, 8381, 1433, 30]\n",
      "Decoded Summary: #Person1# ĠìĿĢ Ġì²´ì¤ĳìĿ´ Ġì¦Ŀê°Ģ íĸĪì§Ģë§Į Ġ #Person2# ĠëĬ Ķ Ġ #Person1# ĠìĿ´ Ġìŀĺ Ġë³´ìĿ¸ëĭ¤ê³ł ĠìĥĿê°ģíķľëĭ¤.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Dialogue: #Person1#: 너 역사 231 수업 들어본 적 있어?\n",
      "#Person2#: 응, 지난 학기에 들었어.\n",
      "#Person1#: 교수님 누구셨어?\n",
      "#Person2#: 존슨 교수님이셨어.\n",
      "#Person1#: 나 이번 학기에 그 교수님 수업 듣는데, 어떤 사람이야?\n",
      "#Person2#: 그는 교육 방식이 별로고 요구사항도 많지만, 다행히 그의 수업에서는 쉽게 높은 점수를 받을 수 있어.\n",
      "#Person1#: 너는 어떤 점수 받았어?\n",
      "#Person2#: 나는 A를 받았는데, 시험 점수는 그리 높지 않았어. 그래서 어떻게 그런 좋은 점수를 받았는지 모르겠어.\n",
      "#Person1#: 정말이야? 나 포기하려고 했는데, 너의 경험을 듣고 나니까 계속 수업을 들어야겠어.\n",
      "#Person2#: 너는 시험 점수보다 더 좋은 성적을 받을 거야.\n",
      "#Person1#: 정보 고마워. 이제 안심이 돼.\n",
      "Tokenized Dialogue: [5, 139, 42, 368, 3454, 29232, 930, 4668, 685, 474, 45, 138, 6, 139, 42, 1010, 28, 1307, 4661, 2244, 30, 138, 5, 139, 42, 2693, 1829, 6850, 45, 138, 6, 139, 42, 3820, 6650, 6850, 30, 138, 5, 139, 42, 310, 1003, 4661, 207, 2693, 930, 19009, 28, 562, 6613, 45, 138, 6, 139, 42, 772, 1992, 10651, 1453, 202, 12291, 269, 9750, 28, 3917, 1173, 11851, 3057, 2707, 6077, 1621, 257, 474, 30, 138, 5, 139, 42, 974, 562, 10005, 4281, 45, 138, 6, 139, 42, 567, 11931, 8223, 28, 1513, 11244, 2079, 13930, 2267, 30, 934, 499, 554, 561, 6077, 19113, 1826, 30, 138, 5, 139, 42, 4075, 45, 310, 4309, 1186, 2946, 28, 1506, 2791, 2386, 27005, 1261, 1855, 5887, 975, 30, 138, 6, 139, 42, 974, 1513, 29483, 421, 561, 6346, 1621, 572, 30, 138, 5, 139, 42, 1326, 1205, 30, 910, 27139, 1165, 30]\n",
      "Decoded Dialogue: #Person1# Ġ : ĠëĦĪ ĠìĹŃìĤ¬ Ġ231 ĠìĪĺìĹħ Ġëĵ¤ìĸ´ë³¸ Ġìłģ ĠìŀĪìĸ´? Ċ #Person2# Ġ : ĠìĿĳ, Ġì§ĢëĤľ ĠíķĻê¸°ìĹĲ Ġëĵ¤ìĹĪìĸ´. Ċ #Person1# Ġ : ĠêµĲìĪĺëĭĺ ĠëĪĦêµ¬ ìħ¨ìĸ´? Ċ #Person2# Ġ : Ġì¡´ìĬ¨ ĠêµĲìĪĺëĭĺìĿ´ ìħ¨ìĸ´. Ċ #Person1# Ġ : ĠëĤĺ ĠìĿ´ë²Ī ĠíķĻê¸°ìĹĲ Ġê·¸ ĠêµĲìĪĺëĭĺ ĠìĪĺìĹħ Ġëĵ£ëĬĶëį°, Ġìĸ´ëĸ¤ ĠìĤ¬ëŀĮìĿ´ìķ¼? Ċ #Person2# Ġ : Ġê·¸ëĬĶ ĠêµĲìľ¡ Ġë°©ìĭĿìĿ´ Ġë³Ħë¡ľ ê³ł ĠìļĶêµ¬ìĤ¬íķŃ ëıĦ Ġë§İì§Ģë§Į, Ġëĭ¤íĸīíŀĪ Ġê·¸ìĿĺ ĠìĪĺìĹħìĹĲìĦľëĬĶ Ġìī½ê²Į ĠëĨĴìĿĢ ĠìłĲìĪĺë¥¼ Ġë°ĽìĿĦ ĠìĪĺ ĠìŀĪìĸ´. Ċ #Person1# Ġ : ĠëĦĪëĬĶ Ġìĸ´ëĸ¤ ĠìłĲìĪĺ Ġë°Ľìķĺìĸ´? Ċ #Person2# Ġ : ĠëĤĺëĬĶ ĠAë¥¼ Ġë°ĽìķĺëĬĶëį°, ĠìĭľíĹĺ ĠìłĲìĪĺëĬĶ Ġê·¸ë¦¬ ĠëĨĴì§Ģ ĠìķĬìķĺìĸ´. Ġê·¸ëŀĺìĦľ Ġìĸ´ëĸ»ê²Į Ġê·¸ëŁ° Ġì¢ĭìĿĢ ĠìłĲìĪĺë¥¼ Ġë°ĽìķĺëĬĶì§Ģ Ġëª¨ë¥´ê²łìĸ´. Ċ #Person1# Ġ : Ġìłķë§ĲìĿ´ìķ¼? ĠëĤĺ Ġíı¬ê¸° íķĺëł¤ê³ł ĠíĸĪëĬĶëį°, ĠëĦĪìĿĺ Ġê²½íĹĺìĿĦ Ġëĵ£ê³ł ĠëĤĺëĭĪê¹Į Ġê³ĦìĨį ĠìĪĺìĹħìĿĦ Ġëĵ¤ìĸ´ìķ¼ ê²łìĸ´. Ċ #Person2# Ġ : ĠëĦĪëĬĶ ĠìĭľíĹĺ ĠìłĲìĪĺë³´ëĭ¤ ĠëįĶ Ġì¢ĭìĿĢ ĠìĦ±ìłģìĿĦ Ġë°ĽìĿĦ Ġê±°ìķ¼. Ċ #Person1# Ġ : Ġìłķë³´ Ġê³łë§ĪìĽĮ. ĠìĿ´ìłľ ĠìķĪìĭ¬ìĿ´ Ġëı¼.\n",
      "\n",
      "\n",
      "Original Summary: #Person2#는 #Person1#에게 존슨 교수님이 요구사항이 많지만 높은 점수를 주는 교수라고 말한다. #Person1#는 안심하고 수업을 계속 듣기로 결정한다.\n",
      "Tokenized Summary: [6, 679, 160, 139, 5, 1263, 230, 3820, 6650, 14532, 9750, 2707, 6077, 3316, 2155, 424, 1033, 30, 139, 5, 679, 160, 10632, 332, 1855, 1261, 1414, 1228, 3741, 30]\n",
      "Decoded Summary: #Person2# ĠëĬ Ķ Ġ #Person1# ĠìĹĲ ê²Į Ġì¡´ìĬ¨ ĠêµĲìĪĺëĭĺìĿ´ ĠìļĶêµ¬ìĤ¬íķŃìĿ´ Ġë§İì§Ģë§Į ĠëĨĴìĿĢ ĠìłĲìĪĺë¥¼ Ġì£¼ëĬĶ ĠêµĲìĪĺ ëĿ¼ê³ł Ġë§Ĳíķľëĭ¤. Ġ #Person1# ĠëĬ Ķ ĠìķĪìĭ¬ íķĺê³ł ĠìĪĺìĹħìĿĦ Ġê³ĦìĨį Ġëĵ£ ê¸°ë¡ľ Ġê²°ìłķíķľëĭ¤.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Dialogue: #Person1#: 당신 무엇을 한 거에요! \n",
      "#Person2#: 정말 죄송합니다, 선생님. 바로 천을 가져다 드리겠습니다.\n",
      "#Person1#: 그래요, 그리고 서둘러요!\n",
      "#Person2#: 제 부주의함에 대해 사과드립니다. 제가 대신 청소해 드릴까요?\n",
      "#Person1#: 아니요, 제가 직접 할게요. \n",
      "#Person2#: 이게 제 명함입니다, 선생님. 청소 비용 청구서를 보내주시면 제가 환불해 드리겠습니다.\n",
      "#Person1#: 그래야지요!\n",
      "#Person2#: 이런 불편을 끼쳐 드려 대단히 죄송합니다.\n",
      "#Person1#: 네, 앞으로 좀 더 조심하길 바래요!\n",
      "#Person2#: 그럴게요, 선생님. 정말로 매우 죄송합니다.\n",
      "#Person1#: 괜찮아요.\n",
      "Tokenized Dialogue: [5, 139, 42, 452, 889, 503, 2207, 18, 139, 138, 6, 139, 42, 381, 1377, 28, 847, 30, 1069, 17101, 1983, 2475, 30, 138, 5, 139, 42, 1882, 28, 519, 22100, 18, 138, 6, 139, 42, 298, 12122, 7398, 441, 9657, 30, 614, 1936, 1830, 224, 2485, 45, 138, 5, 139, 42, 741, 28, 614, 1824, 2978, 30, 139, 138, 6, 139, 42, 1822, 298, 10096, 28, 847, 30, 1830, 1361, 6088, 21484, 614, 10578, 2475, 30, 138, 5, 139, 42, 6690, 11417, 18, 138, 6, 139, 42, 1129, 8811, 10820, 5833, 8794, 1377, 30, 138, 5, 139, 42, 356, 28, 2994, 617, 421, 2734, 4200, 10404, 18, 138, 6, 139, 42, 6709, 28, 847, 30, 1257, 748, 1377, 30, 138, 5, 139, 42, 1971, 30]\n",
      "Decoded Dialogue: #Person1# Ġ : Ġëĭ¹ìĭł Ġë¬´ìĹĩìĿĦ Ġíķľ Ġê±°ìĹĲìļĶ! Ġ Ċ #Person2# Ġ : Ġìłķë§Ĳ Ġì£ĦìĨ¡íķ©ëĭĪëĭ¤, ĠìĦłìĥĿëĭĺ. Ġë°Ķë¡ľ Ġì²ľìĿĦ Ġê°Ģìł¸ëĭ¤ Ġëĵľë¦¬ê²łìĬµëĭĪëĭ¤. Ċ #Person1# Ġ : Ġê·¸ëŀĺìļĶ, Ġê·¸ë¦¬ê³ł ĠìĦľëĳĺëŁ¬ìļĶ! Ċ #Person2# Ġ : Ġìłľ Ġë¶Ģì£¼ìĿĺ íķ¨ìĹĲ ĠëĮĢíķ´ ĠìĤ¬ê³¼ëĵľë¦½ëĭĪëĭ¤. Ġìłľê°Ģ ĠëĮĢìĭł Ġì²ŃìĨĮ íķ´ Ġëĵľë¦´ê¹ĮìļĶ? Ċ #Person1# Ġ : ĠìķĦëĭĪìļĶ, Ġìłľê°Ģ Ġì§ģìłĳ Ġíķłê²ĮìļĶ. Ġ Ċ #Person2# Ġ : ĠìĿ´ê²Į Ġìłľ Ġëªħíķ¨ìŀħëĭĪëĭ¤, ĠìĦłìĥĿëĭĺ. Ġì²ŃìĨĮ Ġë¹Ħìļ© Ġì²Ńêµ¬ìĦľë¥¼ Ġë³´ëĤ´ì£¼ìĭľë©´ Ġìłľê°Ģ ĠíĻĺë¶Īíķ´ Ġëĵľë¦¬ê²łìĬµëĭĪëĭ¤. Ċ #Person1# Ġ : Ġê·¸ëŀĺìķ¼ ì§ĢìļĶ! Ċ #Person2# Ġ : ĠìĿ´ëŁ° Ġë¶Īíİ¸ìĿĦ Ġëģ¼ì³Ĳ Ġëĵľëł¤ ĠëĮĢëĭ¨íŀĪ Ġì£ĦìĨ¡íķ©ëĭĪëĭ¤. Ċ #Person1# Ġ : ĠëĦ¤, Ġìķŀìľ¼ë¡ľ Ġì¢Ģ ĠëįĶ Ġì¡°ìĭ¬ íķĺê¸¸ Ġë°ĶëŀĺìļĶ! Ċ #Person2# Ġ : Ġê·¸ëŁ´ê²ĮìļĶ, ĠìĦłìĥĿëĭĺ. Ġìłķë§Ĳë¡ľ Ġë§¤ìļ° Ġì£ĦìĨ¡íķ©ëĭĪëĭ¤. Ċ #Person1# Ġ : Ġê´ľì°®ìķĦìļĶ.\n",
      "\n",
      "\n",
      "Original Summary: #Person2#가 실수로 #Person1#의 옷을 더럽혔다. #Person1#이 화를 낸다. #Person2#는 사과하고 청소 비용을 환불하겠다고 약속한다.\n",
      "Tokenized Summary: [6, 258, 8118, 139, 5, 779, 2017, 17706, 4693, 189, 30, 139, 5, 249, 4810, 4179, 189, 30, 139, 6, 679, 160, 7133, 1830, 2799, 4455, 4651, 9078, 30]\n",
      "Decoded Summary: #Person2# Ġê°Ģ Ġìĭ¤ìĪĺë¡ľ Ġ #Person1# ĠìĿĺ Ġìĺ·ìĿĦ ĠëįĶëŁ½ íĺĶ ëĭ¤. Ġ #Person1# ĠìĿ´ ĠíĻĶë¥¼ ĠëĤ¸ ëĭ¤. Ġ #Person2# ĠëĬ Ķ ĠìĤ¬ê³¼íķĺê³ł Ġì²ŃìĨĮ Ġë¹Ħìļ©ìĿĦ ĠíĻĺë¶Ī íķĺê²łëĭ¤ê³ł Ġìķ½ìĨįíķľëĭ¤.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_and_tokenize(df, pretrained_tokenizer, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
