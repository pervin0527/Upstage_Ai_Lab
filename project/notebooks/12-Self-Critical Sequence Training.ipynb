{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "from datetime import datetime\n",
    "\n",
    "from torch import nn\n",
    "from konlpy.tag import Mecab\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PhoneNumber#', '#Address#', '#PassportNumber#', '#CardNumber#', '#Email#', '#DateOfBirth#', '<sep>']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"psyche/KoT5-summarization\"\n",
    "train_df = pd.read_csv('../dataset/cleaned_train.csv')\n",
    "val_df = pd.read_csv('../dataset/cleaned_dev.csv')\n",
    "test_df = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "patience = 5\n",
    "\n",
    "init_lr = 0.0001\n",
    "max_lr = 0.001\n",
    "warmup_epochs = 10\n",
    "T_0 = 100\n",
    "T_mult = 1\n",
    "T_gamma = 0.5\n",
    "\n",
    "dig_max_len = 1024\n",
    "sum_max_len = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "special_tokens_dict={'additional_special_tokens': ['#Person1#', '#Person2#','#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PhoneNumber#', \n",
    "                                                   '#Address#', '#PassportNumber#', '#CardNumber#', '#Email#', '#DateOfBirth#',\n",
    "                                                   '<sep>']}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "remove_tokens = [\n",
    "    '<usr>',\n",
    "    f\"{tokenizer.unk_token}\", \n",
    "    f\"{tokenizer.eos_token}\", \n",
    "    f\"{tokenizer.pad_token}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, input_len, summ_len, is_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df.copy()\n",
    "        self.source_len = input_len\n",
    "        self.summ_len = summ_len\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.df.loc[:, 'dialogue'] = self.df['dialogue'].apply(self.add_sep_tokens)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.input_ids = tokenizer(self.df['dialogue'].tolist(), return_tensors=\"pt\", padding=True,\n",
    "                                add_special_tokens=True, truncation=True, max_length=512, return_token_type_ids=False).input_ids\n",
    "            self.labels = tokenizer(self.df['summary'].tolist(), return_tensors=\"pt\", padding=True,\n",
    "                                add_special_tokens=True, truncation=True, max_length=100, return_token_type_ids=False).input_ids\n",
    "        else:\n",
    "            self.input_ids = tokenizer(self.df['dialogue'].tolist(), return_tensors=\"pt\", padding=True,\n",
    "                                add_special_tokens=True, truncation=True, max_length=512, return_token_type_ids=False).input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.input_ids[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.input_ids[idx]\n",
    "\n",
    "    def add_sep_tokens(self, dialogue):\n",
    "        pattern = r'(#Person\\d+#)'\n",
    "        parts = re.split(pattern, dialogue)\n",
    "        result = []\n",
    "        prev_speaker = None\n",
    "        for part in parts:\n",
    "            if re.match(pattern, part):\n",
    "                if prev_speaker and prev_speaker != part:\n",
    "                    result.append('<sep>')\n",
    "                prev_speaker = part\n",
    "            result.append(part)\n",
    "        return ''.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df[['dialogue', 'summary']], tokenizer, dig_max_len, sum_max_len)\n",
    "val_dataset = CustomDataset(val_df[['dialogue', 'summary']], tokenizer, dig_max_len, sum_max_len)\n",
    "test_dataset = CustomDataset(test_df[['dialogue']], tokenizer, dig_max_len, sum_max_len, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_words(tokenizer, preds, labels):\n",
    "    decoded_preds = tokenizer.batch_decode(preds, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "        \n",
    "    return replaced_predictions, replaced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(replaced_predictions, replaced_labels):\n",
    "    rouge = Rouge()\n",
    "\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, input_ids, max_length):\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def greedy_sequence(model, input_ids, max_length):\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=1,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predictions, references, avg=True)\n",
    "    return {key: value['f'] for key, value in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scst_loss(pred_ids, baseline_ids, labels, tokenizer, model):\n",
    "    replaced_preds, replaced_labels = ids_to_words(tokenizer, pred_ids, labels)\n",
    "    replaced_baseline, _ = ids_to_words(tokenizer, baseline_ids, labels)\n",
    "\n",
    "    pred_scores = compute_rouge(replaced_preds, replaced_labels)\n",
    "    baseline_scores = compute_rouge(replaced_baseline, replaced_labels)\n",
    "\n",
    "    rewards = torch.tensor([pred_scores['rouge-l'] for _ in range(len(pred_ids))]).to(pred_ids.device)\n",
    "    baseline = torch.tensor([baseline_scores['rouge-l'] for _ in range(len(baseline_ids))]).to(baseline_ids.device)\n",
    "\n",
    "    log_probs = compute_log_probs(model, pred_ids, labels)\n",
    "    loss = -((rewards - baseline) * log_probs).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_log_probs(model, pred_ids, labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=pred_ids, labels=labels)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(2, labels.unsqueeze(2)).squeeze(2)\n",
    "    mask = (labels != tokenizer.pad_token_id).float()\n",
    "    token_log_probs = token_log_probs * mask\n",
    "    seq_log_probs = token_log_probs.sum(dim=1)\n",
    "    \n",
    "    return seq_log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scst(epoch, model, device, train_loader, optimizer, train_step, writer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch}\", leave=False):\n",
    "        input_ids = batch[0].to(device, dtype=torch.long)\n",
    "        labels = batch[1].to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        baseline_ids = greedy_sequence(model, input_ids, max_length=256)\n",
    "        pred_ids = sample_sequence(model, input_ids, max_length=256)\n",
    "\n",
    "        loss = scst_loss(pred_ids, baseline_ids, labels, tokenizer, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_step += 1\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "    \n",
    "    return train_step, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_scst(tokenizer, model, device, val_loader, writer, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_results = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validating\", leave=False):\n",
    "            input_ids = batch[0].to(device, dtype=torch.long)\n",
    "            labels = batch[1].to(device, dtype=torch.long)\n",
    "\n",
    "            pred_ids = model.generate(input_ids=input_ids, max_length=256, num_beams=4)\n",
    "\n",
    "            replaced_predictions, replaced_labels = ids_to_words(tokenizer, pred_ids, labels)\n",
    "            result = compute_metrics(replaced_predictions, replaced_labels)\n",
    "\n",
    "            all_results.append(result)\n",
    "            all_predictions.extend(replaced_predictions)\n",
    "            all_labels.extend(replaced_labels)\n",
    "\n",
    "    avg_result = {key: sum(r[key] for r in all_results) / len(all_results) for key in all_results[0]}\n",
    "    writer.add_scalar('ROUGE/rouge-1', avg_result['rouge-1'], epoch)\n",
    "    writer.add_scalar('ROUGE/rouge-2', avg_result['rouge-2'], epoch)\n",
    "    writer.add_scalar('ROUGE/rouge-l', avg_result['rouge-l'], epoch)\n",
    "\n",
    "    return avg_result, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 0.765012\n",
      "Validation Loss: 0.586895, Rouge-1: 0.342694, Rouge-2: 0.111150, Rouge-l: 0.320415\n",
      "Average ROUGE: 0.258086\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 숨쉬기가 힘들다고 느끼고, 알레르기가 있는지 묻습니다. #Person2# 는 폐 전문의에게 천식에 대한 검사를 받게 할 것입니다.                                     \n",
      "GOLD: #Person2# 는 숨쉬기에 어려움을 겪는다. 의사는 #Person1# 에게 이에 대해 묻고, #Person2# 를 폐 전문의에게 보낼 예정이다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   헤이, 지미는 3시 30분에 운동하러 가자고 제안한다. 그들은 단지 두 날을 바꾸는 것을 제안하고, 금요일에 다리를 할 수 있다고 말한다.                                   \n",
      "GOLD: #Person1# 은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 더 이상 건강에 해로운 음식을 먹는 것을 멈추라고 #Person2# 에게 말한다. #Person1# 는 주로 과일, 채소, 그리고 닭고기를 먹는 편이다.                                   \n",
      "GOLD: #Person1# 은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2# 는 자신의 건강한 레시피를 #Person1# 와 공유한다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "New best model saved with average ROUGE: 0.258086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.580652\n",
      "Validation Loss: 0.557176, Rouge-1: 0.338138, Rouge-2: 0.108483, Rouge-l: 0.318480\n",
      "Average ROUGE: 0.255034\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   의사 선생님은 #Person1# 에게 최근 숨쉬기가 힘들다고 말하고, 알레르기가 있는지 묻습니다. #Person2# 는 폐 전문의에게 천식에 대한 검사를 받도록 할 것입니다.                         \n",
      "GOLD: #Person2# 는 숨쉬기에 어려움을 겪는다. 의사는 #Person1# 에게 이에 대해 묻고, #Person2# 를 폐 전문의에게 보낼 예정이다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   헤이, 지미는 3시 30분에 헬스장에 가자고 제안한다. 그들은 금요일에 두 날을 바꾸기로 결정하고 다시 만나기로 한다.                                \n",
      "GOLD: #Person1# 은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   #Person1# 과 #Person2# 는 건강에 해로운 음식을 먹는 것을 멈추고 있다. 그들은 주로 과일, 채소, 그리고 닭고기를 먹는 편이다.                                  \n",
      "GOLD: #Person1# 은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2# 는 자신의 건강한 레시피를 #Person1# 와 공유한다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Not improve. Early stopping counter: 1/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.530246\n",
      "Validation Loss: 0.534735, Rouge-1: 0.373175, Rouge-2: 0.130584, Rouge-l: 0.350167\n",
      "Average ROUGE: 0.284642\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 #Person2# 에게 최근 숨쉬기가 힘들다고 말합니다. #Person1# 는 폐 전문의에게 천식에 대한 검사를 받도록 요청합니다.                                 \n",
      "GOLD: #Person2# 는 숨쉬기에 어려움을 겪는다. 의사는 #Person1# 에게 이에 대해 묻고, #Person2# 를 폐 전문의에게 보낼 예정이다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   지미는 #Person1# 에게 운동하러 가자고 제안하지만, #Person2# 은 지미 때문에 모든 것이 망가지고 있다고 말한다.                                    \n",
      "GOLD: #Person1# 은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 과 #Person2# 는 건강에 해로운 음식을 먹는 것을 멈추고 있다. 그들은 주로 과일, 채소, 그리고 닭고기를 먹는다.                                     \n",
      "GOLD: #Person1# 은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2# 는 자신의 건강한 레시피를 #Person1# 와 공유한다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "New best model saved with average ROUGE: 0.284642\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.484493\n",
      "Validation Loss: 0.526318, Rouge-1: 0.363181, Rouge-2: 0.125128, Rouge-l: 0.341818\n",
      "Average ROUGE: 0.276709\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 숨쉬기가 힘들다고 #Person2# 에게 말하고, 알레르기는 없지만 운동을 할 때 많이 나타난다고 말한다. #Person1# 는 폐 전문의에게 천식 검사를 받도록 요청한다.             \n",
      "GOLD: #Person2# 는 숨쉬기에 어려움을 겪는다. 의사는 #Person1# 에게 이에 대해 묻고, #Person2# 를 폐 전문의에게 보낼 예정이다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   지미는 #Person1# 에게 운동하러 가자고 제안한다. 그들은 금요일에 다리를 할 수 있도록 두 날을 바꾼다.                           \n",
      "GOLD: #Person1# 은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 #Person2# 에게 건강에 해로운 음식을 먹는 것을 멈춰야 한다고 말한다. #Person1# 는 최근에 더 건강한 음식을 먹기 시작했다.                         \n",
      "GOLD: #Person1# 은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2# 는 자신의 건강한 레시피를 #Person1# 와 공유한다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Not improve. Early stopping counter: 1/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.447137\n",
      "Validation Loss: 0.525816, Rouge-1: 0.372769, Rouge-2: 0.129491, Rouge-l: 0.351387\n",
      "Average ROUGE: 0.284549\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 숨쉬기가 좀 힘들다고 #Person2# 에게 말합니다. #Person1# 는 알레르기가 없고 운동을 할 때 많이 나타납니다.                             \n",
      "GOLD: #Person2# 는 숨쉬기에 어려움을 겪는다. 의사는 #Person1# 에게 이에 대해 묻고, #Person2# 를 폐 전문의에게 보낼 예정이다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   지미는 #Person1# 에게 운동하러 가자고 제안한다. 그들은 금요일에 다리를 할 수 있도록 두 날을 바꾸기로 한다.                               \n",
      "GOLD: #Person1# 은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 더 이상 건강에 해로운 음식을 먹는 것을 멈추고 싶어한다. #Person2# 는 최근에 더 건강한 음식을 먹기 시작했으며, 주로 과일, 채소, 닭고기를 먹는다.                       \n",
      "GOLD: #Person1# 은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2# 는 자신의 건강한 레시피를 #Person1# 와 공유한다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Not improve. Early stopping counter: 2/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.412844\n",
      "Validation Loss: 0.533887, Rouge-1: 0.370534, Rouge-2: 0.128973, Rouge-l: 0.348303\n",
      "Average ROUGE: 0.282603\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 #Person2# 에게 최근 숨쉬기가 힘들다고 말하고, 알레르기가 없다고 말한다. #Person1# 는 폐 전문의에게 천식 검사를 받도록 요청할 것이다.                          \n",
      "GOLD: #Person2# 는 숨쉬기에 어려움을 겪는다. 의사는 #Person1# 에게 이에 대해 묻고, #Person2# 를 폐 전문의에게 보낼 예정이다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 지미에게 운동하러 가자고 제안한다. 지미는 팔과 배를 운동하자고 제안하지만, #Person2# 은 주간 스케줄을 따르고 있다. 그들은 금요일에 다리를 할 것이다.                      \n",
      "GOLD: #Person1# 은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# 은 #Person2# 에게 건강에 해로운 음식을 먹지 말고 과일, 채소, 닭고기를 먹으라고 조언한다.                                      \n",
      "GOLD: #Person1# 은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2# 는 자신의 건강한 레시피를 #Person1# 와 공유한다.                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Not improve. Early stopping counter: 3/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "train_step = 0\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_path = os.path.join(\"./T5_runs\", timestamp)\n",
    "\n",
    "weights_path = os.path.join(save_path, 'weights')\n",
    "logs_path = os.path.join(save_path, 'logs')\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "best_avg_rouge = 0\n",
    "early_stopping_counter = 0\n",
    "writer = SummaryWriter(log_dir=logs_path)\n",
    "scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_max=max_lr,  T_up=warmup_epochs, gamma=T_gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_step, train_loss = train_scst(epoch, model, device, train_loader, optimizer, train_step, writer)\n",
    "    val_result, val_predictions, val_labels = validate_scst(tokenizer, model, device, val_loader, writer, epoch)\n",
    "\n",
    "    avg_rouge = (val_result['rouge-1'] + val_result['rouge-2'] + val_result['rouge-l']) / 3\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"Rouge-1: {val_result['rouge-1']:.6f}, Rouge-2: {val_result['rouge-2']:.6f}, Rouge-l: {val_result['rouge-l']:.6f}\")\n",
    "    print(f\"Average ROUGE: {avg_rouge:.6f}\")\n",
    "\n",
    "    print('-'*150)\n",
    "    for i in range(3):\n",
    "        print(f\"PRED: {val_predictions[i]}\")\n",
    "        print(f\"GOLD: {val_labels[i]}\")\n",
    "        print('-'*150)\n",
    "\n",
    "    if avg_rouge > best_avg_rouge:\n",
    "        best_avg_rouge = avg_rouge\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), os.path.join(weights_path, 'best.pth'))\n",
    "        print(f\"New best model saved with average ROUGE: {best_avg_rouge:.6f}\")\n",
    "\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"Not improved. Early stopping counter: {early_stopping_counter}/{patience}\")\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(weights_path, f'epoch-{epoch}.pth'))\n",
    "    print()\n",
    "\n",
    "writer.close()\n",
    "torch.save(model.state_dict(), os.path.join(weights_path, 'last.pth'))\n",
    "print(\"Training completed. Last model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, device, test_loader, fname):\n",
    "    model.eval()\n",
    "    summary = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids in tqdm(test_loader):\n",
    "            input_ids = input_ids.to(device, dtype=torch.long)\n",
    "\n",
    "            pred_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=256, \n",
    "                num_beams=4,\n",
    "                repetition_penalty=2.0, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "            for ids in pred_ids:\n",
    "                result = tokenizer.decode(ids)\n",
    "                summary.append(result)\n",
    "                \n",
    "    remove_tokens = ['<usr>', f\"{tokenizer.unk_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    preprocessed_summary = summary.copy()\n",
    "    for token in remove_tokens:\n",
    "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": fname,\n",
    "            \"summary\" : preprocessed_summary,\n",
    "        }\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(f'{save_path}/weights/best.pth')\n",
    "output = predict(tokenizer, model, device, test_loader, test_df['fname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(f\"{save_path}/prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
