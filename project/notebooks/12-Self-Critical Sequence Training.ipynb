{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "from datetime import datetime\n",
    "\n",
    "from torch import nn\n",
    "from konlpy.tag import Mecab\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"psyche/KoT5-summarization\"\n",
    "train_df = pd.read_csv('../dataset/cleaned_train.csv')\n",
    "val_df = pd.read_csv('../dataset/cleaned_dev.csv')\n",
    "test_df = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "patience = 5\n",
    "\n",
    "init_lr = 0.0001\n",
    "max_lr = 0.001\n",
    "warmup_epochs = 10\n",
    "T_0 = 100\n",
    "T_mult = 1\n",
    "T_gamma = 0.5\n",
    "\n",
    "dig_max_len = 1024\n",
    "sum_max_len = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "special_tokens_dict={'additional_special_tokens': ['#Person1#', '#Person2#','#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PhoneNumber#', \n",
    "                                                   '#Address#', '#PassportNumber#', '#CardNumber#', '#Email#', '#DateOfBirth#',\n",
    "                                                   '<sep>']}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "remove_tokens = [\n",
    "    '<usr>',\n",
    "    f\"{tokenizer.unk_token}\", \n",
    "    f\"{tokenizer.eos_token}\", \n",
    "    f\"{tokenizer.pad_token}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, input_len, summ_len, is_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df.copy()\n",
    "        self.source_len = input_len\n",
    "        self.summ_len = summ_len\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.df.loc[:, 'dialogue'] = self.df['dialogue'].apply(self.add_sep_tokens)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.input_ids = tokenizer(self.df['dialogue'].tolist(), return_tensors=\"pt\", padding=True,\n",
    "                                add_special_tokens=True, truncation=True, max_length=512, return_token_type_ids=False).input_ids\n",
    "            self.labels = tokenizer(self.df['summary'].tolist(), return_tensors=\"pt\", padding=True,\n",
    "                                add_special_tokens=True, truncation=True, max_length=100, return_token_type_ids=False).input_ids\n",
    "        else:\n",
    "            self.input_ids = tokenizer(self.df['dialogue'].tolist(), return_tensors=\"pt\", padding=True,\n",
    "                                add_special_tokens=True, truncation=True, max_length=512, return_token_type_ids=False).input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.input_ids[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.input_ids[idx]\n",
    "\n",
    "    def add_sep_tokens(self, dialogue):\n",
    "        pattern = r'(#Person\\d+#)'\n",
    "        parts = re.split(pattern, dialogue)\n",
    "        result = []\n",
    "        prev_speaker = None\n",
    "        for part in parts:\n",
    "            if re.match(pattern, part):\n",
    "                if prev_speaker and prev_speaker != part:\n",
    "                    result.append('<sep>')\n",
    "                prev_speaker = part\n",
    "            result.append(part)\n",
    "        return ''.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df[['dialogue', 'summary']], tokenizer, dig_max_len, sum_max_len)\n",
    "val_dataset = CustomDataset(val_df[['dialogue', 'summary']], tokenizer, dig_max_len, sum_max_len)\n",
    "test_dataset = CustomDataset(test_df[['dialogue']], tokenizer, dig_max_len, sum_max_len, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_words(tokenizer, preds, labels):\n",
    "    decoded_preds = tokenizer.batch_decode(preds, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "        \n",
    "    return replaced_predictions, replaced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(replaced_predictions, replaced_labels):\n",
    "    rouge = Rouge()\n",
    "\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, input_ids, max_length):\n",
    "    # 실제 샘플링 구현\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=2,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "    return output.sequences, output.scores\n",
    "\n",
    "def greedy_sequence(model, input_ids, max_length):\n",
    "    # 그리디 디코딩 구현\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=1,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predictions, references, avg=True)\n",
    "    return {key: value['f'] for key, value in scores.items()}\n",
    "\n",
    "\n",
    "def compute_log_probs(model, input_ids, sampled_ids):\n",
    "    # 로그 확률 계산을 위해 모델을 다시 실행\n",
    "    outputs = model(input_ids=input_ids, labels=sampled_ids)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)\n",
    "    target_log_probs = torch.gather(log_probs, 2, sampled_ids[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # 패딩 마스크 생성\n",
    "    mask = (sampled_ids[:, 1:] != model.config.pad_token_id).float()\n",
    "    \n",
    "    # 마스크를 적용하여 패딩 토큰을 무시\n",
    "    masked_log_probs = target_log_probs * mask\n",
    "    sequence_log_probs = masked_log_probs.sum(dim=1)\n",
    "    \n",
    "    return sequence_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scst_loss(model, input_ids, pred_ids, baseline_ids, labels, tokenizer):\n",
    "    # 예측 시퀀스와 baseline 시퀀스를 디코딩\n",
    "    replaced_preds, replaced_labels = ids_to_words(tokenizer, pred_ids, labels)\n",
    "    replaced_baseline, _ = ids_to_words(tokenizer, baseline_ids, labels)\n",
    "\n",
    "    # ROUGE 점수 계산\n",
    "    pred_scores_rouge = compute_rouge(replaced_preds, replaced_labels)\n",
    "    baseline_scores = compute_rouge(replaced_baseline, replaced_labels)\n",
    "\n",
    "    # 보상 계산 (여기서는 ROUGE-L 점수를 사용)\n",
    "    rewards = torch.tensor([pred_scores_rouge['rouge-l'] for _ in range(len(pred_ids))]).to(pred_ids.device)\n",
    "    baseline = torch.tensor([baseline_scores['rouge-l'] for _ in range(len(baseline_ids))]).to(baseline_ids.device)\n",
    "\n",
    "    # 로그 확률 계산\n",
    "    log_probs = compute_log_probs(model, input_ids, pred_ids)\n",
    "\n",
    "    # SCST 손실 계산\n",
    "    loss = -((rewards - baseline.detach()) * log_probs).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scst(epoch, model, device, train_loader, optimizer, tokenizer, writer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch}\", leave=False):\n",
    "        input_ids = batch[0].to(device, dtype=torch.long)\n",
    "        labels = batch[1].to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Baseline 예측 (greedy decoding)\n",
    "        with torch.no_grad():\n",
    "            baseline_ids = greedy_sequence(model, input_ids, max_length=256)\n",
    "\n",
    "        # 샘플링을 통한 예측\n",
    "        pred_ids, _ = sample_sequence(model, input_ids, max_length=256)\n",
    "\n",
    "        # SCST로 loss 계산\n",
    "        loss = scst_loss(model, input_ids, pred_ids, baseline_ids, labels, tokenizer)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_scst(epoch, model, device, val_loader, tokenizer, writer):\n",
    "    model.eval()\n",
    "    total_rouge_1 = 0.0\n",
    "    total_rouge_2 = 0.0\n",
    "    total_rouge_l = 0.0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validating\", leave=False):\n",
    "            input_ids = batch[0].to(device, dtype=torch.long)\n",
    "            labels = batch[1].to(device, dtype=torch.long)\n",
    "\n",
    "            # 샘플링을 통한 예측\n",
    "            pred_ids, _ = sample_sequence(model, input_ids, max_length=256)\n",
    "\n",
    "            # 디코딩\n",
    "            predictions, references = ids_to_words(tokenizer, pred_ids, labels)\n",
    "            \n",
    "            # ROUGE 점수 계산\n",
    "            rouge_scores = compute_rouge(predictions, references)\n",
    "            \n",
    "            total_rouge_1 += rouge_scores['rouge-1']\n",
    "            total_rouge_2 += rouge_scores['rouge-2']\n",
    "            total_rouge_l += rouge_scores['rouge-l']\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_references.extend(references)\n",
    "\n",
    "    avg_rouge_1 = total_rouge_1 / len(val_loader)\n",
    "    avg_rouge_2 = total_rouge_2 / len(val_loader)\n",
    "    avg_rouge_l = total_rouge_l / len(val_loader)\n",
    "\n",
    "    writer.add_scalar('ROUGE/rouge-1', avg_rouge_1, epoch)\n",
    "    writer.add_scalar('ROUGE/rouge-2', avg_rouge_2, epoch)\n",
    "    writer.add_scalar('ROUGE/rouge-l', avg_rouge_l, epoch)\n",
    "\n",
    "    return {\n",
    "        'rouge-1': avg_rouge_1,\n",
    "        'rouge-2': avg_rouge_2,\n",
    "        'rouge-l': avg_rouge_l\n",
    "    }, all_predictions, all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = 0\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_path = os.path.join(\"./T5_runs\", timestamp)\n",
    "\n",
    "weights_path = os.path.join(save_path, 'weights')\n",
    "logs_path = os.path.join(save_path, 'logs')\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "best_rouge_l = 0\n",
    "best_avg_rouge = 0\n",
    "early_stopping_counter = 0\n",
    "writer = SummaryWriter(log_dir=logs_path)\n",
    "scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_max=max_lr,  T_up=warmup_epochs, gamma=T_gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train_scst(epoch, model, device, train_loader, optimizer, tokenizer, writer)\n",
    "    val_result, val_predictions, val_references = validate_scst(epoch, model, device, val_loader, tokenizer, writer)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"Validation ROUGE-1: {val_result['rouge-1']:.6f}\")\n",
    "    print(f\"Validation ROUGE-2: {val_result['rouge-2']:.6f}\")\n",
    "    print(f\"Validation ROUGE-L: {val_result['rouge-l']:.6f}\")\n",
    "\n",
    "    # 예시 출력\n",
    "    print('-' * 100)\n",
    "    for i in range(min(3, len(val_predictions))):\n",
    "        print(f\"예측: {val_predictions[i]}\")\n",
    "        print(f\"참조: {val_references[i]}\")\n",
    "        print('-' * 100)\n",
    "\n",
    "    # 모델 저장 및 조기 종료 확인\n",
    "    if val_result['rouge-l'] > best_rouge_l:\n",
    "        best_rouge_l = val_result['rouge-l']\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), os.path.join(weights_path, 'best_model.pth'))\n",
    "        print(f\"새로운 최고 모델이 저장되었습니다. ROUGE-L: {best_rouge_l:.6f}\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"성능 향상 없음. 조기 종료 카운터: {early_stopping_counter}/{patience}\")\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"조기 종료 실행.\")\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "torch.save(model.state_dict(), os.path.join(weights_path, 'final_model.pth'))\n",
    "print(\"훈련이 완료되었습니다. 최종 모델이 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, device, test_loader, fname):\n",
    "    model.eval()\n",
    "    summary = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids in tqdm(test_loader):\n",
    "            input_ids = input_ids.to(device, dtype=torch.long)\n",
    "\n",
    "            pred_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=256, \n",
    "                num_beams=4,\n",
    "                repetition_penalty=2.0, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "            for ids in pred_ids:\n",
    "                result = tokenizer.decode(ids)\n",
    "                summary.append(result)\n",
    "                \n",
    "    remove_tokens = ['<usr>', f\"{tokenizer.unk_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    preprocessed_summary = summary.copy()\n",
    "    for token in remove_tokens:\n",
    "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": fname,\n",
    "            \"summary\" : preprocessed_summary,\n",
    "        }\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(f'{save_path}/weights/best.pth')\n",
    "output = predict(tokenizer, model, device, test_loader, test_df['fname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(f\"{save_path}/prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
