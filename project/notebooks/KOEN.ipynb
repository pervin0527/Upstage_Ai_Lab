{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from googletrans import Translator\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(\"../dataset/cleaned_train.csv\")\n",
    "model_name = 'Helsinki-NLP/opus-mt-ko-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences(sentences, model, tokenizer):\n",
    "    # 여러 문장을 한 번에 번역\n",
    "    tokenized_texts = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    tokenized_texts = {k: v.to(device) for k, v in tokenized_texts.items()}  # 입력 텐서를 GPU로 이동\n",
    "    translated = model.generate(**tokenized_texts)\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    return translated_texts\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer):\n",
    "    # 하나의 문장을 번역\n",
    "    tokenized_text = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    tokenized_text = {k: v.to(device) for k, v in tokenized_text.items()}  # 입력 텐서를 GPU로 이동\n",
    "    translated = model.generate(**tokenized_text)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "def translation(dialogue):\n",
    "    lines = dialogue.split('\\n')\n",
    "    sentences = [line.split(': ', 1)[1] for line in lines if ': ' in line]\n",
    "    \n",
    "    translated_sentences = translate_sentences(sentences, model, tokenizer)\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if ': ' in line:\n",
    "            speaker, _ = line.split(': ', 1)\n",
    "            lines[i] = f\"{speaker}: {translated_sentences.pop(0)}\"\n",
    "    \n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[0]['dialogue']\n",
    "print(f\"{example}\\n\\n\")\n",
    "print(translation(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_summaries = []\n",
    "translated_dialogues = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    translated_dialogue = translation(row['dialogue'])\n",
    "    translated_summary = translate_sentence(row['summary'], model, tokenizer)\n",
    "\n",
    "    translated_dialogues.append(translated_dialogue)\n",
    "    translated_summaries.append(translated_summary)\n",
    "\n",
    "df['translated_dialogue'] = translated_dialogues\n",
    "df['translated_summary'] = translated_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[17]['translated_dialogue'])\n",
    "print(df.iloc[17]['translated_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../dataset/en_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/cleaned_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_summaries = []\n",
    "translated_dialogues = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    translated_dialogue = translation(row['dialogue'])\n",
    "    translated_summary = translate_sentence(row['summary'], model, tokenizer)\n",
    "\n",
    "    translated_dialogues.append(translated_dialogue)\n",
    "    translated_summaries.append(translated_summary)\n",
    "\n",
    "df['translated_dialogue'] = translated_dialogues\n",
    "df['translated_summary'] = translated_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../dataset/en_dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    '#Person1#',\n",
    "    '#Person2#',\n",
    "    '#Person3#',\n",
    "    '#Person4#',\n",
    "    '#Person5#',\n",
    "    '#Person6#',\n",
    "    '#Person7#',\n",
    "    '#SSN#',\n",
    "    '#Email#',\n",
    "    '#Address#',\n",
    "    '#Reaction#',\n",
    "    '#CarNumber#',\n",
    "    '#Movietitle#',\n",
    "    '#DateOfBirth#',\n",
    "    '#CardNumber#',\n",
    "    '#PhoneNumber#',\n",
    "    '#PassportNumber#'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../dataset/en_train.csv\")\n",
    "valid_df = pd.read_csv(\"../dataset/en_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_train'] = 1\n",
    "valid_df['is_train'] = 0\n",
    "total_df = pd.concat([train_df, valid_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#Person1', '#This', '#Person6', '#1', '#사람1만기', '#Yes', '#or', '#All', '#Ratection', '#Person4', '#작은', '#Person3', '#in', '#I', '#5', '#How', '#Really', '#72', '#샐러드용', '#2', '#나', '#Oh', '#No', '#Person5', '#CardNumber', '#DeateOfBirth', '#right', '#고객님', '#Mobititle', '#Reaction', '#one', '#Adress', '#PhoneNumber', '#Pasport', '#Person', '#Rate', '#Camation', '#You', '#다음', '#CarNumber', '#Look', '#Person2', '#Addresss', '#여기서', '#Niel', '#어디', '#여기', '#Person7', '#time', '#Rection', '#잠깐만요', '#Rase', '#PhoneNomber', '#하지만', '#PoneNumber', '#Swice', '#Cliffs', '#Mobiettetle', '#karNumber', '#DateOfBirth', '#Moby', '#PasportNumber', '#B형', '#Email', '#페리에와', '#passport', '#Address', '#Mobiettele', '#SSN', '#Thank', '#Card', '#Hm'}\n",
      "{'#Person1', '#Verson2', '#680', '#Serson1', '#1', '#Sfrights', '#Yon', '#aded', '#wanson1', '#The', '#I', '#Treate', '#Torress', '#Herson2', '#Painson1', '#Gerry', '#TM', '#Saturative', '#No', '#A', '#Amanda', '#Amenity', '#Secret', '#30', '#feson1', '#will', '#Cola', '#seded', '#6215', '#Terson1', '#35', '#Caren', '#feerson1', '#Let', '#120', '#4000', '#Deige', '#Pointed', '#Run', '#Pherson1', '#Alabama', '#Thank', '#Rost', '#New', '#14', '#Person4', '#Person3', '#Linda', '#Person11', '#Absolute1', '#6', '#401K', '#si', '#Moblogging', '#Blended', '#12', '#Jaz', '#320', '#Henry', '#15', '#State', '#Person2', '#Congratulation', '#Rutin', '#Gamie', '#Instant', '#486', '#pam', '#about', '#GPA', '#3time', '#EMS', '#Serson2', '#tomorrow', '#Ikea', '#Ivy', '#Person6', '#dms', '#Ferson1', '#All', '#don', '#lucky', '#FM', '#5', '#Aperifipro', '#fessson2', '#Bultain', '#11', '#SpanishPad', '#Mary', '#Person', '#Send', '#Snobos', '#Ferson2', '#Salba', '#Da', '#Dan', '#Waltz', '#Trillto', '#It', '#Sister', '#Cultural', '#today', '#ads', '#Presidents', '#Gold', '#Steam', '#wants', '#3', '#Extra', '#Good', '#IBA', '#150', '#Dads', '#25', '#AAAAAAAAA', '#Happy', '#88', '#How', '#Herson1', '#735', '#8', '#2', '#He', '#good', '#Id', '#Prinson1', '#Tend', '#T', '#Boon', '#Bucky', '#100', '#5003', '#202', '#They', '#10', '#Harson2', '#In'}\n"
     ]
    }
   ],
   "source": [
    "# 특수 문자열 패턴 찾기 함수\n",
    "def find_special_strings(text):\n",
    "    return re.findall(r'#\\w+', text)\n",
    "\n",
    "total_df['dialogue_special_strings'] = total_df['translated_dialogue'].apply(find_special_strings)\n",
    "total_df['summary_special_strings'] = total_df['translated_summary'].apply(find_special_strings)\n",
    "\n",
    "# total_df[['dialogue_special_strings', 'summary_special_strings']].head()\n",
    "unique_dialogue_strings = set([item for sublist in total_df['dialogue_special_strings'] for item in sublist])\n",
    "unique_summary_strings = set([item for sublist in total_df['summary_special_strings'] for item in sublist])\n",
    "\n",
    "print(unique_dialogue_strings)\n",
    "print(unique_summary_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
