{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [24:04<00:00, 361.24s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"saltlux/Ko-Llama3-Luxia-8B\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=tokenizer.eos_token_id  # 또는 다른 적절한 토큰 ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../dataset/cleaned_train.csv\")\n",
    "valid_df = pd.read_csv(\"../dataset/cleaned_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "\n",
      "스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "ko_dig = train_df.iloc[0]['dialogue']\n",
    "ko_sum = train_df.iloc[0]['summary']\n",
    "\n",
    "print(f\"{ko_dig}\\n\\n{ko_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_message, user_message):\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : system_message},\n",
    "        {\"role\" : \"user\", \"content\" : user_message},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    \n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=5000,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id  # 명시적으로 pad_token_id 설정\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "system_message = \"\"\"You are a precise Korean to English translator. Your task is to:\n",
    "1. Translate the given Korean text after '#:' into English.\n",
    "2. Preserve any '#Person#' tags exactly as they appear.\n",
    "3. Start your response with the same speaker tag as the input.\n",
    "4. Provide ONLY the translation, without any additional text or explanations.\n",
    "5. If you cannot translate, respond with 'UNABLE_TO_TRANSLATE'.\n",
    "\n",
    "Example input: #Person1#: 안녕하세요, 오늘 날씨가 좋네요.\n",
    "Example output: #Person1#: Hello, the weather is nice today.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_response(system_message, ko_dig)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dialogue(dialogue):\n",
    "    return generate_response(system_message, dialogue)\n",
    "\n",
    "def translate_summary(summary):\n",
    "    return generate_response(system_message, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_train'] = 1\n",
    "valid_df['is_train'] = 0\n",
    "combined_df = pd.concat([train_df, valid_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: 지금 체크인하고 싶습니다. 여기 제 항공권입니다.\n",
      "#Person2#: 체크인할 짐이 없으신가요?\n",
      "#Person1#: 네, 없습니다.\n",
      "#Person2#: 선호하는 좌석이 있으신가요?\n",
      "#Person1#: 통로 좌석을 원합니다. 창가 근처는 끔찍해서요.\n",
      "#Person2#: 여기 탑승권입니다. 좋은 하루 되세요.\n",
      "#Person1#은 체크인하고 #Person2#의 도움으로 복도쪽 좌석을 얻었다.\n",
      "\n",
      "\n",
      "#Person1#: I'd like to check in now. Here's my boarding pass.\n",
      "#Person2#: Do you have any luggage to check in?\n",
      "#Person1#: No, I don't.\n",
      "#Person2#: Do you have a preferred seat?\n",
      "#Person1#: I want a window seat. I hate being near the aisle.\n",
      "#Person2#: Here's your boarding pass. Have a good day.\n",
      "#Person1#: I got a seat by the corridor with the help of #Person2# after checking in.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ko_dig = train_df.iloc[838]['dialogue']\n",
    "ko_sum = train_df.iloc[838]['summary']\n",
    "\n",
    "print(f\"{ko_dig}\\n{ko_sum}\\n\\n\")\n",
    "\n",
    "trans_dig = generate_response(system_message, ko_dig)\n",
    "trans_sum = generate_response(system_message, ko_sum)\n",
    "\n",
    "print(f\"{trans_dig}\\n{trans_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 12781/12956 [16:46:05<11:28,  3.93s/it]  "
     ]
    }
   ],
   "source": [
    "translated_dialogues = []\n",
    "translated_summaries = []\n",
    "\n",
    "for index, row in tqdm(combined_df.iterrows(), total=len(combined_df)):\n",
    "    try:\n",
    "        translated_dialogue = translate_dialogue(row['dialogue'])\n",
    "        translated_summary = translate_summary(row['summary'])\n",
    "        \n",
    "        translated_dialogues.append(translated_dialogue)\n",
    "        translated_summaries.append(translated_summary)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {index}\")\n",
    "        print(f\"Dialogue:\\n{row['dialogue']}\")\n",
    "        print(f\"Summary:\\n{row['summary']}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_translated = combined_df[combined_df['is_train'] == 1].drop('is_train', axis=1)\n",
    "valid_df_translated = combined_df[combined_df['is_train'] == 0].drop('is_train', axis=1)\n",
    "\n",
    "train_df_translated.to_csv(\"../dataset/translated_train.csv\", index=False)\n",
    "valid_df_translated.to_csv(\"../dataset/translated_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
