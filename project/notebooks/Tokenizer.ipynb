{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "\n",
    "from mecab import MeCab\n",
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12457, 4)\n",
      "     fname                                           dialogue  \\\n",
      "0  train_0  #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나...   \n",
      "1  train_1  #Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\\n#Person2#...   \n",
      "2  train_2  #Person1#: 실례합니다, 열쇠 한 묶음 보셨나요?\\n#Person2#: 어떤...   \n",
      "3  train_3  #Person1#: 왜 너는 여자친구가 있다는 걸 말해주지 않았어?\\n#Person...   \n",
      "4  train_4  #Person1#: 안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 ...   \n",
      "\n",
      "                                             summary     topic  \n",
      "0  스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...   건강검진 받기  \n",
      "1  파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...        백신  \n",
      "2  #Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...     열쇠 찾기  \n",
      "3  #Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...  여자친구가 있다  \n",
      "4  말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...        댄스  \n",
      "\n",
      "(499, 4)\n",
      "   fname                                           dialogue  \\\n",
      "0  dev_0  #Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \\n#Person2#: 요즘...   \n",
      "1  dev_1  #Person1#: 헤이, 지미. 나중에 운동하러 가자.\\n#Person2#: 그래...   \n",
      "2  dev_2  #Person1#: 나는 더 이상 건강에 해로운 음식을 먹는 것을 멈춰야 해.\\n#...   \n",
      "3  dev_3  #Person1#: UFO를 믿으세요?\\n#Person2#: 물론이죠, 그들은 저기...   \n",
      "4  dev_4  #Person1#: 오늘 학교에 갔어?\\n#Person2#: 당연하지. 너는?\\n#...   \n",
      "\n",
      "                                             summary      topic  \n",
      "0  #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대...  의사에게 상담하기  \n",
      "1    #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.       운동하기  \n",
      "2  #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Pe...     건강한 음식  \n",
      "3  #Person2#는 UFO를 믿고 꿈에서 그들을 볼 수 있다고 말한다. #Perso...   UFO와 외계인  \n",
      "4  #Person1#은 오늘 학교에 가지 않았다. #Person2#는 내일 수업을 빼먹...      학교 가기  \n"
     ]
    }
   ],
   "source": [
    "data_path = \"../dataset\"\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_path}/cleaned_train.csv\")\n",
    "valid_df = pd.read_csv(f\"{data_path}/cleaned_dev.csv\")\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())\n",
    "print()\n",
    "\n",
    "print(valid_df.shape)\n",
    "print(valid_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_train'] = 1\n",
    "valid_df['is_train'] = 0\n",
    "total_df = pd.concat([train_df, valid_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [#문자열#:]을 제거\n",
    "def remove_speaker_tags(text):\n",
    "    return re.sub(r'#\\w+#:', '', text)\n",
    "\n",
    "## 개행 문자 제거.\n",
    "def remove_newlines(text):\n",
    "    # 개행 문자를 문장부호 뒤에 공백과 함께 추가\n",
    "    text = re.sub(r'([.?!])\\n', r'\\1 ', text)\n",
    "\n",
    "    # 개행 문자가 남아있는 경우 제거\n",
    "    return text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '하', '세요', ',', '스미스', '씨', '.', '저', '는', '호킨스', '의사', '입니다', '.', '오늘', '왜', '오', '셨', '나요', '?', '건강', '검진', '을', '받', '는', '것', '이', '좋', '을', '것', '같', '아서요', '.', '그렇', '군요', ',', '당신', '은', '5', '년', '동안', '건강', '검진', '을', '받', '지', '않', '았', '습니다', '.', '매년', '받', '아야', '합니다', '.', '알', '고', '있', '습니다', '.', '하지만', '아무', '문제', '가', '없', '다면', '왜', '의사', '를', '만나', '러', '가', '야', '하', '나요', '?', '심각', '한', '질병', '을', '피하', '는', '가장', '좋', '은', '방법', '은', '이', '를', '조기', '에', '발견', '하', '는', '것', '입니다', '.', '그러', '니', '당신', '의', '건강', '을', '위해', '최소한', '매년', '한', '번', '은', '오', '세요', '.', '알', '겠', '습니다', '.', '여기', '보', '세요', '.', '당신', '의', '눈', '과', '귀', '는', '괜찮', '아', '보', '입니다', '.', '깊', '게', '숨', '을', '들이쉬', '세요', '.', '스미스', '씨', ',', '담배', '피우', '시', '나요', '?', '네', '.', '당신', '도', '알', '다시피', ',', '담배', '는', '폐암', '과', '심장', '병', '의', '주요', '원인', '입니다', '.', '정말로', '끊', '으셔야', '합니다', '.', '수백', '번', '시도', '했', '지만', ',', '습관', '을', '버리', '는', '것', '이', '어렵', '습니다', '.', '우리', '는', '도움', '이', '될', '수', '있', '는', '수업', '과', '약물', '들', '을', '제공', '하', '고', '있', '습니다', '.', '나가', '기', '전', '에', '더', '많', '은', '정보', '를', '드리', '겠', '습니다', '.', '알', '겠', '습니다', ',', '감사', '합니다', ',', '의사', '선생', '님', '.']\n"
     ]
    }
   ],
   "source": [
    "## Sample\n",
    "cleaned_dialogue_sample = remove_speaker_tags(total_df['dialogue'].iloc[0])\n",
    "cleaned_dialogue_sample = remove_newlines(cleaned_dialogue_sample)\n",
    "\n",
    "mecab = MeCab()\n",
    "morphs = mecab.morphs(cleaned_dialogue_sample)\n",
    "print(morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "      <th>is_train</th>\n",
       "      <th>cleaned_dialogue</th>\n",
       "      <th>cleaned_summary</th>\n",
       "      <th>dialogue_morphs</th>\n",
       "      <th>summary_morphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나...</td>\n",
       "      <td>스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...</td>\n",
       "      <td>건강검진 받기</td>\n",
       "      <td>1</td>\n",
       "      <td>안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?  건강검진을 ...</td>\n",
       "      <td>스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...</td>\n",
       "      <td>[안녕, 하, 세요, ,, 스미스, 씨, ., 저, 는, 호킨스, 의사, 입니다, ...</td>\n",
       "      <td>[스미스, 씨, 가, 건강, 검진, 을, 받, 고, 있, 고, ,, 호킨스, 의사,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>#Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\\n#Person2#...</td>\n",
       "      <td>파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...</td>\n",
       "      <td>백신</td>\n",
       "      <td>1</td>\n",
       "      <td>안녕하세요, 파커 부인, 어떻게 지내셨나요?  안녕하세요, 피터스 박사님. 잘 지...</td>\n",
       "      <td>파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...</td>\n",
       "      <td>[안녕, 하, 세요, ,, 파커, 부인, ,, 어떻게, 지내, 셨, 나요, ?, 안...</td>\n",
       "      <td>[파커, 부인, 이, 리키, 를, 데리, 고, 백신, 접종, 을, 하, 러, 갔, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>#Person1#: 실례합니다, 열쇠 한 묶음 보셨나요?\\n#Person2#: 어떤...</td>\n",
       "      <td>#Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...</td>\n",
       "      <td>열쇠 찾기</td>\n",
       "      <td>1</td>\n",
       "      <td>실례합니다, 열쇠 한 묶음 보셨나요?  어떤 종류의 열쇠인가요?  5개의 열쇠와 ...</td>\n",
       "      <td>#Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...</td>\n",
       "      <td>[실례, 합니다, ,, 열쇠, 한, 묶음, 보, 셨, 나요, ?, 어떤, 종류, 의...</td>\n",
       "      <td>[#, Person, 1, #, 은, 열쇠, 한, 묶음, 을, 찾, 고, 있, 고,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>#Person1#: 왜 너는 여자친구가 있다는 걸 말해주지 않았어?\\n#Person...</td>\n",
       "      <td>#Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...</td>\n",
       "      <td>여자친구가 있다</td>\n",
       "      <td>1</td>\n",
       "      <td>왜 너는 여자친구가 있다는 걸 말해주지 않았어?  미안해, 너가 이미 알고 있다고...</td>\n",
       "      <td>#Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...</td>\n",
       "      <td>[왜, 너, 는, 여자, 친구, 가, 있, 다는, 걸, 말, 해, 주, 지, 않, ...</td>\n",
       "      <td>[#, Person, 1, #, 은, #, Person, 2, #, 가, 여자, 친...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>#Person1#: 안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 ...</td>\n",
       "      <td>말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...</td>\n",
       "      <td>댄스</td>\n",
       "      <td>1</td>\n",
       "      <td>안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 나와 함께 해줄래?...</td>\n",
       "      <td>말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...</td>\n",
       "      <td>[안녕, ,, 숙녀, 분, 들, !, 오늘, 밤, 당신, 들, 은, 정말, 멋져, ...</td>\n",
       "      <td>[말릭, 이, 니키, 에게, 춤, 을, 요청, 한다, ., 말, 릭, 이, 발, 을...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fname                                           dialogue  \\\n",
       "0  train_0  #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나...   \n",
       "1  train_1  #Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\\n#Person2#...   \n",
       "2  train_2  #Person1#: 실례합니다, 열쇠 한 묶음 보셨나요?\\n#Person2#: 어떤...   \n",
       "3  train_3  #Person1#: 왜 너는 여자친구가 있다는 걸 말해주지 않았어?\\n#Person...   \n",
       "4  train_4  #Person1#: 안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 ...   \n",
       "\n",
       "                                             summary     topic  is_train  \\\n",
       "0  스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...   건강검진 받기         1   \n",
       "1  파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...        백신         1   \n",
       "2  #Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...     열쇠 찾기         1   \n",
       "3  #Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...  여자친구가 있다         1   \n",
       "4  말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...        댄스         1   \n",
       "\n",
       "                                    cleaned_dialogue  \\\n",
       "0   안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?  건강검진을 ...   \n",
       "1   안녕하세요, 파커 부인, 어떻게 지내셨나요?  안녕하세요, 피터스 박사님. 잘 지...   \n",
       "2   실례합니다, 열쇠 한 묶음 보셨나요?  어떤 종류의 열쇠인가요?  5개의 열쇠와 ...   \n",
       "3   왜 너는 여자친구가 있다는 걸 말해주지 않았어?  미안해, 너가 이미 알고 있다고...   \n",
       "4   안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 나와 함께 해줄래?...   \n",
       "\n",
       "                                     cleaned_summary  \\\n",
       "0  스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...   \n",
       "1  파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...   \n",
       "2  #Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...   \n",
       "3  #Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...   \n",
       "4  말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...   \n",
       "\n",
       "                                     dialogue_morphs  \\\n",
       "0  [안녕, 하, 세요, ,, 스미스, 씨, ., 저, 는, 호킨스, 의사, 입니다, ...   \n",
       "1  [안녕, 하, 세요, ,, 파커, 부인, ,, 어떻게, 지내, 셨, 나요, ?, 안...   \n",
       "2  [실례, 합니다, ,, 열쇠, 한, 묶음, 보, 셨, 나요, ?, 어떤, 종류, 의...   \n",
       "3  [왜, 너, 는, 여자, 친구, 가, 있, 다는, 걸, 말, 해, 주, 지, 않, ...   \n",
       "4  [안녕, ,, 숙녀, 분, 들, !, 오늘, 밤, 당신, 들, 은, 정말, 멋져, ...   \n",
       "\n",
       "                                      summary_morphs  \n",
       "0  [스미스, 씨, 가, 건강, 검진, 을, 받, 고, 있, 고, ,, 호킨스, 의사,...  \n",
       "1  [파커, 부인, 이, 리키, 를, 데리, 고, 백신, 접종, 을, 하, 러, 갔, ...  \n",
       "2  [#, Person, 1, #, 은, 열쇠, 한, 묶음, 을, 찾, 고, 있, 고,...  \n",
       "3  [#, Person, 1, #, 은, #, Person, 2, #, 가, 여자, 친...  \n",
       "4  [말릭, 이, 니키, 에게, 춤, 을, 요청, 한다, ., 말, 릭, 이, 발, 을...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = MeCab()\n",
    "\n",
    "# Dialogue와 Summary 형태소 분석\n",
    "total_df['cleaned_dialogue'] = total_df['dialogue'].apply(lambda x: remove_newlines(remove_speaker_tags(x)))\n",
    "total_df['cleaned_summary'] = total_df['summary'].apply(lambda x: remove_newlines(remove_speaker_tags(x)))\n",
    "\n",
    "total_df['dialogue_morphs'] = total_df['cleaned_dialogue'].apply(lambda x: mecab.morphs(x))\n",
    "total_df['summary_morphs'] = total_df['cleaned_summary'].apply(lambda x: mecab.morphs(x))\n",
    "\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = total_df['dialogue_morphs'].tolist() + total_df['summary_morphs'].tolist()\n",
    "corpus = [' '.join(morphs) for morphs in corpus]\n",
    "\n",
    "with open('./spm_corpus.txt', 'w') as f:\n",
    "    for line in corpus:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 안녕 하 세요 , 스미스 씨 . 저 는 호킨스 의사 입니다 . 오늘 왜 오 셨 나요 ? 건강 검진 을 받 는 것 이 좋 을 것 같 아서요 . 그렇 군요 , 당신 은 5 년 동안 건강 검진 을 받 지 않 았 습니다 . 매년 받 아야 합니다 . 알 고 있 습니다 . 하지만 아무 문제 가 없 다면 왜 의사 를 만나 러 가 야 하 나요 ? 심각 한 질병 을 피하 는 가장 좋 은 방법 은 이 를 조기 에 발견 하 는 것 입니다 . 그러 니 당신 의 건강 을 위해 최소한 매년 한 번 은 오 세요 . 알 겠 습니다 . 여기 보 세요 . 당신 의 눈 과 귀 는 괜찮 아 보 입니다 . 깊 게 숨 을 들이쉬 세요 . 스미스 씨 , 담배 피우 시 나요 ? 네 . 당신 도 알 다시피 , 담배 는 폐암 과 심장 병 의 주요 원인 입니다 . 정말로 끊 으셔야 합니다 . 수백 번 시도 했 지만 , 습관 을 버리 는 것 이 어렵 습니다 . 우리 는 도움 이 될 수 있 는 수업 과 약물 들 을 제공 하 고 있 습니다 . 나가 기 전 에 더 많 은 정보 를 드리 겠 습니다 . 알 겠 습니다 , 감사 합니다 , 의사 선생 님 .\n",
      "Tokenized: ['▁안녕', '▁하', '▁세요', '▁,', '▁스미', '스', '▁씨', '▁.', '▁저', '▁는', '▁호', '킨', '스', '▁의사', '▁입니다', '▁.', '▁오늘', '▁왜', '▁오', '▁셨', '▁나', '요', '▁?', '▁건강', '▁검', '진', '▁을', '▁받', '▁는', '▁', '것', '▁이', '▁좋', '▁을', '▁', '것', '▁같', '▁아서', '요', '▁.', '▁그렇', '▁군요', '▁,', '▁당', '신', '▁은', '▁5', '▁', '년', '▁동', '안', '▁건강', '▁검', '진', '▁을', '▁받', '▁지', '▁', '않', '▁았', '▁습니다', '▁.', '▁매', '년', '▁받', '▁아야', '▁합니다', '▁.', '▁알', '▁고', '▁', '있', '▁습니다', '▁.', '▁하', '지만', '▁아무', '▁문제', '▁가', '▁없', '▁다면', '▁왜', '▁의사', '▁', '를', '▁만', '나', '▁러', '▁가', '▁야', '▁하', '▁나', '요', '▁?', '▁심', '각', '▁한', '▁질', '병', '▁을', '▁피', '하', '▁는', '▁가장', '▁좋', '▁은', '▁방', '법', '▁은', '▁이', '▁', '를', '▁조', '기', '▁에', '▁발', '견', '▁하', '▁는', '▁', '것', '▁입니다', '▁.', '▁그러', '▁니', '▁당', '신', '▁의', '▁건강', '▁을', '▁위해', '▁최소', '한', '▁매', '년', '▁한', '▁번', '▁은', '▁오', '▁세요', '▁.', '▁알', '▁겠', '▁습니다', '▁.', '▁여기', '▁보', '▁세요', '▁.', '▁당', '신', '▁의', '▁눈', '▁과', '▁귀', '▁는', '▁괜', '찮', '▁아', '▁보', '▁입니다', '▁.', '▁깊', '▁게', '▁숨', '▁을', '▁들', '이', '쉬', '▁세요', '▁.', '▁스미', '스', '▁씨', '▁,', '▁담배', '▁피', '우', '▁시', '▁나', '요', '▁?', '▁네', '▁.', '▁당', '신', '▁도', '▁알', '▁다시', '피', '▁,', '▁담배', '▁는', '▁폐', '암', '▁과', '▁심장', '▁병', '▁의', '▁주', '요', '▁원', '인', '▁입니다', '▁.', '▁정말', '로', '▁끊', '▁으', '셔야', '▁합니다', '▁.', '▁수백', '▁번', '▁시', '도', '▁했', '▁지만', '▁,', '▁습', '관', '▁을', '▁버', '리', '▁는', '▁', '것', '▁이', '▁어', '렵', '▁습니다', '▁.', '▁우리', '▁는', '▁도', '움', '▁이', '▁될', '▁수', '▁', '있', '▁는', '▁수업', '▁과', '▁약', '물', '▁들', '▁을', '▁', '제공', '▁하', '▁고', '▁', '있', '▁습니다', '▁.', '▁나', '가', '▁기', '▁전', '▁에', '▁더', '▁많', '▁은', '▁정보', '▁', '를', '▁드리', '▁겠', '▁습니다', '▁.', '▁알', '▁겠', '▁습니다', '▁,', '▁감사', '▁합니다', '▁,', '▁의사', '▁선', '생', '▁님', '▁.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./spm_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ./spm_unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8784\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./spm_corpus.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4409 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 25897 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 15 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=6867782\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=1119\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 25897 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3034157\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 12248 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 25897\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 22917\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 22917 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10553 obj=8.18851 num_tokens=45918 num_tokens/piece=4.35118\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8712 obj=7.71281 num_tokens=46270 num_tokens/piece=5.31107\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ./spm_unigram.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ./spm_unigram.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='./spm_corpus.txt', \n",
    "    model_prefix='./spm_unigram', \n",
    "    vocab_size=8784,  # 원하는 vocab 크기로 조정 가능\n",
    "    model_type='unigram'\n",
    ")\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file='./spm_unigram.model')\n",
    "sample_text = ' '.join(total_df['dialogue_morphs'].iloc[0])\n",
    "print(f\"Original Text: {sample_text}\")\n",
    "print(f\"Tokenized: {sp.encode_as_pieces(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m▁\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 22\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mSentencePieceTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./spm_unigram.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mSentencePieceTokenizer.__init__\u001b[0;34m(self, sp_model_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sp_model_path):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp\u001b[38;5;241m.\u001b[39mload(sp_model_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils.py:436\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens_extended\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_added_tokens_encoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils.py:544\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._add_tokens\u001b[0;34m(self, new_tokens, special_tokens)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m added_tokens\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# TODO this is fairly slow to improve!\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m current_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    545\u001b[0m new_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(current_vocab)  \u001b[38;5;66;03m# only call this once, len gives the last index + 1\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m new_tokens:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1697\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;124;03m    Returns the vocabulary as a dictionary of token to index.\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;124;03m        `Dict[str, int]`: The vocabulary.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class SentencePieceTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, sp_model_path):\n",
    "        super().__init__()\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(sp_model_path)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self.sp.encode_as_pieces(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.sp.piece_to_id(token)\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.sp.id_to_piece(index)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return ''.join(tokens).replace('▁', ' ').strip()\n",
    "    \n",
    "tokenizer = SentencePieceTokenizer('./spm_unigram.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer(\n",
    "    \"./spm_unigram.model\",\n",
    "    unk_token=\"<unk>\"\n",
    ")\n",
    "\n",
    "tokenizer.save(\"./tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
